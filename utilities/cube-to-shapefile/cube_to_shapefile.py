USAGE = """

Create shapefile of Cube network, roadway and transit.

Requires geopandas.

Run it in the directory in which you want the shapefile, e.g. model_dir\\OUTPUT\\shapefile
python cube_to_shapefile.py
  --trn_stop_info "M:\\Application\Model One\\Networks\\TM1_2015_Base_Network\\Node Description.xls"
  --linefile ..\\..\\trn\\transitLines.lin
  --loadvol_dir ..\\trn
  --transit_crowding ..\\metrics\\transit_crowding_complete.csv
  ..\\avgload5period.net

 It saves all output to the current working directory.
 The roadway network is: network_nodes.shp and network_links.shp

 If --linefile is specified, then this script creates three additional shapefiles.
 If --by_operator is specified, then these three shapefiles will be split by operator group, defined in the script below.
 If --loadvol_dir is specified, then loaded transit assignment files are read:
   * trnlink(EA|AM|MD|PM|EV)_ALLMSA.dbf  - the consolidated DBFs by time period
   * trnlink.csv - the consolidated CSVs, generated by scripts\core_summaries\ConsolidateLoadedTransit.R
     Note that these have some more detailed info, including support links and link distances/times by time period
     If this file isn't present, then support links and these extra fields will be ommited from network_trn_links.shp
If --transit_crowding is specified, then the complete transit crowding file is read (see transitcrowding.py)
   * VEHTYPE, VEHCAP and PDCAP are all assumed to be more accurate from this source, so they'll override the version version
     from the loadvol_dir
   * seated vehicle capacity, seated period vehicle capacity and load seated are also reported.

   1) network_trn_lines.shp - this has all the information unique to the line, such as FREQ per time period,
      ONE_WAY, LONG_NAME, etc.
      * "NAME" represents unique lines, variations of the same route (e.g. peak-service and off-peak service) are represented
        by separate lines; "NAME_SET" represents "route", variations of the same route have the same "NAME_SET",
        two lines with opposite directions (non-circular route, ONE_WAY==0) have the same "NAME_SET".
      * If --loadvol_dir is specified, then VEHTYPE, VEHCAP, and PDCAP will be given for each time period
        representing standing capacity/load.      
      * If --transit_crowding is specified, then seated capacity (SEATCAP), period seat cap (PDSECAP) will be reported.
        Fields when both are specified:
        ['Id', 'NAME', 'NAME_SET', 'LONG_NAME',
        'FREQ_EA', 'FREQ_AM', 'FREQ_MD', 'FREQ_PM', 'FREQ_EV',
        'ONEWAY', 'MODE', 'MODE_NAME', 'MODE_TYPE', 'OPERATOR_T',
        'FIRST_N', 'FIRST_NAME', 'LAST_N', 'LAST_NAME', 'N_OR_S', 'E_OR_W',
        'VEHTYPE_EA', 'VEHTYPE_AM', 'VEHTYPE_MD', 'VEHTYPE_PM', 'VEHTYPE_EV',
        'VEHCAP_EA', 'VEHCAP_AM', 'VEHCAP_MD', 'VEHCAP_PM', 'VEHCAP_EV',
        'PDCAP_EA', 'PDCAP_AM', 'PDCAP_MD', 'PDCAP_PM', 'PDCAP_EV',
        'SEATCAP_EA', 'SEATCAP_AM', 'SEATCAP_MD', 'SEATCAP_PM', 'SEATCAP_EV',
        'PDSECAP_EA', 'PDSECAP_AM', 'PDSECAP_MD', 'PDSECAP_PM', 'PDSECAP_EV', 'geometry']

   2) network_trn_links.shp - this has per line, link information, including A, B, SEQ.
      * If --loadvol_dir is specified, then AB_VOL and LOAD will be given per time period as well.
      * If --transit_crowding is specified, then LOADSE (load seated) will be reported per time period.
        Fields when both are specified: 
        ['Id', 'NAME', 'A', 'B', 'A_STATION', 'B_STATION', 'SEQ', 'NAMESEQAB', 'MODE', 'MODE_NAME', 'MODE_TYPE',
        'AB_VOL_EA', 'AB_VOL_AM', 'AB_VOL_MD', 'AB_VOL_PM', 'AB_VOL_EV',
        'LOAD_EA', 'LOAD_AM', 'LOAD_MD', 'LOAD_PM', 'LOAD_EV',
        'LOADSE_EA', 'LOADSE_AM', 'LOADSE_MD', 'LOADSE_PM', 'LOADSE_EV',
        'DIST_EA', 'DIST_AM', 'DIST_MD', 'DIST_PM', 'DIST_EV',
        'TIME_EA', 'TIME_AM', 'TIME_MD', 'TIME_PM', 'TIME_EV', 'geometry']
      
   3) network_trn_route_links.shp - this maps each link (A, B) to all the routes ("NAME_SET") that contains the link.
      * "LINE_COUNT" indicates how many lines contain this link, in other words, these lines have geographically overlapping
        transit services. 
      * Note on joining this table with other tables: this table only has "NAME_SET" field, no "NAME" field. A link being used
        by a route ("NAME_SET") doesn't necessarily mean it is used by all lines ("NAME") of that route.
      * Fields when both "--loadvol_dir" and "--transit_crowding" are specified:
        ['Id', 'A', 'B', 'A_STATION', 'B_STATION', 'NAME_SET', 'MODE', 'MODE_NAME', 'MODE_TYPE',
        'OPERATOR_T', 'LINE_COUNT', 'ROUTE_A_B',
        'TRIPS_EA', 'TRIPS_AM', 'TRIPS_MD', 'TRIPS_PM', 'TRIPS_EV',
        'PDCAP_EA', 'PDCAP_AM', 'PDCAP_MD', 'PDCAP_PM', 'PDCAP_EV',
        'PDSECAP_EA', 'PDSECAP_AM', 'PDSECAP_MD', 'PDSECAP_PM', 'PDSECAP_EV',
        'ABVOL_EA', 'ABVOL_AM', 'ABVOL_MD', 'ABVOL_PM', 'ABVOL_EV',
        'LOAD_EA', 'LOAD_AM', 'LOAD_MD', 'LOAD_PM', 'LOAD_EV',
        'LOADSE_EA', 'LOADSE_AM', 'LOADSE_MD', 'LOADSE_PM', 'LOADSE_EV', 'geometry']

   4) network_trn_stops.shp - this has per stop information, including LINE_NAME, STATION, N, SEQ, IS_STOP.
      If --loadvol_dr is specified, then boardings (BRD) and exits (XIT) as given per time period.

"""

import argparse, collections, copy, csv, logging, os, pathlib, re, subprocess, sys, traceback
import geopandas as gpd
import pandas as pd
import shapely.geometry
import pyproj

RUNTPP_PATH     = pathlib.Path("C:\\Program Files (x86)\\Citilabs\\CubeVoyager")
LOG_FILE        = "cube_to_shapefile.log"

# shapefiles
NODE_SHPFILE    = "network_nodes.shp"
LINK_SHPFILE    = "network_links.shp"

TRN_LINES_SHPFILE = "network_trn_lines{}.shp"
TRN_LINKS_SHPFILE = "network_trn_links{}.shp"
TRN_STOPS_SHPFILE = "network_trn_stops{}.shp"

# aggregated by name set
TRN_ROUTE_LINKS_SHPFILE = "network_trn_route_links.shp"

COUNTY_SHPFILE = pathlib.Path("X:/travel-model-one-master/utilities/geographies/region_county.shp")

TIMEPERIOD_DURATIONS = collections.OrderedDict([
    ("EA",3.0),
    ("AM",4.0),
    ("MD",5.0),
    ("PM",4.0),
    ("EV",8.0)
])

MODE_NUM_TO_NAME = {
    # number: [name, operator] (https://github.com/BayAreaMetro/modeling-website/wiki/TransitModes)
    # Support
    1  :["Walk access connector",                           "NA"],
    2  :["Drive access connector",                          "NA"],
    3  :["Stop-to-stop transfer",                           "NA"],
    4  :["Drive access funnel link",                        "NA"],
    5  :["Walk access funnel link",                         "NA"],
    6  :["Walk egress connector",                           "NA"],
    7  :["Drive egress connector",                          "NA"],
    # Local Bus
    10 :["West Berkeley",                             "Other"      ],
    11 :["Broadway Shuttle",                          "Other"      ],
    12 :["Emery Go Round",                            "Other"      ],
    13 :["Stanford Shuttles",                         "Other"      ],
    14 :["Caltrain Shuttles",                         "Other"      ],
    15 :["VTA Shuttles",                              "Other"      ],
    16 :["Palo Alto/Menlo Park Shuttles",             "Other"      ],
    17 :["Wheels ACE Shuttles",                       "Other"      ],
    18 :["Amtrak Shuttles",                           "Other"      ],
    19 :["San Leandro Links",                         "Other"      ],
    20 :["MUNI Cable Cars",                           "SF_Muni"    ],
    21 :["MUNI Local",                                "SF_Muni"    ],
    24 :["SamTrans Local",                            "SM_SamTrans"],
    27 :["Santa Clara VTA Community bus",             "SC_VTA"     ],
    28 :["Santa Clara VTA Local",                     "SC_VTA"     ],
    30 :["AC Transit Local",                          "AC_Transit" ],
    33 :["WHEELS Local",                              "Other"      ],
    38 :["Union City Transit ",                       "Other"      ],
    40 :["AirBART",                                   "Other"      ],
    42 :["County Connection (CCTA) Local",            "Other"      ],
    44 :["Tri-Delta",                                 "Other"      ],
    46 :["WestCAT Local",                             "Other"      ],
    49 :["Vallejo Transit Local",                     "Other"      ],
    52 :["Fairfield And Suisun Transit Local",        "Other"      ],
    55 :["American Canyon Transit",                   "Other"      ],
    56 :["Vacaville City Coach",                      "Other"      ],
    58 :["Benicia Breeze",                            "Other"      ],
    60 :["VINE Local",                                "Other"      ],
    63 :["Sonoma County Transit Local",               "Other"      ],
    66 :["Santa Rosa City Bus",                       "Other"      ],
    68 :["Petaluma Transit",                          "Other"      ],
    70 :["Golden Gate Transit Local",                 "GG_Transit" ],
    # Express Bus
    80 :["SamTrans Express",                          "SM_SamTrans"],
    81 :["Santa Clara VTA Express",                   "SC_VTA"     ],
    82 :["Dumbarton Express",                         "Other"      ],
    83 :["AC Transit Transbay",                       "AC_Transit" ],
    84 :["AC Transit Transbay",                       "AC_Transit" ],
    85 :["AC Transit BRT",                            "AC_Transit" ],
    86 :["County Connection Express",                 "Other"      ],
    87 :["Golden Gate Transit Express San Francisco", "GG_Transit" ],
    88 :["Golden Gate Transit Express Richmond",      "GG_Transit" ],
    90 :["WestCAT Express",                           "Other"      ],
    91 :["Vallejo Transit Express",                   "Other"      ],
    92 :["Fairfield And Suisun Transit Express",      "Other"      ],
    93 :["VINE Express",                              "Other"      ],
    94 :["SMART Temporary Express",                   "Other"      ],
    95 :["VINE Express",                              "Other"      ],
    96 :["Tri-Delta - BRT",                           "Other"      ],
    98 :["Regional Express ReX",                      "Other"      ],
    # Ferry
    100:["East Bay Ferries",                          "Other"      ],
    101:["Golden Gate Ferry - Larkspur",              "GG_Transit" ],
    102:["Golden Gate Ferry - Sausalito",             "GG_Transit" ],
    103:["Tiburon Ferry",                             "Other"      ],
    104:["Vallejo Baylink Ferry",                     "Other"      ],
    105:["South City Ferry",                          "Other"      ],
    107:["Treasure Island Ferry",                     "Other"      ],
    # Light Rail
    110:["MUNI Metro",                                "SF_Muni"    ],
    111:["Santa Clara VTA LRT",                       "SC_VTA"     ],
    # Heavy Rail
    120:["BART",                                      "BART"       ],
    121:["Oakland Airport Connector",                 "BART"       ],
    122:["E-BART",                                    "BART"       ],
    # Commuter Rail
    130:["Caltrain",                                  "Caltrain"   ],
    131:["Amtrak - Capitol Corridor",                 "Other"      ],
    132:["Amtrak - San Joaquin",                      "Other"      ],
    133:["ACE",                                       "Other"      ],
    134:["Dumbarton Rail",                            "Other"      ],
    135:["SMART",                                     "Other"      ],
    137:["High-Speed Rail",                           "Other"      ],
}

def runCubeScript(workingdir, script_filename, script_env):
    """
    Run the cube script specified in the workingdir specified.
    Returns the return code.
    """
    # run it
    logging.debug(f"script_env={script_env}")
    RUNTPP_EXE = RUNTPP_PATH / 'runtpp.exe'
    proc = subprocess.Popen(f'\"{RUNTPP_EXE}\" \"{script_filename}\"', 
                            cwd=workingdir, env=script_env,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    for line in proc.stdout:
        line_str = line.decode("utf-8")
        line_str = line_str.strip('\r\n')
        logging.info("  stdout: {0}".format(line_str))
    for line in proc.stderr:
        line_str = line.decode("utf-8")
        line_str = line_str.strip('\r\n')
        logging.info("  stderr: {0}".format(line_str))
    retcode = proc.wait()
    if retcode == 2:
        raise Exception(f"Failed to run Cube script {script_filename}")
    logging.info(f"  Received {retcode} from 'runtpp {script_filename}'")


def get_name_set(line_name, mode_type):
    """
    Generalizes line name to a name set for aggregation.
    """
    prefix  = None
    primary = None
    suffix  = None

    # Commuter rail and heavy rail are special -- just aggregate to a single route
    if mode_type in ["Commuter Rail", "Heavy Rail"]:
        line_group_pattern_simple = re.compile(r"([A-Z0-9]+)_(.*)()")
        match_obj = re.match(line_group_pattern_simple, line_name)
        prefix    = match_obj.group(1)
        primary   = "all"

    else:
        # this one takes these substrings and classifies them as suffices
        line_group_pattern_suffix = re.compile(r"([A-Z0-9]+)_(.+?(?=EB|WB|SB|NB|IN|OUT|R|S|N|AM|PM|Lim|-))(EB|WB|SB|NB|IN|OUT|R|S|N|AM|PM|Lim|-)?(.*?)")
        # this is the simple version if there is no suffix
        line_group_pattern_simple = re.compile(r"([A-Z0-9]+)_(.*)()")

        match_obj = re.match(line_group_pattern_suffix, line_name)
        if match_obj == None:
            match_obj = re.match(line_group_pattern_simple, line_name)

        prefix  = match_obj.group(1)
        primary = match_obj.group(2)
        suffix  = match_obj.group(3)

    logging.debug("get_name_set: {:25}  [{}] [{}] [{}]".format(line_name, prefix, primary, suffix))
    return "{}_{}".format(prefix, primary)

def cube_network_to_shapefiles(
    NETFILE: pathlib.Path,
    LINE_FILE: pathlib.Path,
    TRN_STOP_INFO_FILE: pathlib.Path,
    LOADVOL_DIR: pathlib.Path,
    TRANSIT_CROWDING_FILE: pathlib.Path,
    WORKING_DIR: pathlib.Path,
    by_operator: bool = False
):
    """ See comprehensive documenation in USAGE string.

    Args:
        NETFILE (pathlib.Path): location of roadway network file
        LINE_FILE (pathlib.Path): location of transit line file; Can be None
        TRN_STOP_INFO_FILE (pathlib.Path): location of transit stop excel file, for labeling. Can be None.
        LOADVOL_DIR (pathlib.Path): Location of loaded transit volumes file (trnlink.csv). Can be None.
        TRANSIT_CROWDING_FILE (pathlib.Path): Location of transit crowding file. Can be None.
        WORKING_DIR (pathlib.Path): Working directory (output will be created here)

    Returns:
        None
    """
    # make sure this exists
    os.makedirs(WORKING_DIR, exist_ok=True)

    # setup the environment
    script_env                 = os.environ.copy()
    script_env["PATH"]         = f"{script_env['PATH']};{str(RUNTPP_PATH)}"
    script_env["NET_INFILE"]   = str(NETFILE)
    script_env["NODE_OUTFILE"] = str(NODE_SHPFILE)
    script_env["LINK_OUTFILE"] = str(LINK_SHPFILE)

    # if these exist, check the modification stamp of them and of the source file to give user the option to opt-out of re-exporting
    do_export = True
    if os.path.exists(NODE_SHPFILE) and os.path.exists(LINK_SHPFILE):
        net_mtime  = os.path.getmtime(NETFILE)
        node_mtime = os.path.getmtime(NODE_SHPFILE)
        link_mtime = os.path.getmtime(LINK_SHPFILE)
        if (net_mtime < node_mtime) and (net_mtime < link_mtime):
            # give a chance to opt-out since it's slowwwwww
            print(f"{NODE_SHPFILE} and {LINK_SHPFILE} exist with modification times after source network modification time.  Re-export? (y/n)")
            response = input("")
            if response in ["n","N"]:
                do_export = False

    # run the script to do the work
    if do_export:
        # assume code dir is where this script is
        CODE_DIR = pathlib.Path(__file__).parent
        runCubeScript(WORKING_DIR, CODE_DIR / "export_network.job", script_env)
        logging.info(f"Wrote network node file to {NODE_SHPFILE}")
        logging.info(f"Wrote network link file to {LINK_SHPFILE}")

        # write the projection; cube doesn't do this
        crs = pyproj.CRS.from_epsg(26910)
        with open(NODE_SHPFILE.replace('.shp','.prj'), 'w') as prj_file:
            prj_file.write(crs.to_wkt())
        with open(LINK_SHPFILE.replace('.shp','.prj'), 'w') as prj_file:
            prj_file.write(crs.to_wkt())
    else:
        logging.info(f"Opted out of re-exporting roadway network file.  Using existing {NODE_SHPFILE} and {LINK_SHPFILE}")

    # if we don't have a transit file, then we're done
    if not LINE_FILE: sys.exit(0)

    import Wrangler

    operator_files = [""]
    if by_operator:
        operator_files = set(f"_{x[1]}" for x in list(MODE_NUM_TO_NAME.values()))

    # read the loaded trnlink csv
    trnlink_df   = pd.DataFrame()
    trnlink_recs = {}

    if LOADVOL_DIR:

        # generated by scripts/core_summaries/ConsolidateLoadedTransit.R
        link_filename = LOADVOL_DIR / "trnlink.csv"
        if not os.path.exists(link_filename):
            logging.info(f"{link_filename} does not exist; no transit link support links generated")

        else:
            trnlink_df = pd.read_csv(link_filename)
            logging.info(f"Read {len(trnlink_df):,} rows from {link_filename}")

            # filter to just these cols
            trnlink_df = trnlink_df[["A","B","mode","name","time","distance","AB_VOL","source"]]
            logging.debug(f"\n{trnlink_df.head()}")

            # source = 'trnlinkea_wlk_loc_wlk.csv' - pull timeperiod
            # group by A,B,mode,timeperiod -- so rollup across source for the timeperiod
            trnlink_df['timeperiod'] = trnlink_df.source.str[7:9]
            trnlink_df = trnlink_df.groupby(["A","B","mode","name","timeperiod"]).agg({"AB_VOL" :'sum',
                                                                                       "distance":'mean',
                                                                                       "time"    :'mean'})
            trnlink_df.reset_index(drop=False, inplace=True)
            logging.info(f"Aggregated to {len(trnlink_df):,} links across time periods")

            # move timeperiods to columns
            trnlink_df = pd.pivot_table(trnlink_df, index=["A","B","name","mode"], columns=["timeperiod"], values=["AB_VOL","distance","time"])
            trnlink_df.columns = ["_".join(col).strip() for col in trnlink_df.columns.values]
            trnlink_df.fillna(value=0, inplace=True)
            logging.info(f"Pivoted to {len(trnlink_df):,} links")

            logging.debug("\n{}".format(trnlink_df.head()))

            # index-oriented records, so A,B,name,mode => dict
            trnlink_recs = trnlink_df.to_dict(orient="index")

    # if we read the transit crowding file, some lines (pseudo lines) may have been aggregated into their actual counterparts (see transitcrowding.py)
    transit_crowding_lines = []  # list of line names
    crowding_line_dict     = {}  # name -> {(veh_type_updated|seatcap|period_seatcap|period_standcap, EA|AM|MD|PM|EV) -> value}
    crowding_link_dict     = {}  # (name,a,b,seq) -> {(load_seatcap|load_standcap|AB_VOL|AB_BRDA|AB_XITB, EA|AM|MD|PM|EV) -> value}
    if TRANSIT_CROWDING_FILE:

        transit_crowding_df = pd.read_csv(TRANSIT_CROWDING_FILE)
        transit_crowding_lines = transit_crowding_df["NAME"].drop_duplicates().tolist()
        logging.info(f"Read {len(transit_crowding_df):,} rows and {len(transit_crowding_lines):,} unique line names from {TRANSIT_CROWDING_FILE}")
        logging.debug(f"transit_crowding_df:\n{transit_crowding_df}")

        # Pivot and make into a lookup for line attributes
        crowding_line_df = transit_crowding_df[["NAME","period","veh_type_updated","seatcap","period_seatcap","standcap","period_standcap"]]
        crowding_line_df = crowding_line_df.drop_duplicates().pivot_table(index=["NAME"], columns=["period"], 
                                                                          values=["veh_type_updated","seatcap","period_seatcap","standcap","period_standcap"],
                                                                          aggfunc='first', fill_value=0)
        logging.debug(f"crowding_line_df head=\n{crowding_line_df.head(20)}")
        crowding_line_dict = crowding_line_df.to_dict(orient='index')

        # need AB_XITA for stops -- so create it by joining with next link
        ab_xita_df = transit_crowding_df[["NAME","period","SEQ","AB_XITB"]].copy()  # these exits occurred at B -- shift to A
        ab_xita_df["SEQ"] = ab_xita_df["SEQ"] + 1
        ab_xita_df.rename(columns={"AB_XITB":"AB_XITA"}, inplace=True)
        transit_crowding_df = pd.merge(
            left=transit_crowding_df, 
            right=ab_xita_df, 
            how="left", 
            on=["NAME","period","SEQ"]
        ).fillna(value={"AB_XITA":0})

        # Pivot and make into a lookup for link attributes
        crowding_link_df = transit_crowding_df[["NAME","A","B","SEQ","period","load_seatcap","load_standcap","AB_VOL","AB_BRDA","AB_XITB","AB_XITA"]].pivot_table(
                                                                          index=["NAME","A","B","SEQ"], columns=["period"], fill_value=0)
        logging.debug(f"crowding_link_df head=\n{crowding_link_df.head(20)}")
        crowding_link_dict = crowding_link_df.to_dict(orient='index')

    # these will become our dataframes; key = opertor_file, value = list of dictionaries
    line_dict_list     = {}
    link_dict_list     = {}
    stop_dict_list     = {}
    # not by operator_file
    agg_link_dict_list = []

    # read the node points
    nodes_gdf = gpd.read_file(WORKING_DIR / NODE_SHPFILE)

    node_dicts = {}
    node_dicts["X"] = dict(zip(nodes_gdf["N"].tolist(), nodes_gdf.geometry.x.tolist()))
    node_dicts["Y"] = dict(zip(nodes_gdf["N"].tolist(), nodes_gdf.geometry.y.tolist()))

    # read the stop information, if there is any
    stops_to_station = {}
    if TRN_STOP_INFO_FILE:
        stop_info_df = pd.read_excel(TRN_STOP_INFO_FILE, header=None, names=["Node", "Station"])
        logging.info(f"Read {len(stop_info_df):,} lines from {TRN_STOP_INFO_FILE}")

        # only want node numbers and names
        stop_info_dict = stop_info_df[["Node","Station"]].to_dict(orient='list')
        stops_to_station = dict(zip(stop_info_dict["Node"],
                                    stop_info_dict["Station"]))
        logging.debug("stops_to_station: {stops_to_station}")

    trn_net = Wrangler.TransitNetwork(modelType="TravelModelOne", modelVersion=1.5)
    trn_net.parseFile(fullfile=str(LINE_FILE))
    logging.info(f"Read trn_net: {trn_net}")

    # read the loaded volume dbfs
    Wrangler.TransitNetwork.initializeTransitCapacity(directory=LINE_FILE.parent)
    tads = {}
    if LOADVOL_DIR:
        for timeperiod in TIMEPERIOD_DURATIONS.keys():
            loadvol_file = LOADVOL_DIR / f"trnlink{timeperiod}_ALLMSA.dbf"
            tads[timeperiod] = Wrangler.TransitAssignmentData(timeperiod=timeperiod,
                                                              modelType=Wrangler.Network.MODEL_TYPE_TM1,
                                                              ignoreModes=[1,2,3,4,5,6,7],
                                                              tpfactor="constant_with_peaked_muni",
                                                              transitCapacity=Wrangler.TransitNetwork.capacity,
                                                              lineLevelAggregateFilename=loadvol_file)
            logging.info(f"Read {len(tads[timeperiod].trnAsgnTable):,} rows from {loadvol_file}")

    # build lines and links
    line_count = 0
    link_count = 0

    line_group_pattern = re.compile(r"([A-Z0-9]+_[A-Z0-9]+)((EB|WB|NB|SB|AM|PM)?[A-Z0-9])*")

    all_lines = trn_net.line(re.compile(".*"))
    # if the line is two way, make sure we add the reverse
    reverse_lines = []
    for line in all_lines:
        if line.isOneWay() == False:
            rev_line = copy.deepcopy(line)
            rev_line.reverse() 
            rev_line.name = rev_line.name[:-1] + "-" # make the last character - rather than R
            logging.debug(f"Adding reverse line {rev_line}")
            reverse_lines.append(rev_line)
    all_lines.extend(reverse_lines)

    # sort the lines by the line name
    def get_line_name(line):
        return line.name

    all_lines.sort(key=get_line_name)

    total_line_count = len(all_lines)

    for line in all_lines:

        # if we read the transit crowding file, some lines (pseudo lines) may have been aggregated into their actual counterparts (see transitcrowding.py)
        # so if they're omitted from that file, omit them here
        if TRANSIT_CROWDING_FILE and line.name.upper() not in transit_crowding_lines:
            logging.warn("Skipping line [{line.name}] because it was removed in transit crowding processing")
            continue

        line_point_array = []
        link_point_array = []
        op_txt           = "unknown_op"
        mode_name        = "unknown_mode"
        mode_num         = int(line.attr['MODE'])
        if mode_num in MODE_NUM_TO_NAME:
            mode_name = MODE_NUM_TO_NAME[mode_num][0]
            op_txt    = MODE_NUM_TO_NAME[mode_num][1]
        else:
            logging.warn("MODE not recognized: {}".format(line.attr['MODE']))

        mode_type = "Commuter Rail"
        if mode_num < 10:
            mode_type = "Support"
        elif mode_num < 80:
            mode_type = "Local Bus"
        elif mode_num < 100:
            mode_type = "Express Bus"
        elif mode_num < 110:
            mode_type = "Ferry"
        elif mode_num < 120:
            mode_type = "Light Rail"
        elif mode_num < 130:
            mode_type = "Heavy Rail"

        # figure out the name set
        name_set  = get_name_set(line.name, mode_type)

        if by_operator:
            operator_file = "_{}".format(op_txt)
        else:
            operator_file = ""

        logging.info(f"Adding line {line_count+1:4}/{total_line_count:4} {line.name:15} set {name_set:15} operator {op_txt:15} to operator_file [{operator_file}]")
        # for attr_key in line.attr: print(attr_key, line.attr[attr_key])

        # keep information about first and last nodes
        first_n     = -1
        first_name  = "not_set"
        first_point = None
        second_n    = -1
        last_n      = -1
        last_station= "not_set"
        last_point  = None
        last_is_stop= True
        seq         = 1
        stop_b_row  = None

        # and vehicle type, capacity information
        vehtypes   = collections.OrderedDict([("EA","NA"), ("AM","NA"), ("MD","NA"), ("PM","NA"), ("EV","NA")])
        vehcaps    = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])
        pdcaps     = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])
        seatcaps   = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])
        pdseatcaps = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])

        for node in line.n:
            n = abs(int(node.num))
            station = stops_to_station[n] if n in stops_to_station else ""
            is_stop = 1 if node.isStop() else 0

            # first link - create line_abnameseq
            if first_n > 0 and second_n < 0:
                second_n   = n
                line_abnameseq  = "{} {} {} 1".format(first_n, second_n, line.name).encode()  # byte string

                # lookup vehicle type, vehicle capacity and period capacity
                if LOADVOL_DIR:

                    # if transit crowding file has been read, that overrides
                    cr_line_dict = None
                    if line.name.upper() in crowding_line_dict:
                        cr_line_dict = crowding_line_dict[line.name.upper()]

                    for timeperiod in TIMEPERIOD_DURATIONS.keys():
                        if line_abnameseq in tads[timeperiod].trnAsgnTable:
                            vehtypes[timeperiod] = tads[timeperiod].trnAsgnTable[line_abnameseq]["VEHTYPE"]
                            vehcaps[timeperiod]  = tads[timeperiod].trnAsgnTable[line_abnameseq]["VEHCAP"]
                            pdcaps[timeperiod]   = tads[timeperiod].trnAsgnTable[line_abnameseq]["PERIODCAP"]

                        # override from transit crowding
                        if cr_line_dict:
                            vehtypes[timeperiod]   = cr_line_dict[("veh_type_updated", timeperiod)] if cr_line_dict[("veh_type_updated", timeperiod)] else "NA"
                            vehcaps[timeperiod]    = cr_line_dict[("standcap",         timeperiod)]
                            pdcaps[timeperiod]     = cr_line_dict[("period_standcap",  timeperiod)]
                            seatcaps[timeperiod]   = cr_line_dict[("seatcap",          timeperiod)]
                            pdseatcaps[timeperiod] = cr_line_dict[("period_seatcap",   timeperiod)]
            if first_n < 0:
                first_n    = n
                first_name = station
                first_point = (node_dicts["X"][n], node_dicts["Y"][n])

            # print(node.num, n, node.attr, node.stop)
            point = shapely.geometry.Point( node_dicts["X"][n], node_dicts["Y"][n] )

            # get stop B ready
            stop_b_row = {
                'LINE_NAME':line.name,
                'geometry' :point,
                'STATION'  :station,
                'N'        :n,
                'SEQ'      :seq,
                'IS_STOP'  :is_stop
            }

            # add to line array
            line_point_array.append(point)

            # and link array
            link_point_array.append(point)

            if len(link_point_array) > 1:
                link_row = {
                    'NAME'     :line.name, 
                    'geometry' :shapely.geometry.LineString(link_point_array),
                    'A'        :last_n,
                    'B'        :n,
                    'A_STATION':last_station,
                    'B_STATION':station,
                    'SEQ'      :seq-1,
                    'NAMESEQAB':f"{line.name} {seq-1} {last_n} {n}",
                    'MODE'     :mode_num,
                    'MODE_NAME':mode_name,
                    'MODE_TYPE':mode_type
                }
                # add stop A (last stop)
                stop_row = {
                    'LINE_NAME':line.name, 
                    'geometry' :shapely.geometry.Point(last_point[0], last_point[1]), 
                    'STATION'  :last_station, 
                    'N'        :last_n,
                    'SEQ'      :seq-1,
                    'IS_STOP'  :last_is_stop
                }
                
                if LOADVOL_DIR:
                    # and vehicle type, capacity information
                    abvols      = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])
                    loads_stand = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])
                    loads_seat  = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])
                    brdas       = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])
                    xitas       = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])
                    xitbs       = collections.OrderedDict([("EA",  0 ), ("AM",  0 ), ("MD",  0 ), ("PM",  0 ), ("EV",  0 )])

                    abnameseq = f"{last_n} {n} {line.name} {seq-1}".encode()  # byte string

                    # if transit crowding file has been read, that overrides
                    cr_link_dict = None
                    if (line.name.upper(),last_n,n,seq-1) in crowding_link_dict:
                        cr_link_dict = crowding_link_dict[(line.name.upper(),last_n,n,seq-1)]

                    for timeperiod in TIMEPERIOD_DURATIONS.keys():
                        if abnameseq in tads[timeperiod].trnAsgnTable:
                            abvols[timeperiod]      = tads[timeperiod].trnAsgnTable[abnameseq]["AB_VOL"]
                            loads_stand[timeperiod] = tads[timeperiod].trnAsgnTable[abnameseq]["LOAD"]
                            brdas[timeperiod]       = tads[timeperiod].trnAsgnTable[abnameseq]["AB_BRDA"]
                            xitas[timeperiod]       = tads[timeperiod].trnAsgnTable[abnameseq]["AB_XITA"]
                            xitbs[timeperiod]       = tads[timeperiod].trnAsgnTable[abnameseq]["AB_XITB"]

                        # override from transit crowding
                        if cr_link_dict:
                            abvols[timeperiod]       = cr_link_dict[("AB_VOL",        timeperiod)]
                            loads_stand[timeperiod]  = cr_link_dict[("load_standcap", timeperiod)]
                            loads_seat[timeperiod]   = cr_link_dict[("load_seatcap",  timeperiod)]
                            brdas[timeperiod]        = cr_link_dict[("AB_BRDA",       timeperiod)]
                            xitas[timeperiod]        = cr_link_dict[("AB_XITA",       timeperiod)]
                            xitbs[timeperiod]        = cr_link_dict[("AB_XITB",       timeperiod)]

                    link_row.update(dict(zip(list(f'AB_VOL_{x}' for x in TIMEPERIOD_DURATIONS.keys()),
                                             list(abvols.values()))))
                    link_row.update(dict(zip(list(f'LOAD_{x}' for x in TIMEPERIOD_DURATIONS.keys()),
                                             list(loads_stand.values()))))
                    # seated capacity
                    if TRANSIT_CROWDING_FILE: 
                        link_row.update(dict(zip(list(f'LOADSE_{x}' for x in TIMEPERIOD_DURATIONS.keys()),
                                                 list(loads_seat.values()))))

                    stop_row.update(dict(zip(list(f'BRD_{x}' for x in TIMEPERIOD_DURATIONS.keys()),
                                             list(brdas.values()))))
                    stop_row.update(dict(zip(list(f'XIT_{x}' for x in TIMEPERIOD_DURATIONS.keys()),
                                             list(xitas.values()))))
                    stop_b_row.update(dict(zip(list(f'BRD_{x}' for x in TIMEPERIOD_DURATIONS.keys()),
                                               [0,0,0,0,0])))
                    stop_b_row.update(dict(zip(list(f'XIT_{x}' for x in TIMEPERIOD_DURATIONS.keys()),
                                               list(xitbs.values()))))

                    if len(trnlink_recs) > 0:
                        if (last_n,n,line.name,mode_num) in trnlink_recs:
                            rec = trnlink_recs[(last_n,n,line.name,mode_num)]
                            link_row.update({
                               'DIST_EA':rec["distance_ea"],
                               'DIST_AM':rec["distance_am"],
                               'DIST_MD':rec["distance_md"],
                               'DIST_PM':rec["distance_pm"],
                               'DIST_EV':rec["distance_ev"]})
                            link_row.update({
                                'TIME_EA':rec["time_ea"],
                                'TIME_AM':rec["time_am"],
                                'TIME_MD':rec["time_md"],
                                'TIME_PM':rec["time_pm"],
                                'TIME_EV':rec["time_ev"]})
                        else:
                            # link_row.extend([0,0,0,0,0])
                            # link_row.extend([0,0,0,0,0])
                            pass
                           
                # for each link, add stop A to stops
                if operator_file not in stop_dict_list.keys(): stop_dict_list[operator_file] = []
                stop_dict_list[operator_file].append(stop_row)

                if operator_file not in link_dict_list.keys(): link_dict_list[operator_file] = []
                link_dict_list[operator_file].append(link_row)
                # save the link data for aggregation
                agg_link_rows_item = {
                    'A'         :last_n,
                    'A_X'       :last_point[0],
                    'A_Y'       :last_point[1],
                    'A_STATION' :last_station,
                    'B'         :n,
                    'B_X'       :node_dicts["X"][n], 
                    'B_Y'       :node_dicts["Y"][n],
                    'B_STATION' :station,
                    'NAME'      :line.name,
                    'NAME_SET'  :name_set,
                    'MODE'      :line.attr['MODE'],
                    'MODE_NAME' :mode_name,
                    'MODE_TYPE' :mode_type,
                    'OPERATOR_T':op_txt,
                    # trips per time period
                    'TRIPS_EA'  :TIMEPERIOD_DURATIONS["EA"]*60.0/float(line.attr['FREQ[1]']) if float(line.attr['FREQ[1]'])>0 else 0,
                    'TRIPS_AM'  :TIMEPERIOD_DURATIONS["AM"]*60.0/float(line.attr['FREQ[2]']) if float(line.attr['FREQ[2]'])>0 else 0,
                    'TRIPS_MD'  :TIMEPERIOD_DURATIONS["MD"]*60.0/float(line.attr['FREQ[3]']) if float(line.attr['FREQ[3]'])>0 else 0,
                    'TRIPS_PM'  :TIMEPERIOD_DURATIONS["PM"]*60.0/float(line.attr['FREQ[4]']) if float(line.attr['FREQ[4]'])>0 else 0,
                    'TRIPS_EV'  :TIMEPERIOD_DURATIONS["EV"]*60.0/float(line.attr['FREQ[5]']) if float(line.attr['FREQ[5]'])>0 else 0
                }
                if LOADVOL_DIR:
                    agg_link_rows_item.update({
                        # period cap
                        'PDCAP_EA':pdcaps["EA"],
                        'PDCAP_AM':pdcaps["AM"],
                        'PDCAP_MD':pdcaps["MD"],
                        'PDCAP_PM':pdcaps["PM"],
                        'PDCAP_EV':pdcaps["EV"],
                        # abvols
                        'ABVOL_EA':abvols["EA"],
                        'ABVOL_AM':abvols["AM"],
                        'ABVOL_MD':abvols["MD"],
                        'ABVOL_PM':abvols["PM"],
                        'ABVOL_EV':abvols["EV"]
                    })
                    if TRANSIT_CROWDING_FILE:
                         agg_link_rows_item.update({
                             'PDSECAP_EA':pdseatcaps["EA"],
                             'PDSECAP_AM':pdseatcaps["AM"],
                             'PDSECAP_MD':pdseatcaps["MD"],
                             'PDSECAP_PM':pdseatcaps["PM"],
                             'PDSECAP_EV':pdseatcaps["EV"]
                        })
                agg_link_dict_list.append( agg_link_rows_item )

                link_count += 1
                link_point_array = [point]

            last_n       = n
            last_station = station
            last_point   = (node_dicts["X"][n], node_dicts["Y"][n])
            last_is_stop = is_stop

            seq += 1

        # last stop still needs to be added
        stop_dict_list[operator_file].append(stop_b_row)

        line_row = {
            'NAME'      :line.name,
            'NAME_SET'  :name_set,
            'LONG_NAME' :line.attr["LONGNAME"] if "LONGNAME" in line.attr else "", 
            'geometry'  :shapely.geometry.LineString(line_point_array),
            'FREQ_EA'   :float(line.attr['FREQ[1]']),
            'FREQ_AM'   :float(line.attr['FREQ[2]']),
            'FREQ_MD'   :float(line.attr['FREQ[3]']),
            'FREQ_PM'   :float(line.attr['FREQ[4]']),
            'FREQ_EV'   :float(line.attr['FREQ[5]']),
            'ONEWAY'    :1 if line.isOneWay() else 0,
            'MODE'      :line.attr['MODE'],
            'MODE_NAME' :mode_name,
            'MODE_TYPE' :mode_type,
            'OPERATOR_T':op_txt, # operator
            'FIRST_N'   :first_n,
            'FIRST_NAME':first_name,
            'LAST_N'    :last_n,
            'LAST_NAME' :last_station,
            'N_OR_S'    :"N" if last_point[1] > first_point[1] else "S",
            'E_OR_W'    :"E" if last_point[0] > first_point[0] else "W"
        }

        if LOADVOL_DIR:
            line_row.update(dict(zip(list(list(f'VEHTYPE_{x}' for x in TIMEPERIOD_DURATIONS.keys())),
                                     list(vehtypes.values()))))
            line_row.update(dict(zip(list(list(f'VEHCAP_{x}' for x in TIMEPERIOD_DURATIONS.keys())),
                                     list(vehcaps.values()))))
            line_row.update(dict(zip(list(list(f'PDCAP_{x}' for x in TIMEPERIOD_DURATIONS.keys())),
                                     list(pdcaps.values()))))
        if TRANSIT_CROWDING_FILE:
            line_row.update(dict(zip(list(list(f'SEATCAP_{x}' for x in TIMEPERIOD_DURATIONS.keys())),
                                     list(seatcaps.values()))))
            line_row.update(dict(zip(list(list(f'PDSECAP_{x}' for x in TIMEPERIOD_DURATIONS.keys())),
                                     list(pdseatcaps.values()))))

        if operator_file not in line_dict_list.keys(): line_dict_list[operator_file] = []
        line_dict_list[operator_file].append(line_row)
        line_count += 1

    # add support links since they're not added above
    support_df = trnlink_df.reset_index(drop=False)
    if len(support_df) > 0:
        support_df = support_df.loc[ support_df["mode"]<10]

    support_recs = support_df.to_dict(orient="records")
    for support_link in support_recs:

        station_a = stops_to_station[support_link["A"]] if support_link["A"] in stops_to_station else ""
        station_b = stops_to_station[support_link["B"]] if support_link["B"] in stops_to_station else ""

        mode_name = MODE_NUM_TO_NAME[support_link['mode']][0]
        mode_type = mode_name
        point_a   = shapely.geometry.Point(node_dicts["X"][support_link["A"]], node_dicts["Y"][support_link["A"]])
        point_b   = shapely.geometry.Point(node_dicts["X"][support_link["B"]], node_dicts["Y"][support_link["B"]])

        link_row = {
            'NAME'     : support_link["name"], 
            'geometry' : shapely.geometry.LineString([point_a, point_b]),
            'A'        :support_link["A"],
            'B'        :support_link["B"],
            'A_STATION':station_a,
            'B_STATION':station_b,
            'SEQ'      :-1,
            'NAMESEQAB':f'{support_link["name"]} {-1} {support_link["A"]} {support_link["B"]}',
            'MODE'     :support_link["mode"],
            'MODE_NAME':mode_name,
            'MODE_TYPE':mode_type
        }
        link_row.update({
            'AB_VOL_EA':support_link["AB_VOL_ea"],
            'AB_VOL_AM':support_link["AB_VOL_am"],
            'AB_VOL_MD':support_link["AB_VOL_md"],
            'AB_VOL_PM':support_link["AB_VOL_pm"],
            'AB_VOL_EV':support_link["AB_VOL_ev"],
            'DIST_EA':support_link["distance_ea"],
            'DIST_AM':support_link["distance_am"],
            'DIST_MD':support_link["distance_md"],
            'DIST_PM':support_link["distance_pm"],
            'DIST_EV':support_link["distance_ev"],
            'TIME_EA':support_link["time_ea"],
            'TIME_AM':support_link["time_am"],
            'TIME_MD':support_link["time_md"],
            'TIME_PM':support_link["time_pm"],
            'TIME_EV':support_link["time_ev"],
        })

        link_dict_list[operator_file].append(link_row)

    # write the shapefiles out
    # crs is http://spatialreference.org/ref/epsg/nad83-utm-zone-10n/
    for operator_file in stop_dict_list.keys():
        stops_gdf = gpd.GeoDataFrame( pd.DataFrame(stop_dict_list[operator_file]), geometry='geometry', crs="EPSG:26910")
        logging.debug(f"stops_gdf[{operator_file}]:\n{stops_gdf}")
        stops_gdf.to_file(TRN_STOPS_SHPFILE.format(operator_file))
        logging.info(f"Wrote {len(stops_gdf):,} stops to {TRN_STOPS_SHPFILE.format(operator_file)}")

        lines_gdf = gpd.GeoDataFrame( pd.DataFrame(line_dict_list[operator_file]), geometry='geometry', crs="EPSG:26910")
        logging.debug(f"lines_gdf[{operator_file}]:\n{lines_gdf}")
        lines_gdf.to_file(TRN_LINES_SHPFILE.format(operator_file))
        logging.info(f"Wrote {len(lines_gdf):,} lines to {TRN_LINES_SHPFILE.format(operator_file)}")

        links_gdf = gpd.GeoDataFrame( pd.DataFrame(link_dict_list[operator_file]), geometry='geometry', crs="EPSG:26910")
        logging.debug(f"links_gdf[{operator_file}]:\n{links_gdf}")
        links_gdf.to_file(TRN_LINKS_SHPFILE.format(operator_file))
        logging.info(f"Wrote {len(links_gdf):,} links and {len(support_df):,} support links to {TRN_LINKS_SHPFILE.format(operator_file)}")

    # aggregate link level data for network_trn_route_links.shp
    agg_links_df = pd.DataFrame(agg_link_dict_list)
    agg_links_df["LINE_COUNT"] = 1
    logging.debug(f"agg_links_df=\n{agg_links_df.head(20)}")

    # aggregate by A,B,MODE,MODE_NAME,MODE_TYPE,OPERATOR_T,NAME_SET
    agg_links_df_GB = agg_links_df.groupby(by=["A","B","A_STATION","B_STATION","NAME_SET","MODE","MODE_NAME","MODE_TYPE","OPERATOR_T"])
    agg_dict = {
        "A_X":"first", 
        "A_Y":"first",
        "B_X":"first",
        "B_Y":"first",
        "LINE_COUNT":"sum",
        "TRIPS_EA":"sum",
        "TRIPS_AM":"sum",
        "TRIPS_MD":"sum",
        "TRIPS_PM":"sum",
        "TRIPS_EV":"sum",
    }
    if LOADVOL_DIR:
        agg_dict.update({
            "PDCAP_EA":"sum",
            "PDCAP_AM":"sum",
            "PDCAP_MD":"sum",
            "PDCAP_PM":"sum",
            "PDCAP_EV":"sum",
            "ABVOL_EA":"sum",
            "ABVOL_AM":"sum",
            "ABVOL_MD":"sum",
            "ABVOL_PM":"sum",
            "ABVOL_EV":"sum",
        })
        if TRANSIT_CROWDING_FILE:
            agg_dict.update({
                "PDSECAP_EA":"sum",
                "PDSECAP_AM":"sum",
                "PDSECAP_MD":"sum",
                "PDSECAP_PM":"sum",
                "PDSECAP_EV":"sum",
            })

    agg_links_df    = agg_links_df_GB.agg(agg_dict).reset_index()
    agg_links_df["ROUTE_A_B"] = agg_links_df["NAME_SET"] + " " + agg_links_df["A"].astype(str) + "_" + agg_links_df["B"].astype(str)

    if LOADVOL_DIR:
        for timeperiod in TIMEPERIOD_DURATIONS.keys():
            agg_links_df["LOAD_{}".format(timeperiod)] = 0.0
            agg_links_df.loc[ agg_links_df["PDCAP_{}".format(timeperiod)] > 0, "LOAD_{}".format(timeperiod)] = \
                agg_links_df["ABVOL_{}".format(timeperiod)] / agg_links_df["PDCAP_{}".format(timeperiod)]
        if TRANSIT_CROWDING_FILE:
            for timeperiod in TIMEPERIOD_DURATIONS.keys():
                agg_links_df["LOADSE_{}".format(timeperiod)] = 0.0
                agg_links_df.loc[ agg_links_df["PDSECAP_{}".format(timeperiod)] > 0, "LOADSE_{}".format(timeperiod)] = \
                    agg_links_df["ABVOL_{}".format(timeperiod)] / agg_links_df["PDSECAP_{}".format(timeperiod)]

    agg_links_df['geometry'] = agg_links_df.apply(lambda row: shapely.geometry.LineString([
        (row['A_X'], row['A_Y']),
        (row['B_X'], row['B_Y'])
    ]), axis=1)
    # we're done with these
    agg_links_df.drop(columns=['A_X','A_Y','B_X','B_Y'], inplace=True)
    logging.debug(f"agg_links_df\n{agg_links_df.head(20)}")
    agg_links_gdf = gpd.GeoDataFrame(agg_links_df, geometry='geometry', crs="EPSG:26910")
    agg_links_gdf.to_file(TRN_ROUTE_LINKS_SHPFILE)
    logging.info(f"Wrote {len(agg_links_gdf):,} links to {TRN_ROUTE_LINKS_SHPFILE}")

if __name__ == '__main__':
    pd.options.display.width = 1000
    # pd.options.display.max_rows = 1000
    pd.options.display.max_columns = None

    parser = argparse.ArgumentParser(description=USAGE, formatter_class=argparse.RawDescriptionHelpFormatter,)
    parser.add_argument("netfile",  metavar="network.net", help="Cube input roadway network file")
    parser.add_argument("--outdir", help="Output directory.  Assumes current dir if not specified.")
    parser.add_argument("--linefile", metavar="transit.lin", help="Cube input transit line file", required=False)
    parser.add_argument("--by_operator", action="store_true", help="Split transit lines by operator")
    parser.add_argument("--trn_stop_info", metavar="transit_stops.xlsx", help="Workbook with extra transit stop information")
    parser.add_argument("--loadvol_dir", help="Directory with loaded volume files for joining")
    parser.add_argument("--transit_crowding", help="Transit crowding link file for joining. If this argument is specified, then specifying a loadvol_dir is also required.")
    args = parser.parse_args()
    # print(args)

    NETFILE               = pathlib.Path(args.netfile)
    LINE_FILE             = pathlib.Path(args.linefile) if args.linefile else None
    TRN_STOP_INFO_FILE    = pathlib.Path(args.trn_stop_info) if args.trn_stop_info else None
    LOADVOL_DIR           = pathlib.Path(args.loadvol_dir) if args.loadvol_dir else None
    TRANSIT_CROWDING_FILE = pathlib.Path(args.transit_crowding) if args.transit_crowding else None

    if TRANSIT_CROWDING_FILE and not LOADVOL_DIR:
        print(USAGE)
        print("\n\nFATAL: If --transit_crowding argument is specified, then specifying a loadvol_dir is also required.\n\n")
        sys.exit(2)

    # if output dir specified, normalize all other paths first and then switch to it
    if args.outdir:
        WORKING_DIR = pathlib.Path(args.outdir).absolute()
        NETFILE     = NETFILE.absolute()

        if LINE_FILE:
            LINE_FILE = LINE_FILE.absolute()
        if TRN_STOP_INFO_FILE:
            TRN_STOP_INFO_FILE = TRN_STOP_INFO_FILE.absolute()
        if LOADVOL_DIR:
            LOADVOL_DIR = LOADVOL_DIR.absolute()
        if TRANSIT_CROWDING_FILE:
            TRANSIT_CROWDING_FILE = TRANSIT_CROWDING_FILE.absolute()

        os.chdir( WORKING_DIR )
    else:
        # assume current directory
        WORKING_DIR = pathlib.Path.cwd()

    # create logger
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    # console handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p'))
    logger.addHandler(ch)
    # file handler
    fh = logging.FileHandler(LOG_FILE, mode='w')
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p'))
    logger.addHandler(fh)

    cube_network_to_shapefiles(
        NETFILE,
        LINE_FILE,
        TRN_STOP_INFO_FILE,
        LOADVOL_DIR,
        TRANSIT_CROWDING_FILE,
        WORKING_DIR,
        args.by_operator
    )