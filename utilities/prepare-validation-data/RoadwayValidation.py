USAGE = """

 Creates a CSV for validating roadway volumes compared to PeMS, PeMS truck census, or CalTrans count data.

 Input:
========
1) .\\avgload5period[_vehclasses].csv: model data
   Required columns: a, b, lanes
   For trucks:         vol[EA,AM,MD,PM,EV]_[sm,smt,hv,hvt]
   For PeMS, Caltrans: vol[EA,AM,MD,PM,EV]_tot

If PeMS years specified:

p2) M:\Crosswalks\PeMSStations_TM1network\crosswalk_2015.csv: maps model roadway links to PeMS stations
    Required columns: a, b, station, HOV

p3) (Box: Share Data\pems-typical-weekday\pems_period.csv): PeMS observed data, generated by
    https://github.com/BayAreaMetro/pems-typical-weekday
    Required columns: station, route, direction, time_period, 
                      lanes, median_flow, avg_flow, abs_pm, latitude, longitude, year

If CalTrans years specified:

c2) model_to_caltrans.csv: maps model roadway links to CalTrans count locations
    Required columns: a, b, county, route, postmile, direction, leg, description

c3) (Box: Share Data\caltrans-typical-weekday\\typical-weekday-counts.csv): CalTrans observed data,
    generated by https://github.com/BayAreaMetro/caltrans-typical-weekday-counts

If Truck cnesus years specified:

t2) M:\Crosswalks\PeMSStations_TM1network\\truck_census_stations_manual.csv: maps truck census
    locations to model roadway links

t3) (Box: Share Data\pems-typical-weekday\pems_truck_period.csv): PeMS Truck Census data,
    generated by https://github.com/BayAreaMetro/pems-typical-weekday

Output:
========

If PeMS years specified:

p1) Roadways to PeMS.csv: dataframe containing both modeled and observed data where each row is one volume
    (e.g. observed OR modeled)

    Columns: a, b, A_B, at, ft, county, sep_HOV,                              (model link)
             district, station, route, direction, type,                       (PeMS station)
             abs_pm, latitude, longitude,                                     (PeMS station location)
             link_count, pemsonlink, distlink, lanes match,                   (matching info)
             time_period, lanes, volume, category                             (volumes)
    Where category is one of [(year) Modeled, (year) Observed]

    Note: PeMS data without model links are in this dataset with a,b=-1

p2) Roadways to PeMS_wide.csv: dataframe containing both modeled and observed data where each row is one set
    of volumes (observed AND modeled)

    Columns: a, b, A_B, at, ft, county, sep_HOV, lanes modeled,               (model link)
             district, station, route, direction, type, lanes observed,       (PeMS station)
             abs_pm, latitude, longitude,                                     (PeMS station location)
             link_count, pemsonlink, distlink, lanes match,                   (matching info)
             time_period, 2015 Modeled, 2014 Observed, 2015 Observed, 2016 Observed, Average Observed


If CalTrans years specified:

"""
import argparse, os, sys
import pandas

TIMEPERIODS = ["EA", "AM", "MD", "PM", "EV"]
TM_HOV_TO_GP_FILE = "M:\Crosswalks\PeMSStations_TM1network\hov_to_gp_links.csv"
PEMS_MAP_FILE = "M:\Crosswalks\PeMSStations_TM1network\crosswalk_2015.csv"
TRUCK_MAP_FILE = (
    "M:\Crosswalks\PeMSStations_TM1network\\truck_census_stations_manual.csv"
)
CALTRANS_MAP_FILE = "M:\Crosswalks\CaltransCountLocations_TM1network\\typical-weekday-counts-xy-TM1link.csv"
MODEL_FILE = "avgload5period.csv"
MODEL_VCLASS_FILE = "avgload5period_vehclasses.csv"

SHARE_DATA = os.path.join(
    os.environ["USERPROFILE"], "Box", "Modeling and Surveys", "Share Data"
)
if os.environ["USERNAME"] == "lzorn":
    SHARE_DATA = os.path.join("E:\\", "Box", "Modeling and Surveys", "Share Data")

PEMS_FILE = os.path.join(SHARE_DATA, "pems-typical-weekday", "pems_period.csv")
CALTRANS_FILE = os.path.join(
    SHARE_DATA, "caltrans-typical-weekday", "typical-weekday-counts.csv"
)
TRUCK_FILE = os.path.join(SHARE_DATA, "pems-typical-weekday", "pems_truck_period.csv")
PEMS_OUTPUT_FILE = "Roadways to PeMS"
CALTRANS_OUTPUT_FILE = "Roadways to Caltrans"
TRUCK_OUTPUT_FILE = "Roadways to Truck Census"

MODEL_NONTRUCK_COLUMNS = ["a", "b", "ft", "at", "county", "lanes"] + [
    "vol{}_tot".format(timeperiod) for timeperiod in TIMEPERIODS
]
MODEL_TRUCK_COLUMNS = (
    ["a", "b", "ft", "at", "county", "lanes"]
    + ["vol{}_sm_tot".format(timeperiod) for timeperiod in TIMEPERIODS]
    + ["vol{}_hv_tot".format(timeperiod) for timeperiod in TIMEPERIODS]
)
PEMS_COLUMNS = [
    "station",
    "route",
    "direction",
    "time_period",
    "lanes",
    "avg_flow",
    "abs_pm",
    "latitude",
    "longitude",
    "year",
]

# these are bad crosswalk
PEMS_BAD_STATION_CROSSWALK = [401819, 401820]

if __name__ == "__main__":

    pandas.options.display.width = 1000
    pandas.options.display.max_rows = 1000
    pandas.options.display.max_columns = 35

    parser = argparse.ArgumentParser(
        description=USAGE, formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("-m", "--model_year", type=int, required=True)
    parser.add_argument("-c", "--caltrans_year", type=int, nargs="*")
    parser.add_argument("-p", "--pems_year", type=int, nargs="*")
    parser.add_argument("-t", "--truck_year", type=int, nargs="*")
    args = parser.parse_args()

    args_specified = 0
    if args.caltrans_year:
        args_specified += 1
    if args.pems_year:
        args_specified += 1
    if args.truck_year:
        args_specified += 1

    if args_specified == 0:
        print(USAGE)
        print("No PeMS year nor CalTrans year no Trucks year argument specified.")
        sys.exit()

    if args_specified != 1:
        print(USAGE)
        print("Please specify ONE of pems_year, caltrans_year or truck_year")
        sys.exit()
    print(args)

    ############ read the mapping first
    mapping_df = None
    out_file = None
    obs_cols = []
    if args.pems_year:
        mapping_df = pandas.read_csv(PEMS_MAP_FILE)
        model_file = MODEL_FILE
        model_cols = MODEL_NONTRUCK_COLUMNS
        out_file = PEMS_OUTPUT_FILE
        obs_cols = ["{} Observed".format(year) for year in args.pems_year]
    elif args.caltrans_year:
        mapping_df = pandas.read_csv(CALTRANS_MAP_FILE)
        mapping_df.rename(
            columns={"postmileValue": "post_mile", "routeNumber": "route"}, inplace=True
        )
        model_file = MODEL_FILE
        model_cols = MODEL_NONTRUCK_COLUMNS
        out_file = CALTRANS_OUTPUT_FILE
        obs_cols = ["{} Observed".format(year) for year in args.caltrans_year]
    else:
        mapping_df = pandas.read_csv(TRUCK_MAP_FILE)
        model_file = MODEL_VCLASS_FILE  # need vehicle classes for truck comparison
        model_cols = MODEL_TRUCK_COLUMNS
        out_file = TRUCK_OUTPUT_FILE
        obs_cols = ["{} Observed".format(year) for year in args.truck_year]

    print(
        "Read {} lines from mapping_df; head:\n{}".format(
            len(mapping_df), mapping_df.head()
        )
    )
    # column name for model year observed
    modelyear_observed = "{} Observed".format(args.model_year)

    ############ read the model data
    model_df = pandas.read_csv(model_file)

    # strip the column names
    col_rename = {}
    for colname in model_df.columns.values.tolist():
        col_rename[colname] = colname.strip()
    model_df.rename(columns=col_rename, inplace=True)
    model_df.rename(columns={"gl": "county"}, inplace=True)

    # truck processing requires a little aggregation
    if args.truck_year:
        # aggregate no-toll + toll
        for timeperiod in TIMEPERIODS:
            model_df["vol{}_sm_tot".format(timeperiod)] = (
                model_df["vol{}_sm".format(timeperiod)]
                + model_df["vol{}_smt".format(timeperiod)]
            )
            model_df["vol{}_hv_tot".format(timeperiod)] = (
                model_df["vol{}_hv".format(timeperiod)]
                + model_df["vol{}_hvt".format(timeperiod)]
            )

    # select only the columns we want
    model_df = model_df[model_cols]

    # add daily column
    if args.truck_year:
        model_df["Daily_sm"] = model_df[
            [
                "volEA_sm_tot",
                "volAM_sm_tot",
                "volMD_sm_tot",
                "volPM_sm_tot",
                "volEV_sm_tot",
            ]
        ].sum(axis=1)
        model_df["Daily_hv"] = model_df[
            [
                "volEA_hv_tot",
                "volAM_hv_tot",
                "volMD_hv_tot",
                "volPM_hv_tot",
                "volEV_hv_tot",
            ]
        ].sum(axis=1)
        model_cols = model_cols + ["Daily_sm", "Daily_hv"]
    else:
        model_df["Daily"] = model_df[
            ["volEA_tot", "volAM_tot", "volMD_tot", "volPM_tot", "volEV_tot"]
        ].sum(axis=1)
        model_cols = model_cols + [
            "Daily",
        ]

    print("model_cols: {}".format(model_cols))

    # the model data has a, b, lanes, vol*
    # but some of these links are HOV links and the volums should be summed to the same link as the non-hov link
    # read the hov -> gp mapping
    model_hov_to_gp_df = pandas.read_csv(TM_HOV_TO_GP_FILE)
    # keep only those where we succeeded finding GP for now
    model_hov_to_gp_df = model_hov_to_gp_df.loc[model_hov_to_gp_df.A_B_GP != "NA_NA"]
    # print(model_hov_to_gp_df.head())
    #        A      B  LANES  USE  FT  ROUTENUM ROUTEDIR    A_GP    B_GP         A_B     A_B_GP
    # 10  8900   8901      1    3   2       880        S  3400.0  3399.0   8900_8901  3400_3399
    # 12  8903   8904      1    3   2       880        S  3396.0  3416.0   8903_8904  3396_3416
    # 13  8904   8905      1    3   2       880        S  3416.0  3606.0   8904_8905  3416_3606
    # 14  8905  20229      1    3   2       880        S  3606.0  3603.0  8905_20229  3606_3603
    # 15  8907  20231      1    3   2       880        S  3601.0  3640.0  8907_20231  3601_3640

    # remove me eventually - compensate for bug where HOV link on bridge has wrong county coded
    # https://github.com/BayAreaMetro/TM1_2015_Base_Network/commit/8ecb3cae55616f7ac6fb2ebb2a3f134bd239a13d
    model_df.loc[(model_df.a == 20294) & (model_df.b == 10601), "county"] = 5
    model_df.loc[(model_df.a == 10601) & (model_df.b == 10607), "county"] = 5

    # join the model data to the hov -> gp mapping
    model_df = pandas.merge(
        left=model_df,
        right=model_hov_to_gp_df[["A", "B", "LANES", "USE", "A_GP", "B_GP"]],
        left_on=["a", "b"],
        right_on=["A", "B"],
        how="left",
    )
    # print("model_df hov links head\n{}".format(model_df.loc[ pandas.notnull(model_df.A_GP)].head()))
    # set those to HOV true and set the a,b to the GP versions
    model_df["sep_HOV"] = False
    model_df.loc[pandas.notnull(model_df.A_GP), "sep_HOV"] = True
    model_df.loc[pandas.notnull(model_df.A_GP), "a"] = model_df.A_GP
    model_df.loc[pandas.notnull(model_df.A_GP), "b"] = model_df.B_GP
    # drop other cols and make a,b back into int
    model_df = model_df[model_cols + ["sep_HOV"]]
    model_df["a"] = model_df["a"].astype(int)
    model_df["b"] = model_df["b"].astype(int)
    # now a,b isn't unique so group
    model_df["link_count"] = 1
    # facility types may not match since GP toll plazas have ft=6 so take min
    model_agg_dict = {
        "ft": "min",
        "link_count": "sum",
        "lanes": "sum",
        "sep_HOV": "sum",
    }
    # sum volumes
    for column in model_cols:
        if column.startswith("vol") or column.startswith("Daily"):
            model_agg_dict[column] = "sum"

    model_df = (
        model_df.groupby(["a", "b", "at", "county"]).agg(model_agg_dict).reset_index()
    )
    print("model_df head\n{}".format(model_df.loc[model_df.link_count > 1].head()))

    # create a multi index for stacking
    model_df.set_index(
        ["a", "b", "ft", "at", "county", "lanes", "sep_HOV", "link_count"], inplace=True
    )
    # stack: so now we have a series with multiindex: a,b,lanes,varname
    model_df = pandas.DataFrame({"volume": model_df.stack()})
    # reset the index
    model_df.reset_index(inplace=True)
    print("model_df head\n{}".format(model_df.head(12)))

    # and rename it - truck has truck class included
    if args.truck_year:
        model_df["truck_class"] = model_df["level_8"].str[6:8]  # 'sm' or 'hv'
        model_df.loc[model_df.level_8.str.startswith("Daily"), "level_8"] = "Daily"

    model_df.rename(columns={"level_8": "time_period"}, inplace=True)
    # remove extra chars: 'volAM_tot' => 'AM'
    model_df.loc[
        model_df["time_period"].str.startswith("vol"), "time_period"
    ] = model_df["time_period"].str[3:5]
    print("model_df head\n{}".format(model_df.head(12)))

    print(model_df.loc[(model_df.a == 11828) & (model_df.b == 8676)])

    if args.pems_year:
        ############ read the pems data
        obs_df = pandas.read_csv(PEMS_FILE, na_values="NA", engine="python")

        # select only the columns we want
        obs_df = obs_df[PEMS_COLUMNS]
        # select only the years in question
        obs_df = obs_df[obs_df["year"].isin(args.pems_year)].reset_index(drop=True)

        # create missing cols in PeMS
        obs_df.rename(columns={"avg_flow": "volume", "year": "category"}, inplace=True)
        print("obs_df len={} head\n{}".format(len(obs_df), obs_df.head(22)))
        #    station  route direction time_period  lanes        volume   abs_pm   latitude   longitude  category
        # 0    400001    101         N          AM      5  23462.250000  387.897  37.364085 -121.901149      2014
        # 1    400001    101         N          EA      5   7549.107143  387.897  37.364085 -121.901149      2014
        # 2    400001    101         N          EV      5   9162.789474  387.897  37.364085 -121.901149      2014
        # 3    400001    101         N          MD      5  18982.450000  387.897  37.364085 -121.901149      2014
        # 4    400001    101         N          PM      5  11771.904762  387.897  37.364085 -121.901149      2014

        obs_daily_df = obs_df.groupby(
            [
                "station",
                "route",
                "direction",
                "lanes",
                "category",
                "abs_pm",
                "longitude",
                "latitude",
            ]
        ).aggregate({"time_period": "count", "volume": "sum"})
        obs_daily_df.reset_index(inplace=True)
        assert len(obs_daily_df.loc[obs_daily_df.time_period > 5]) == 0
        # drop those with fewer than 5 time periods
        obs_daily_df = obs_daily_df.loc[obs_daily_df.time_period == 5]
        assert len(obs_daily_df.loc[obs_daily_df.time_period != 5]) == 0
        # add these
        obs_daily_df["time_period"] = "Daily"
        obs_df = pandas.concat(
            [obs_df, obs_daily_df], axis="index", sort=True
        )  # sort means sort columns first so they are aligned
        obs_df.sort_values(
            by=["station", "route", "direction", "lanes", "category", "time_period"],
            inplace=True,
        )
        obs_df.reset_index(drop=True, inplace=True)
        print("obs_df len={} head\n{}".format(len(obs_df), obs_df.head(22)))

        #      abs_pm  category direction  lanes   latitude   longitude  route  station time_period         volume
        # 0   387.897      2014         N      5  37.364085 -121.901149    101   400001          AM   23462.250000
        # 1   387.897      2014         N      5  37.364085 -121.901149    101   400001       Daily   70928.501378
        # 2   387.897      2014         N      5  37.364085 -121.901149    101   400001          EA    7549.107143
        # 3   387.897      2014         N      5  37.364085 -121.901149    101   400001          EV    9162.789474
        # 4   387.897      2014         N      5  37.364085 -121.901149    101   400001          MD   18982.450000
        # 5   387.897      2014         N      5  37.364085 -121.901149    101   400001          PM   11771.904762
        obs_df["category"] = obs_df.category.map(str) + " Observed"

        # want to bring category into columns
        obs_wide = pandas.pivot_table(
            obs_df,
            values="volume",
            index=[
                "station",
                "route",
                "direction",
                "abs_pm",
                "latitude",
                "longitude",
                "lanes",
                "time_period",
            ],
            columns="category",
        )
        obs_wide["Average Observed"] = obs_wide.mean(
            axis=1
        )  # this will not include NaNs or missing vals so it handles them correctly
        obs_wide.reset_index(inplace=True)

        print("obs_wide head\n{}".format(obs_wide.head(12)))
        # category  station  route direction   abs_pm       ...         2014 Observed  2015 Observed  2016 Observed Average Observed
        # 0          400001    101         N  387.897       ...          23462.250000   22879.842857   22948.736111     23096.942989
        # 1          400001    101         N  387.897       ...          70928.501378   72694.426221   72613.167349     72078.698316
        # 2          400001    101         N  387.897       ...           7549.107143    8072.062500    8537.581081      8052.916908
        # 3          400001    101         N  387.897       ...           9162.789474    9398.333333    9323.211268      9294.778025
        # 4          400001    101         N  387.897       ...          18982.450000   20523.112903   20314.888889     19940.150597
        # 5          400001    101         N  387.897       ...          11771.904762   11821.074627   11488.750000     11693.909796
        # 6          400002    101         S  416.893       ...          25756.980769            NaN            NaN     25756.980769
        # 7          400002    101         S  416.893       ...         117650.026125            NaN            NaN    117650.026125
        # 8          400002    101         S  416.893       ...           4034.363636            NaN            NaN      4034.363636
        # 9          400002    101         S  416.893       ...          26445.960000            NaN            NaN     26445.960000
        # 10         400002    101         S  416.893       ...          32385.192308            NaN            NaN     32385.192308
        # 11         400002    101         S  416.893       ...          29027.529412            NaN            NaN     29027.529412
    elif args.caltrans_year:
        ############ read the caltrans data
        obs_df = pandas.read_csv(CALTRANS_FILE)
        print(
            "Read {} rows from {}. head:\n{}".format(
                len(obs_df), CALTRANS_FILE, obs_df.head()
            )
        )

        # make columns conform to previous version and to model data
        obs_df.rename(columns={"county": "countyCode"}, inplace=True)

        # select the relevant years
        obs_df = obs_df.loc[obs_df.year.isin(args.caltrans_year)]

        # add station,description column to mapping_df -- and keep only the relevant entries since mapping_df includes stations for all years
        description_df = obs_df[
            ["route", "countyCode", "post_mile", "direction", "station", "description"]
        ].drop_duplicates()
        print(
            "locations with descriptions ({}) head:\n{}".format(
                len(description_df), description_df.head()
            )
        )
        mapping_df = pandas.merge(left=mapping_df, right=description_df, how="inner")

        id_vars = [
            "route",
            "countyCode",
            "post_mile",
            "leg",
            "direction",
            "station",
            "description",
        ]

        # set the time_period
        obs_df["time_period"] = "EV"
        obs_df.loc[
            (obs_df.integer_hour >= 3) & (obs_df.integer_hour < 6), "time_period"
        ] = "EA"
        obs_df.loc[
            (obs_df.integer_hour >= 6) & (obs_df.integer_hour < 10), "time_period"
        ] = "AM"
        obs_df.loc[
            (obs_df.integer_hour >= 10) & (obs_df.integer_hour < 15), "time_period"
        ] = "MD"
        obs_df.loc[
            (obs_df.integer_hour >= 15) & (obs_df.integer_hour < 19), "time_period"
        ] = "PM"

        # aggregate to time period and verify each is complete
        obs_df = (
            obs_df.groupby(id_vars + ["year", "time_period"])
            .aggregate(
                {
                    "integer_hour": "count",
                    "median_count": "sum",
                    "avg_count": "sum",
                    "days_observed": "mean",
                }
            )
            .reset_index()
        )
        print("obs_df len={} head\n{}".format(len(obs_df), obs_df.head(10)))

        obs_df = obs_df.loc[
            ((obs_df.time_period == "EA") & (obs_df.integer_hour == 3))
            | ((obs_df.time_period == "AM") & (obs_df.integer_hour == 4))
            | ((obs_df.time_period == "MD") & (obs_df.integer_hour == 5))
            | ((obs_df.time_period == "PM") & (obs_df.integer_hour == 4))
            | ((obs_df.time_period == "EV") & (obs_df.integer_hour == 8))
        ]
        print("obs_df len={} head\n{}".format(len(obs_df), obs_df.head(10)))

        # drop integer_hour count, median_count, days_observed -- retain sum of avg_count as the volume
        obs_df.drop(
            columns=["integer_hour", "median_count", "days_observed"], inplace=True
        )
        obs_df.rename(columns={"avg_count": "volume"}, inplace=True)
        print("obs_df len={} head\n{}".format(len(obs_df), obs_df.head(10)))

        # get Daily by year and add it
        obs_daily_df = obs_df.groupby(id_vars + ["year"]).aggregate(
            {"time_period": "count", "volume": "sum"}
        )
        # drop any that are incomplete
        print(
            "Dropping {} obs_daily_df rows for being incomplete".format(
                len(obs_daily_df.loc[obs_daily_df.time_period != 5])
            )
        )
        obs_daily_df = obs_daily_df.loc[obs_daily_df.time_period == 5]
        obs_daily_df["time_period"] = "Daily"
        obs_daily_df.reset_index(inplace=True)
        print(
            "obs_daily_df len={} head\n{}".format(
                len(obs_daily_df), obs_daily_df.head()
            )
        )
        #    route countyCode  post_mile leg direction  station                     description  year time_period        volume
        # 0      4         CC       11.4   B         E    912.0                    PACHECO BLVD  2016       Daily  41295.952381
        # 1      4         CC       11.4   B         W    912.0                    PACHECO BLVD  2016       Daily  45729.761905
        # 2     12        NAP        2.3   B         E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2014       Daily  18502.722816
        # 3     12        NAP        2.3   B         E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2015       Daily  19656.101695
        # 4     12        NAP        2.3   B         E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2016       Daily  20468.062500
        obs_df = pandas.concat(
            [obs_df, obs_daily_df], axis="index", sort=True
        )  # sort means sort columns first so they are aligned
        obs_df["category"] = obs_df.year.map(str) + " Observed"
        obs_df.drop(columns=["year"], inplace=True)
        print("obs_df len={} head\n{}".format(len(obs_df), obs_df.head()))

        # move category (year) to columns
        obs_wide = pandas.pivot_table(
            obs_df,
            index=id_vars + ["time_period"],
            columns=["category"],
            values="volume",
        )
        obs_wide["Average Observed"] = obs_wide.mean(
            axis=1
        )  # this will not include NaNs or missing vals so it handles them correctly
        obs_wide.reset_index(inplace=True)
        print("obs_wide len={} head\n{}".format(len(obs_wide), obs_wide.head()))

    elif args.truck_year:
        ############ read the truck census data
        obs_df = pandas.read_csv(TRUCK_FILE)
        print(obs_df.iloc[0])

        # select only the years in question and district 4
        obs_df = obs_df.loc[obs_df["year"].isin(args.truck_year)]
        obs_df = obs_df.loc[obs_df["District.Identifier"] == 4].reset_index(drop=True)

        # select only the columns we want
        TRUCK_COLUMNS = [
            "Census.Substation.Identifier",
            "Station.Type",
            "Freeway.Identifier",
            "Freeway.Direction",
            "Absolute.Postmile",
            "Latitude",
            "Longitude",
            "Name",
            "Vehicle.Class",
            "lanes",
            "year",
            "time_period",
            "avg_flow",
        ]
        obs_df = obs_df[TRUCK_COLUMNS]

        #### vehicle class processing (see PeMS truck vehicle classes https://app.asana.com/0/0/1203052150304041/f)
        # and https://www.fhwa.dot.gov/policyinformation/tmguide/tmg_2013/vehicle-types.cfm
        # we'll filter classes 7-13 as 4+ axle => heavy
        # all other we'll call small
        # 0 = all categories so drop that
        obs_df = obs_df.loc[obs_df["Vehicle.Class"] != 0].reset_index(drop=True)
        truck_vehicle_class_recode = {
            1: "sm",  # ?
            2: "sm",  # 8-20 ft
            3: "sm",  # 2 Axle, 4T SU
            4: "sm",  # 3 Axle SU
            5: "sm",  # 2 Axle, 6T SU
            6: "sm",  # 3 Axle SU
            7: "hv",  # 4+ Axle ST
            8: "hv",  # <4 Axle ST
            9: "hv",  # 5 Axle ST
            10: "hv",  # 6+ Axle ST
            11: "hv",  # <5 Axle MT
            12: "hv",  # 6 Axle MT
            13: "hv",  # 7+ Axle MT
            14: "sm",  # User-Def
            15: "sm",  # Unknown
        }
        obs_df["truck_class"] = obs_df["Vehicle.Class"].replace(
            to_replace=truck_vehicle_class_recode
        )
        print(obs_df[["Vehicle.Class", "truck_class"]].value_counts())

        # rename to standardized and create category
        obs_df.rename(
            columns={
                "avg_flow": "volume",
                "Census.Substation.Identifier": "station",
                "Station.Type": "type",
                "Freeway.Identifier": "route",
                "Freeway.Direction": "direction",
                "Absolute.Postmile": "abs_pm",
                "Latitude": "latitude",
                "Longitude": "longitude",
                "Name": "station description",
                "year": "category",
            },
            inplace=True,
        )
        print("obs_df len={} head\n{}".format(len(obs_df), obs_df.head(5)))
        # obs_df len=3010 head
        #    station type  route direction  abs_pm   latitude   longitude       station description  Vehicle.Class  lanes  category time_period      volume truck_class
        # 0  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          AM  138.761905          sm
        # 1  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          EA   35.904762          sm
        # 2  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          EV  245.230769          sm
        # 3  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          MD  198.272727          sm
        # 4  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          PM  147.545455          sm
        #
        # aggregate to truck_class
        obs_df = obs_df.groupby(
            [
                "station",
                "type",
                "route",
                "direction",
                "lanes",
                "category",
                "abs_pm",
                "longitude",
                "latitude",
                "truck_class",
                "time_period",
            ]
        ).aggregate({"volume": "sum"})
        obs_df.reset_index(inplace=True)

        # aggregate to daily to get daily sum
        obs_daily_df = obs_df.groupby(
            [
                "station",
                "type",
                "route",
                "direction",
                "lanes",
                "category",
                "abs_pm",
                "longitude",
                "latitude",
                "truck_class",
            ]
        ).aggregate({"time_period": "count", "volume": "sum"})
        obs_daily_df.reset_index(inplace=True)
        assert len(obs_daily_df.loc[obs_daily_df.time_period > 5]) == 0
        # drop those with fewer than 5 time periods
        obs_daily_df = obs_daily_df.loc[obs_daily_df.time_period == 5]
        assert len(obs_daily_df.loc[obs_daily_df.time_period != 5]) == 0

        # add these
        obs_daily_df["time_period"] = "Daily"
        obs_df = pandas.concat(
            [obs_df, obs_daily_df], axis="index", sort=True
        )  # sort means sort columns first so they are aligned
        obs_df.sort_values(
            by=[
                "station",
                "type",
                "route",
                "direction",
                "lanes",
                "category",
                "truck_class",
                "time_period",
            ],
            inplace=True,
        )
        obs_df.reset_index(drop=True, inplace=True)
        print("obs_df len={} head\n{}".format(len(obs_df), obs_df.head(12)))
        # obs_df len=516 head
        #     abs_pm  category direction  lanes   latitude   longitude  route  station time_period truck_class type        volume
        # 0   20.498      2014         E      4  37.991216 -122.309341     80  4902003          AM          hv   ML   1505.904762
        # 1   20.498      2014         E      4  37.991216 -122.309341     80  4902003       Daily          hv   ML   5738.010989
        # 2   20.498      2014         E      4  37.991216 -122.309341     80  4902003          EA          hv   ML    670.952381
        # 3   20.498      2014         E      4  37.991216 -122.309341     80  4902003          EV          hv   ML    866.153846
        # 4   20.498      2014         E      4  37.991216 -122.309341     80  4902003          MD          hv   ML   1983.454545
        # 5   20.498      2014         E      4  37.991216 -122.309341     80  4902003          PM          hv   ML    711.545455
        # 6   20.498      2014         E      4  37.991216 -122.309341     80  4902003          AM          sm   ML   2623.238095
        # 7   20.498      2014         E      4  37.991216 -122.309341     80  4902003       Daily          sm   ML  12028.665002
        # 8   20.498      2014         E      4  37.991216 -122.309341     80  4902003          EA          sm   ML    737.238095
        # 9   20.498      2014         E      4  37.991216 -122.309341     80  4902003          EV          sm   ML   1912.461538
        # 10  20.498      2014         E      4  37.991216 -122.309341     80  4902003          MD          sm   ML   3917.727273
        # 11  20.498      2014         E      4  37.991216 -122.309341     80  4902003          PM          sm   ML   2838.000000
        obs_df["category"] = obs_df.category.map(str) + " Observed"

        # want to bring category into columns
        obs_wide = pandas.pivot_table(
            obs_df,
            values="volume",
            index=[
                "station",
                "type",
                "route",
                "direction",
                "abs_pm",
                "latitude",
                "longitude",
                "lanes",
                "truck_class",
                "time_period",
            ],
            columns="category",
        )
        obs_wide["Average Observed"] = obs_wide.mean(
            axis=1
        )  # this will not include NaNs or missing vals so it handles them correctly
        obs_wide.reset_index(inplace=True)

        print("obs_wide len={} head\n{}".format(len(obs_wide), obs_wide.head(12)))
        # obs_wide len=180 head
        # category  station type  route direction  abs_pm   latitude   longitude  lanes truck_class time_period  2014 Observed  2015 Observed  2016 Observed  Average Observed
        # 0         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          AM    1505.904762    1521.461538    1482.666667       1503.344322
        # 1         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv       Daily    5738.010989    5901.516484    5577.844444       5739.123972
        # 2         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          EA     670.952381     739.076923     753.777778        721.269027
        # 3         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          EV     866.153846     898.285714     852.400000        872.279853
        # 4         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          MD    1983.454545    2014.153846    1903.777778       1967.128723
        # 5         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          PM     711.545455     728.538462     585.222222        675.102046
        # 6         4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          AM    2623.238095    2063.000000    2397.222222       2361.153439
        # 7         4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm       Daily   12028.665002    9184.538462   10282.666667      10498.623377
        # 8         4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          EA     737.238095     653.846154     820.222222        737.102157
        # 9         4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          EV    1912.461538    1281.000000    1792.000000       1661.820513
        # 10        4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          MD    3917.727273    3247.538462    3142.888889       3436.051541
        # 11        4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          PM    2838.000000    1939.153846    2130.333333       2302.495726

    # model has a, b, A_B
    obs_df["a"] = -1
    obs_df["b"] = -1
    obs_df["A_B"] = ""

    # create the final stacked table -- first the model information
    mapping_df.rename(columns={"A": "a", "B": "b"}, inplace=True)
    print("mapping_df head\n{}".format(mapping_df.head()))
    #    station  district  route direction type   latitude   longitude     a     b  distlink        A_B
    # 0   400001         4    101         N   ML  37.364085 -121.901149  5716  5690  0.000855  5716_5690
    # 1   400006         4    880         S   ML  37.605003 -122.065542  3715  3712  0.001245  3715_3712
    # 2   400007         4    101         N   ML  37.586936 -122.337721  6315  6409  0.001026  6315_6409
    # 3   400009         4     80         W   ML  37.864883 -122.303345  2512  2509  0.000422  2512_2509
    # 4   400010         4    101         N   ML  37.629765 -122.402365  6554  6567  0.000303  6554_6567
    # print("mapping_df cols:\n{}\nmodel_df cols:\n{}".format(mapping_df.dtypes, model_df.dtypes))
    model_final_df = pandas.merge(left=mapping_df, right=model_df, how="inner")
    model_final_df["category"] = "%d Modeled" % args.model_year
    print("model_final_df head\n{}".format(model_final_df.head(12)))

    # model_final_df head - pems
    #     station  district  route direction type   latitude   longitude     a     b  distlink        A_B  lanes  sep_HOV  link_count time_period    volume      category
    # 0    400001         4    101         N   ML  37.364085 -121.901149  5716  5690  0.000855  5716_5690    4.0     True           2          EA   4316.77  2015 Modeled
    # 1    400001         4    101         N   ML  37.364085 -121.901149  5716  5690  0.000855  5716_5690    4.0     True           2          AM  23606.98  2015 Modeled
    # 2    400001         4    101         N   ML  37.364085 -121.901149  5716  5690  0.000855  5716_5690    4.0     True           2          MD  18513.31  2015 Modeled
    # 3    400001         4    101         N   ML  37.364085 -121.901149  5716  5690  0.000855  5716_5690    4.0     True           2          PM  12764.99  2015 Modeled
    # 4    400001         4    101         N   ML  37.364085 -121.901149  5716  5690  0.000855  5716_5690    4.0     True           2          EV   5765.52  2015 Modeled
    # 5    400001         4    101         N   ML  37.364085 -121.901149  5716  5690  0.000855  5716_5690    4.0     True           2       Daily  64967.57  2015 Modeled
    # 6    400006         4    880         S   ML  37.605003 -122.065542  3715  3712  0.001245  3715_3712    4.0     True           2          EA   5642.29  2015 Modeled
    # 7    400006         4    880         S   ML  37.605003 -122.065542  3715  3712  0.001245  3715_3712    4.0     True           2          AM  25418.81  2015 Modeled
    # 8    400006         4    880         S   ML  37.605003 -122.065542  3715  3712  0.001245  3715_3712    4.0     True           2          MD  23771.06  2015 Modeled
    # 9    400006         4    880         S   ML  37.605003 -122.065542  3715  3712  0.001245  3715_3712    4.0     True           2          PM  24153.04  2015 Modeled
    # 10   400006         4    880         S   ML  37.605003 -122.065542  3715  3712  0.001245  3715_3712    4.0     True           2          EV  18160.92  2015 Modeled
    # 11   400006         4    880         S   ML  37.605003 -122.065542  3715  3712  0.001245  3715_3712    4.0     True           2       Daily  97146.12  2015 Modeled

    # model_final_df head - caltrans
    #      latitude   longitude countyCode  post_mile  route direction postmileSuffix  link_rowid     a     b  dist_from_link        A_B  ft  at  county  lanes  sep_HOV  link_count time_period    volume      category
    # 0   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          EA   2287.43  2015 Modeled
    # 1   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          AM   8742.13  2015 Modeled
    # 2   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          MD  14216.70  2015 Modeled
    # 3   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          PM  17774.32  2015 Modeled
    # 4   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          EV  14894.33  2015 Modeled
    # 5   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1       Daily  57914.91  2015 Modeled
    # 6   37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          EA   3557.25  2015 Modeled
    # 7   37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          AM  21925.53  2015 Modeled
    # 8   37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          MD  16331.60  2015 Modeled
    # 9   37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          PM  15007.88  2015 Modeled
    # 10  37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          EV   8425.77  2015 Modeled
    # 11  37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1       Daily  65248.03  2015 Modeled

    # model_final_df head - truck
    #     station  district  route direction type  abs_pm   latitude   longitude     a     b  distlink        A_B  stationsonlink  ft  at  county  lanes  sep_HOV  link_count time_period   volume truck_class      category
    # 0   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          EA   290.24          sm  2015 Modeled
    # 1   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          AM   724.00          sm  2015 Modeled
    # 2   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          MD  4367.69          sm  2015 Modeled
    # 3   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          PM  1610.82          sm  2015 Modeled
    # 4   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          EV  1103.75          sm  2015 Modeled
    # 5   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          EA    21.15          hv  2015 Modeled
    # 6   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          AM   184.86          hv  2015 Modeled
    # 7   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          MD    64.48          hv  2015 Modeled
    # 8   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          PM   102.43          hv  2015 Modeled
    # 9   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          EV    15.31          hv  2015 Modeled
    # 10  4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2       Daily  8096.50          sm  2015 Modeled
    # 11  4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2       Daily   388.23          hv  2015 Modeled
    print("Model columns: {}".format(sorted(model_final_df.columns.values.tolist())))
    print("Obsrv columns: {}".format(sorted(obs_df.columns.values.tolist())))
    print("Obs_wide columns: {}".format(sorted(obs_wide.columns.values.tolist())))
    # followed by the observed
    table_df = pandas.concat(
        [model_final_df, obs_df], axis="index", sort=True
    )  # sort means sort columns first so they are aligned
    print(
        "table_df length={} head=\n{}\ntail=\n{}".format(
            len(table_df), table_df.head(12), table_df.tail(12)
        )
    )

    # want a "wide" version, with a column for obsy1, obsy2, obsy3, modeled
    if args.caltrans_year:
        index_cols = ["countyCode", "route", "post_mile", "direction", "time_period"]
    elif args.pems_year:
        index_cols = [
            "station",
            "route",
            "direction",
            "abs_pm",
            "latitude",
            "longitude",
            "time_period",
        ]
    elif args.truck_year:
        index_cols = [
            "station",
            "route",
            "direction",
            "abs_pm",
            "latitude",
            "longitude",
            "time_period",
            "truck_class",
        ]

    model_wide = model_final_df[
        index_cols
        + [
            "a",
            "b",
            "A_B",
            "ft",
            "at",
            "county",
            "sep_HOV",
            "link_count",
            "stationsonlink",
            "distlink",
            "lanes",
            "volume",
        ]
    ].copy()
    model_wide.rename(
        columns={
            "volume": "{} Modeled".format(args.model_year),
            "lanes": "lanes modeled",
        },
        inplace=True,
    )
    print("model_wide head\n{}".format(model_wide.head(12)))

    obs_wide.reset_index(drop=True, inplace=True)
    obs_wide.rename(columns={"lanes": "lanes observed"}, inplace=True)
    print("obs_wide length={} head\n{}".format(len(obs_wide), obs_wide.head(12)))

    # the wide table is an inner join
    table_wide = pandas.merge(
        left=model_wide, right=obs_wide, how="left", on=index_cols
    )
    print("table_wide length={} head\n{}".format(len(table_wide), table_wide.head(12)))

    if args.pems_year or args.truck_year:
        # add lanes match attribute
        table_wide["lanes match"] = 0
        table_wide.loc[
            table_wide["lanes modeled"] == table_wide["lanes observed"], "lanes match"
        ] = 1
    else:
        # no data for caltrans so assume match
        table_wide["lanes match"] = 1

    ### filter down to max of one station per link by adding columns "skip", "skip_reason"
    # iterate through links by time period whittle down to a single observed for each link
    # store results here. columns = A_B, time_period, station, skip, skip_reason
    AB_timeperiod_station = pandas.DataFrame()

    groupby_cols = ["A_B", "time_period"]
    if args.truck_year:
        groupby_cols = groupby_cols + ["truck_class"]
    for AB_timeperiod, group_orig in table_wide.groupby(groupby_cols):
        print("Processing {}".format(AB_timeperiod))
        group = group_orig.copy()  # to make it clear

        group["skip"] = 0
        group["skip_reason"] = ""

        # skip due to lanes mismatch
        group.loc[
            (group.skip == 0) & (group["lanes match"] == 0), "skip_reason"
        ] = "lanes mismatch"
        group.loc[(group.skip == 0) & (group["lanes match"] == 0), "skip"] = 1

        useable = len(group) - group.skip.sum()
        print(
            "  Group has length {} and skips {} with {} remaining as useable ".format(
                len(group), group.skip.sum(), useable
            )
        )
        if useable <= 1:
            AB_timeperiod_station = pandas.concat(
                [
                    AB_timeperiod_station,
                    group[groupby_cols + ["station", "skip", "skip_reason"]],
                ]
            )
            continue

        # if there are some with modelyear observed and some without, kick out the ones without
        obs_target_notnull = group.loc[
            (group.skip == 0) & pandas.notnull(group[modelyear_observed])
        ]
        obs_target_isnull = group.loc[
            (group.skip == 0) & pandas.isnull(group[modelyear_observed])
        ]
        if len(obs_target_notnull) > 0 and len(obs_target_isnull) > 0:
            print(
                "  Skipping {} rows due to null {}".format(
                    len(obs_target_isnull), modelyear_observed
                )
            )
            group.loc[
                (group.skip == 0) & pandas.isnull(group[modelyear_observed]),
                "skip_reason",
            ] = "{} null".format(modelyear_observed)
            group.loc[
                (group.skip == 0) & pandas.isnull(group[modelyear_observed]), "skip"
            ] = 1

            useable = len(group) - group.skip.sum()
            print(
                "  Group has length {} and skips {} with {} remaining as useable ".format(
                    len(group), group.skip.sum(), useable
                )
            )
            if useable <= 1:
                AB_timeperiod_station = pandas.concat(
                    [
                        AB_timeperiod_station,
                        group[groupby_cols + ["station", "skip", "skip_reason"]],
                    ]
                )
                continue

        # if there are some with more observed, kick out the ones with fewer
        group["obs_count"] = 0
        for obs_col in obs_cols:
            group.loc[pandas.notnull(group[obs_col]), "obs_count"] += 1
        max_obs_count = group.loc[group.skip == 0, "obs_count"].max()
        fewer_obs_count = group.loc[
            (group.skip == 0) & (group.obs_count < max_obs_count)
        ]
        if len(fewer_obs_count) > 0:
            print(
                "  Skipping {} rows due to having fewer observations than {}".format(
                    len(fewer_obs_count), max_obs_count
                )
            )
            group.loc[
                (group.skip == 0) & (group.obs_count < max_obs_count), "skip_reason"
            ] = "fewer observations than {}".format(max_obs_count)
            group.loc[(group.skip == 0) & (group.obs_count < max_obs_count), "skip"] = 1

            useable = len(group) - group.skip.sum()
            print(
                "  Group has length {} and skips {} with {} remaining as useable ".format(
                    len(group), group.skip.sum(), useable
                )
            )
            if useable <= 1:
                AB_timeperiod_station = pandas.concat(
                    [
                        AB_timeperiod_station,
                        group[groupby_cols + ["station", "skip", "skip_reason"]],
                    ]
                )
                continue

        # if there are more than two remaining, use distlink to break the tie
        # todo: it would be preferable to choose the median daily value or closest to the middle of the link but those are more work
        min_distlink = group.loc[group.skip == 0, "distlink"].min()
        bigger_distlink = group.loc[(group.skip == 0) & (group.distlink > min_distlink)]
        if len(bigger_distlink) > 0:
            print(
                "  Skipping {} rows due to having bigger distlink than {}".format(
                    len(bigger_distlink), min_distlink
                )
            )
            group.loc[
                (group.skip == 0) & (group.distlink > min_distlink), "skip_reason"
            ] = "bigger distlink than {}".format(min_distlink)
            group.loc[(group.skip == 0) & (group.distlink > min_distlink), "skip"] = 1

            useable = len(group) - group.skip.sum()
            print(
                "  Group has length {} and skips {} with {} remaining as useable ".format(
                    len(group), group.skip.sum(), useable
                )
            )
            if useable <= 1:
                AB_timeperiod_station = pandas.concat(
                    [
                        AB_timeperiod_station,
                        group[groupby_cols + ["station", "skip", "skip_reason"]],
                    ]
                )
                continue

        # if min distlink didn't do it, use station number to break the tie (yes, this happens)
        min_station = group.loc[group.skip == 0, "station"].min()
        bigger_station = group.loc[(group.skip == 0) & (group.station > min_station)]
        if len(bigger_station) > 0:
            print(
                "  Skipping {} rows arbitrarily (station num) {}".format(
                    len(bigger_distlink), min_distlink
                )
            )
            group.loc[
                (group.skip == 0) & (group.station > min_station), "skip_reason"
            ] = "random (non-min station)"
            group.loc[(group.skip == 0) & (group.station > min_station), "skip"] = 1

            useable = len(group) - group.skip.sum()
            print(
                "  Group has length {} and skips {} with {} remaining as useable ".format(
                    len(group), group.skip.sum(), useable
                )
            )
            if useable <= 1:
                AB_timeperiod_station = pandas.concat(
                    [
                        AB_timeperiod_station,
                        group[groupby_cols + ["station", "skip", "skip_reason"]],
                    ]
                )
                continue

        # this shouldn't happen -- but it's useful when constructing above logic
        print(group)
        print(group[["A_B", "station", "distlink", "skip", "skip_reason"]])
        value = input("Type any key to continue...\n")
        break

    # purge observed data with known bad crosswalk
    if args.pems_year:
        AB_timeperiod_station.loc[
            AB_timeperiod_station.station.isin(PEMS_BAD_STATION_CROSSWALK),
            "skip_reason",
        ] = "known bad crosswalk"
        AB_timeperiod_station.loc[
            AB_timeperiod_station.station.isin(PEMS_BAD_STATION_CROSSWALK), "skip"
        ] = 1

    # brink skip, skip_reason back to table_wide
    print(
        "AB_timeperiod_station len={} head=\n{}".format(
            len(AB_timeperiod_station), AB_timeperiod_station.head(12)
        )
    )
    table_wide = pandas.merge(left=table_wide, right=AB_timeperiod_station, how="left")
    print("table_wide length={} head=\n{}".format(len(table_wide), table_wide.head()))

    # bring the attributes "lanes match", "ft", "county", "skip", "skip_reason" back to non-wide table
    lanes_match_df = table_wide[
        index_cols + ["lanes match", "ft", "county", "skip", "skip_reason"]
    ].drop_duplicates()
    print("lanes_match_df head:\n{}".format(lanes_match_df.head()))
    table_df = pandas.merge(
        left=table_df,
        right=lanes_match_df,
        how="left",
        on=index_cols,
        suffixes=("", "_temp"),
    )
    table_df.loc[
        pandas.isnull(table_df.ft) & pandas.notnull(table_df.ft_temp), "ft"
    ] = table_df.ft_temp
    table_df.loc[
        pandas.isnull(table_df.county) & pandas.notnull(table_df.county_temp), "county"
    ] = table_df.county_temp
    table_df.rename(columns={"lanes match_temp": "lanes match"}, inplace=True)
    table_df.drop(columns=["ft_temp", "county_temp"], inplace=True)
    print("table_df len={} head:\n{}".format(len(table_df), table_df.head()))

    # write non-wide
    table_df.to_csv("%s.csv" % out_file, index=False)

    # write wide
    table_wide.to_csv("%s_wide.csv" % out_file, index=False)
