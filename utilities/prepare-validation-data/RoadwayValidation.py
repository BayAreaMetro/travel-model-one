USAGE = """

 Creates a CSV for validating roadway volumes compared to PeMS, PeMS truck census, or CalTrans count data.

 Input:
========
1) .\\avgload5period[_vehclasses].csv: model data
   Required columns: a, b, lanes
   For trucks:         vol[EA,AM,MD,PM,EV]_[sm,smt,hv,hvt]
   For PeMS, Caltrans: vol[EA,AM,MD,PM,EV]_tot

If PeMS years specified:

p2) M:\Crosswalks\PeMSStations_TM1network\crosswalk_[2015,2023].csv: maps model roadway links to PeMS stations
    Required columns: a, b, station, HOV

p3) (Box: Share Data\pems-typical-weekday\pems_period.csv): PeMS observed data, generated by
    https://github.com/BayAreaMetro/pems-typical-weekday
    Required columns: station, route, direction, time_period, 
                      lanes, median_flow, avg_flow, abs_pm, latitude, longitude, year

If CalTrans years specified:

c2) model_to_caltrans.csv: maps model roadway links to CalTrans count locations
    Required columns: a, b, county, route, postmile, direction, leg, description

c3) (Box: Share Data\caltrans-typical-weekday\\typical-weekday-counts.csv): CalTrans observed data,
    generated by https://github.com/BayAreaMetro/caltrans-typical-weekday-counts

If Truck census years specified:

t2) M:\Crosswalks\PeMSStations_TM1network\\truck_census_stations_manual.csv: maps truck census
    locations to model roadway links

t3) (Box: Share Data\pems-typical-weekday\pems_truck_period.csv): PeMS Truck Census data,
    generated by https://github.com/BayAreaMetro/pems-typical-weekday

Output:
========

If PeMS years specified:

p1) Roadways to PeMS.csv: dataframe containing both modeled and observed data where each row is one volume
    (e.g. observed OR modeled)

    Columns: a, b, at, ft, county, sep_HOV,                                   (model link)
             district, station, route, direction, type,                       (PeMS station)
             abs_pm, latitude, longitude,                                     (PeMS station location)
             link_count, pemsonlink, distlink, lanes match,                   (matching info)
             time_period, lanes, volume, category                             (volumes)
    Where category is one of [(year) Modeled, (year) Observed]

    Note: PeMS data without model links are in this dataset with a,b=-1

p2) Roadways to PeMS_wide.csv: dataframe containing both modeled and observed data where each row is one set
    of volumes (observed AND modeled)

    Columns: a, b, at, ft, county, sep_HOV, [model_year] Modeled lanes,       (model link)
             district, station, route, direction, type,                       (PeMS station)
             abs_pm, latitude, longitude,                                     (PeMS station location)
             [pems_year] Observed lanes, Average Observed lanes,              (PeMS station lanes)
             link_count, pemsonlink, distlink, lanes match,                   (matching info)
             time_period, 2015 Modeled, [pems_year] Observed volume, Average Observed volume


If CalTrans years specified:

"""
import argparse, logging, os, sys
import pandas

TIMEPERIODS         = ['EA','AM','MD','PM','EV']
TM_HOVnEL_TO_GP_FILE= "M:\Crosswalks\PeMSStations_TM1network\HOVnEL_to_GP_links_{}.csv" # include model_year
PEMS_MAP_FILE       = "M:\Crosswalks\PeMSStations_TM1network\crosswalk_{}.csv"  # include model_year
TRUCK_MAP_FILE      = "M:\Crosswalks\PeMSStations_TM1network\\truck_census_stations_manual.csv"
CALTRANS_MAP_FILE   = "M:\Crosswalks\CaltransCountLocations_TM1network\\typical-weekday-counts-xy-TM1link.csv"
MODEL_FILE          = "avgload5period.csv"
MODEL_VCLASS_FILE   = "avgload5period_vehclasses.csv"

SHARE_DATA          = os.path.join(os.environ["USERPROFILE"], "Box", "Modeling and Surveys", "Share Data")
if os.environ["USERNAME"] == 'lzorn':
    SHARE_DATA      = os.path.join("E:\\", "Box", "Modeling and Surveys", "Share Data")
if os.environ["USERNAME"] == "MTCPB":
    SHARE_DATA      = os.path.join("\\\\tsclient", "E", "Box", "Modeling and Surveys", "Share Data")

PEMS_FILE           = os.path.join(SHARE_DATA, "pems-typical-weekday", "pems_period.csv")
CALTRANS_FILE       = os.path.join(SHARE_DATA, "caltrans-typical-weekday", "typical-weekday-counts.csv")
TRUCK_FILE          = os.path.join(SHARE_DATA, "pems-typical-weekday", "pems_truck_period.csv")
PEMS_OUTPUT_FILE    = "Roadways to PeMS"
CALTRANS_OUTPUT_FILE= "Roadways to Caltrans"
TRUCK_OUTPUT_FILE   = "Roadways to Truck Census"

MODEL_NONTRUCK_COLUMNS = ['a','b','ft','at','county','lanes'] + \
                         ['vol{}_tot'.format(timeperiod) for timeperiod in TIMEPERIODS]
MODEL_TRUCK_COLUMNS    = ['a','b','ft','at','county','lanes'] + \
                         ['vol{}_sm_tot'.format(timeperiod) for timeperiod in TIMEPERIODS] + \
                         ['vol{}_hv_tot'.format(timeperiod) for timeperiod in TIMEPERIODS]
PEMS_COLUMNS           = ['station','route','direction','time_period','lanes','avg_flow','abs_pm','latitude','longitude','year']

# these are bad crosswalk
PEMS_BAD_STATION_CROSSWALK = [401819, 401820]

if __name__ == '__main__':

    pandas.options.display.width    = 1000
    pandas.options.display.max_rows = 1000
    pandas.options.display.max_columns = 35

    parser = argparse.ArgumentParser(description=USAGE, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("-m","--model_year",    type=int, required=True)
    parser.add_argument("-c","--caltrans_year", type=int, nargs='*')
    parser.add_argument("-p","--pems_year",     type=int, nargs='*')
    parser.add_argument("-t","--truck_year",    type=int, nargs='*')
    args = parser.parse_args()

    args_specified = 0
    validation_type = ""
    if args.caltrans_year:
        args_specified += 1
        validation_type = "caltrans"
    if args.pems_year:
        args_specified += 1
        validation_type = "pems"
    if args.truck_year:
        args_specified += 1
        validation_type = "truck"

    if args_specified == 0:
        print(USAGE)
        print("No PeMS year nor CalTrans year no Trucks year argument specified.")
        sys.exit()

    if args_specified != 1:
        print(USAGE)
        print("Please specify ONE of pems_year, caltrans_year or truck_year")
        sys.exit()

    ############ logging
    LOG_FILE = f"RoadwayValidation_{validation_type}.log"
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    # console handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p'))
    logger.addHandler(ch)
    # file handler
    fh = logging.FileHandler(LOG_FILE, mode='w')
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p'))
    logger.addHandler(fh)

    ############ read the mapping first
    logging.info(args)

    mapping_df = None
    out_file   = None
    obs_cols   = []
    if args.pems_year:
        mapping_f  = PEMS_MAP_FILE.format(args.model_year)
        mapping_df = pandas.read_csv(mapping_f)
        model_file = MODEL_FILE
        model_cols = MODEL_NONTRUCK_COLUMNS
        out_file   = PEMS_OUTPUT_FILE
        obs_cols   = ["{} Observed volume".format(year) for year in args.pems_year]
    elif args.caltrans_year:
        mapping_f  = CALTRANS_MAP_FILE
        mapping_df = pandas.read_csv(mapping_f)
        mapping_df.rename(columns={"postmileValue":"post_mile", "routeNumber":"route"}, inplace=True)
        model_file = MODEL_FILE
        model_cols = MODEL_NONTRUCK_COLUMNS
        out_file   = CALTRANS_OUTPUT_FILE
        obs_cols   = ["{} Observed".format(year) for year in args.caltrans_year]
    else:
        mapping_f  = TRUCK_MAP_FILE
        mapping_df = pandas.read_csv(mapping_f)
        model_file = MODEL_VCLASS_FILE # need vehicle classes for truck comparison
        model_cols = MODEL_TRUCK_COLUMNS
        out_file   = TRUCK_OUTPUT_FILE
        obs_cols   = ["{} Observed".format(year) for year in args.truck_year]

    logging.info(f'Read {len(mapping_df)} lines from {mapping_f}')
    logging.debug(f'mapping_df.head():\n{mapping_df.head()}')
    # column name for model year observed
    modelyear_observed = "{} Observed volume".format(args.model_year)

    ############ read the model data
    model_df = pandas.read_csv(model_file)

    # strip the column names
    col_rename = {}
    for colname in model_df.columns.values.tolist(): col_rename[colname] = colname.strip()
    model_df.rename(columns=col_rename, inplace=True)
    model_df.rename(columns={"gl":"county"}, inplace=True)

    # truck processing requires a little aggregation
    if args.truck_year:
        # aggregate no-toll + toll
        for timeperiod in TIMEPERIODS:
            model_df['vol{}_sm_tot'.format(timeperiod)] = \
                (model_df['vol{}_sm'.format(timeperiod)] + model_df['vol{}_smt'.format(timeperiod)])
            model_df['vol{}_hv_tot'.format(timeperiod)] = \
                (model_df['vol{}_hv'.format(timeperiod)] + model_df['vol{}_hvt'.format(timeperiod)])
        
    # select only the columns we want
    model_df = model_df[model_cols]

    # add daily column
    if args.truck_year:
        model_df['Daily_sm'] = model_df[['volEA_sm_tot','volAM_sm_tot','volMD_sm_tot','volPM_sm_tot','volEV_sm_tot']].sum(axis=1)
        model_df['Daily_hv'] = model_df[['volEA_hv_tot','volAM_hv_tot','volMD_hv_tot','volPM_hv_tot','volEV_hv_tot']].sum(axis=1)
        model_cols = model_cols + ['Daily_sm','Daily_hv']
    else:
        model_df['Daily'] = model_df[['volEA_tot','volAM_tot','volMD_tot','volPM_tot','volEV_tot']].sum(axis=1)
        model_cols = model_cols + ['Daily',]

    logging.info(f'model_cols: {model_cols}')

    # the model data has a, b, lanes, vol*
    # but some of these links are HOV links and the volums should be summed to the same link as the non-hov link
    # read the hov -> gp mapping
    input_file = TM_HOVnEL_TO_GP_FILE.format(args.model_year)
    model_hov_to_gp_df = pandas.read_csv(input_file)
    # keep only those where we succeeded finding GP for now
    model_hov_to_gp_df = model_hov_to_gp_df.loc[ model_hov_to_gp_df.A_B_GP != "NA_NA"]
    logging.info(f"Read {len(model_hov_to_gp_df)} lines from {input_file}")
    logging.debug(f"model_hov_to_gp_df.head():\n{model_hov_to_gp_df.head()}")
    #        A      B  LANES  USE  FT  ROUTENUM ROUTEDIR    A_GP    B_GP         A_B     A_B_GP
    # 10  8900   8901      1    3   2       880        S  3400.0  3399.0   8900_8901  3400_3399
    # 12  8903   8904      1    3   2       880        S  3396.0  3416.0   8903_8904  3396_3416
    # 13  8904   8905      1    3   2       880        S  3416.0  3606.0   8904_8905  3416_3606
    # 14  8905  20229      1    3   2       880        S  3606.0  3603.0  8905_20229  3606_3603
    # 15  8907  20231      1    3   2       880        S  3601.0  3640.0  8907_20231  3601_3640

    # remove me eventually - compensate for bug where HOV link on bridge has wrong county coded
    # https://github.com/BayAreaMetro/TM1_2015_Base_Network/commit/8ecb3cae55616f7ac6fb2ebb2a3f134bd239a13d
    model_df.loc[ (model_df.a==20294)&(model_df.b==10601), "county"] = 5
    model_df.loc[ (model_df.a==10601)&(model_df.b==10607), "county"] = 5

    # join the model data to the hov -> gp mapping
    model_df = pandas.merge(left   =model_df,  right   =model_hov_to_gp_df[["A","B","LANES","USE","A_GP","B_GP"]],
                            left_on=["a","b"], right_on=["A","B"],
                            how    ="left")
    logging.debug(f"model_df hov links head\n{model_df.loc[ pandas.notnull(model_df.A_GP)].head()}")
    # set those to HOV true and set the a,b to the GP versions
    model_df["sep_HOV"] = False
    model_df.loc[ pandas.notnull(model_df.A_GP), "sep_HOV"] = True
    model_df.loc[ pandas.notnull(model_df.A_GP), "a"      ] = model_df.A_GP
    model_df.loc[ pandas.notnull(model_df.A_GP), "b"      ] = model_df.B_GP
    # drop other cols and make a,b back into int
    model_df = model_df[ model_cols + ["sep_HOV"]]
    model_df["a"] = model_df["a"].astype(int)
    model_df["b"] = model_df["b"].astype(int)
    # now a,b isn't unique so group
    model_df["link_count"] = 1
    # facility types may not match since GP toll plazas have ft=6 so take min
    # area types should match but in case there are errors, we don't want duplicate rows, so take min
    # ditto with county
    model_agg_dict = {'ft':'min','at':'min','county':'first','link_count':'sum','lanes':'sum','sep_HOV':'sum'}
    # sum volumes
    for column in model_cols:
        if column.startswith('vol') or column.startswith('Daily'): model_agg_dict[column] = 'sum'

    logging.debug(f"{model_agg_dict=}")
    model_df = model_df.groupby(["a","b"]).agg(model_agg_dict).reset_index()
    logging.debug(f"After combining hov lanes/volumes back into GP links, model_df.head():\n{model_df.loc[model_df.link_count>1].head()}")

    # create a multi index for stacking
    model_df.set_index(['a','b','ft','at','county','lanes','sep_HOV','link_count'], inplace=True)
    # stack: so now we have a series with multiindex: a,b,lanes,varname
    model_df = pandas.DataFrame({'volume': model_df.stack()})
    # reset the index
    model_df.reset_index(inplace=True)
    logging.debug(f"model_df head\n{model_df.head(12)}")

    # and rename it - truck has truck class included
    if args.truck_year: 
        model_df['truck_class'] = model_df['level_8'].str[6:8]  # 'sm' or 'hv'
        model_df.loc[ model_df.level_8.str.startswith('Daily'), 'level_8'] = 'Daily'

    model_df.rename(columns={'level_8':'time_period'}, inplace=True)
    # remove extra chars: 'volAM_tot' => 'AM'
    model_df.loc[model_df['time_period'].str.startswith('vol'),'time_period'] = model_df['time_period'].str[3:5]
    logging.debug(f"model_df head\n{model_df.head(12)}")

    logging.debug(f"model_df for 11828-8676:\n{model_df.loc[(model_df.a==11828) & (model_df.b==8676)]}")

    if args.pems_year:
        ############ read the pems data
        obs_df = pandas.read_csv(PEMS_FILE, na_values='NA', engine='python')

        # select only the columns we want
        obs_df = obs_df[PEMS_COLUMNS]
        # select only the years in question
        obs_df = obs_df[ obs_df['year'].isin(args.pems_year)].reset_index(drop=True)

        logging.debug(f"obs_df for station=402536:\n{obs_df.loc[obs_df.station==402536]}")
        logging.debug(f"obs_df for station=407332:\n{obs_df.loc[obs_df.station==407332]}")
        logging.debug(f"obs_df for station=407359:\n{obs_df.loc[obs_df.station==407359]}")

        # select out abs_pm, latitude, longitude -- that will come from the crosswalk
        obs_df.drop(columns=['abs_pm','latitude','longitude'], inplace=True)

        # create missing cols in PeMS
        obs_df.rename(columns={'avg_flow':'volume', 'year':'category'}, inplace=True)
        logging.debug(f"obs_df len={len(obs_df)} head\n{obs_df.head(22)}")
        #    station  route direction time_period  lanes        volume  category
        #0    400001    101         N          AM      5  23462.250000      2014
        #1    400001    101         N          EA      5   7549.107143      2014
        #2    400001    101         N          EV      5   9162.789474      2014
        #3    400001    101         N          MD      5  18982.450000      2014
        #4    400001    101         N          PM      5  11771.904762      2014

        obs_daily_df = obs_df.groupby(["station","route","direction","lanes","category"]).aggregate({"time_period":"count","volume":"sum"})
        obs_daily_df.reset_index(inplace=True)
        logging.debug(f"obs_daily_df.loc[obs_daily_df.time_period>5]:\n{obs_daily_df.loc[obs_daily_df.time_period>5]}")
        assert(len(obs_daily_df.loc[obs_daily_df.time_period>5])==0)
        # drop those with fewer than 5 time periods
        obs_daily_df = obs_daily_df.loc[ obs_daily_df.time_period == 5 ]
        assert( len(obs_daily_df.loc[obs_daily_df.time_period!=5])==0 )
        # add these
        obs_daily_df["time_period"] = "Daily"
        obs_df = pandas.concat([obs_df, obs_daily_df], axis="index", sort=True) # sort means sort columns first so they are aligned
        obs_df.sort_values(by=["station","route","direction","lanes","category","time_period"], inplace=True)
        obs_df.reset_index(drop=True, inplace=True)
        logging.debug(f"After timeperiod==Daily added, obs_df len={len(obs_df)} head\n{obs_df.head(22)}")

        #      abs_pm  category direction  lanes   latitude   longitude  route  station time_period         volume
        # 0   387.897      2014         N      5  37.364085 -121.901149    101   400001          AM   23462.250000
        # 1   387.897      2014         N      5  37.364085 -121.901149    101   400001       Daily   70928.501378
        # 2   387.897      2014         N      5  37.364085 -121.901149    101   400001          EA    7549.107143
        # 3   387.897      2014         N      5  37.364085 -121.901149    101   400001          EV    9162.789474
        # 4   387.897      2014         N      5  37.364085 -121.901149    101   400001          MD   18982.450000
        # 5   387.897      2014         N      5  37.364085 -121.901149    101   400001          PM   11771.904762
        obs_df['category'] = obs_df.category.map(str) + ' Observed'

        # want to bring category (year) into columns
        obs_wide = pandas.pivot_table(obs_df, values=["volume","lanes"], index=["station","route","direction","time_period"], columns="category")
        obs_wide[("volume","Average Observed")] = obs_wide["volume"].mean(axis=1)  # this will not include NaNs or missing vals so it handles them correctly
        obs_wide[("lanes","Average Observed")]  = obs_wide["lanes"].mean(axis=1)  # this will not include NaNs or missing vals so it handles them correctly
        obs_wide.reset_index(inplace=True)
        # collapse MultiIndex columns
        obs_wide.columns = [' '.join(reversed(col)).strip() for col in obs_wide.columns.values]

        logging.debug(f"obs_wide head\n{obs_wide.head(12)}")
        #     station  route direction time_period  2014 Observed lanes  2015 Observed lanes  2016 Observed lanes  2014 Observed volume  2015 Observed volume  2016 Observed volume  Average Observed volume  Average Observed lanes
        # 0    400001    101         N          AM                  5.0                  5.0                  5.0          23495.139535          22912.985507          22977.394366             23128.506469                     5.0
        # 1    400001    101         N       Daily                  5.0                  5.0                  5.0          70916.375081          72778.537730          72655.542257             72116.818356                     5.0
        # 2    400001    101         N          EA                  5.0                  5.0                  5.0           7530.259259           8080.904762           8532.123288              8047.762436                     5.0
        # 3    400001    101         N          EV                  5.0                  5.0                  5.0           9162.789474           9423.553571           9342.385714              9309.576253                     5.0
        # 4    400001    101         N          MD                  5.0                  5.0                  5.0          18956.282051          20531.639344          20314.888889             19934.270095                     5.0
        # 5    400001    101         N          PM                  5.0                  5.0                  5.0          11771.904762          11829.454545          11488.750000             11696.703102                     5.0
        # 6    400002    101         S          AM                  5.0                  NaN                  NaN          25819.647059                   NaN                   NaN             25819.647059                     5.0
        # 7    400002    101         S       Daily                  5.0                  NaN                  NaN         117791.618033                   NaN                   NaN            117791.618033                     5.0
        # 8    400002    101         S          EA                  5.0                  NaN                  NaN           4039.462963                   NaN                   NaN              4039.462963                     5.0
        # 9    400002    101         S          EV                  5.0                  NaN                  NaN          26510.714286                   NaN                   NaN             26510.714286                     5.0
        # 10   400002    101         S          MD                  5.0                  NaN                  NaN          32390.313725                   NaN                   NaN             32390.313725                     5.0
        # 11   400002    101         S          PM                  5.0                  NaN                  NaN          29031.480000                   NaN                   NaN             29031.480000                     5.0
    elif args.caltrans_year:
        ############ read the caltrans data
        obs_df = pandas.read_csv(CALTRANS_FILE)
        logging.info(f"Read {len(obs_df)} rows from {CALTRANS_FILE}")
        logging.debug(f"head:\n{obs_df.head()}")

        # make columns conform to previous version and to model data
        obs_df.rename(columns={"county":"countyCode"}, inplace=True)

        # select the relevant years
        obs_df = obs_df.loc[ obs_df.year.isin(args.caltrans_year)]

        # add station,description column to mapping_df -- and keep only the relevant entries since mapping_df includes stations for all years
        description_df = obs_df[["route","countyCode","post_mile","direction","station","description"]].drop_duplicates()
        logging.debug(f"locations with descriptions ({len(description_df)}) head:\n{description_df.head()}")
        mapping_df = pandas.merge(left=mapping_df, right=description_df, how="inner")

        id_vars = ["route","countyCode","post_mile","leg","direction","station","description"]

        # set the time_period
        obs_df["time_period"] = "EV"
        obs_df.loc[(obs_df.integer_hour >=  3)&(obs_df.integer_hour <  6), "time_period"] = "EA"
        obs_df.loc[(obs_df.integer_hour >=  6)&(obs_df.integer_hour < 10), "time_period"] = "AM"
        obs_df.loc[(obs_df.integer_hour >= 10)&(obs_df.integer_hour < 15), "time_period"] = "MD"
        obs_df.loc[(obs_df.integer_hour >= 15)&(obs_df.integer_hour < 19), "time_period"] = "PM"

        # aggregate to time period and verify each is complete
        obs_df = obs_df.groupby(id_vars + ["year","time_period"]).aggregate(
            {"integer_hour":"count", "median_count":"sum", "avg_count":"sum", "days_observed":"mean"}).reset_index()
        logging.debug(f"obs_df len={len(obs_df)} head\n{obs_df.head(10)}")

        obs_df = obs_df.loc[ ((obs_df.time_period=="EA")&(obs_df.integer_hour==3))|
                             ((obs_df.time_period=="AM")&(obs_df.integer_hour==4))|
                             ((obs_df.time_period=="MD")&(obs_df.integer_hour==5))|
                             ((obs_df.time_period=="PM")&(obs_df.integer_hour==4))|
                             ((obs_df.time_period=="EV")&(obs_df.integer_hour==8))]
        logging.debug(f"obs_df len={len(obs_df)} head\n{obs_df.head(10)}")

        # drop integer_hour count, median_count, days_observed -- retain sum of avg_count as the volume
        obs_df.drop(columns=["integer_hour","median_count","days_observed"], inplace=True)
        obs_df.rename(columns={"avg_count":"volume"}, inplace=True)
        logging.debug(f"obs_df len={len(obs_df)} head\n{obs_df.head(10)}")

        # get Daily by year and add it
        obs_daily_df = obs_df.groupby(id_vars + ["year"]).aggregate({"time_period":"count","volume":"sum"})
        # drop any that are incomplete
        logging.info(f"Dropping {len(obs_daily_df.loc[obs_daily_df.time_period != 5])} obs_daily_df rows for being incomplete")
        obs_daily_df = obs_daily_df.loc[obs_daily_df.time_period==5]
        obs_daily_df["time_period"] = "Daily"
        obs_daily_df.reset_index(inplace=True)
        print("obs_daily_df len={} head\n{}".format(len(obs_daily_df), obs_daily_df.head()))
        #    route countyCode  post_mile leg direction  station                     description  year time_period        volume
        # 0      4         CC       11.4   B         E    912.0                    PACHECO BLVD  2016       Daily  41295.952381
        # 1      4         CC       11.4   B         W    912.0                    PACHECO BLVD  2016       Daily  45729.761905
        # 2     12        NAP        2.3   B         E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2014       Daily  18502.722816
        # 3     12        NAP        2.3   B         E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2015       Daily  19656.101695
        # 4     12        NAP        2.3   B         E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2016       Daily  20468.062500
        obs_df = pandas.concat([obs_df, obs_daily_df], axis="index", sort=True) # sort means sort columns first so they are aligned
        obs_df['category'] = obs_df.year.map(str) + ' Observed'
        obs_df.drop(columns=["year"], inplace=True)
        logging.debug(f"obs_df len={len(obs_df)} head\n{obs_df.head(10)}")

        # move category (year) to columns
        obs_wide = pandas.pivot_table(obs_df, index=id_vars + ["time_period"], columns=["category"], values="volume")
        obs_wide["Average Observed"] = obs_wide.mean(axis=1)  # this will not include NaNs or missing vals so it handles them correctly
        obs_wide.reset_index(inplace=True)
        logging.debug(f"obs_wide len={len(obs_wide)} head\n{obs_wide.head(10)}")

    elif args.truck_year:
        ############ read the truck census data
        obs_df = pandas.read_csv(TRUCK_FILE)
        logging.debug(obs_df.iloc[0])

        # select only the years in question and district 4
        obs_df = obs_df.loc[obs_df['year'].isin(args.truck_year)]
        obs_df = obs_df.loc[obs_df['District.Identifier'] == 4].reset_index(drop=True)

        # select only the columns we want
        TRUCK_COLUMNS = [
            'Census.Substation.Identifier','Station.Type','Freeway.Identifier','Freeway.Direction','Absolute.Postmile','Latitude','Longitude','Name',
            'Vehicle.Class','lanes','year','time_period','avg_flow']
        obs_df = obs_df[TRUCK_COLUMNS]

        #### vehicle class processing (see PeMS truck vehicle classes https://app.asana.com/0/0/1203052150304041/f)
        # and https://www.fhwa.dot.gov/policyinformation/tmguide/tmg_2013/vehicle-types.cfm
        # we'll filter classes 7-13 as 4+ axle => heavy
        # all other we'll call small
        # 0 = all categories so drop that
        obs_df = obs_df.loc[obs_df['Vehicle.Class'] != 0].reset_index(drop=True)
        truck_vehicle_class_recode = {
            1: 'sm',  # ?
            2: 'sm',  # 8-20 ft
            3: 'sm',  # 2 Axle, 4T SU
            4: 'sm',  # 3 Axle SU
            5: 'sm',  # 2 Axle, 6T SU
            6: 'sm',  # 3 Axle SU
            7: 'hv',  # 4+ Axle ST
            8: 'hv',  # <4 Axle ST
            9: 'hv',  # 5 Axle ST
            10: 'hv', # 6+ Axle ST
            11: 'hv', # <5 Axle MT
            12: 'hv', # 6 Axle MT
            13: 'hv', # 7+ Axle MT
            14: 'sm', # User-Def
            15: 'sm', # Unknown
        }
        obs_df['truck_class'] = obs_df['Vehicle.Class'].replace(to_replace=truck_vehicle_class_recode)
        logging.debug(obs_df[['Vehicle.Class','truck_class']].value_counts())

        # rename to standardized and create category
        obs_df.rename(columns={
            'avg_flow'                      :'volume',
            'Census.Substation.Identifier'  :'station',
            'Station.Type'                  :'type',
            'Freeway.Identifier'            :'route',
            'Freeway.Direction'             :'direction',
            'Absolute.Postmile'             :'abs_pm',
            'Latitude'                      :'latitude',
            'Longitude'                     :'longitude',
            'Name'                          :'station description',
            'year'                          :'category'
        }, inplace=True)
        logging.debug(f"obs_df len={len(obs_df)} head\n{obs_df.head(5)}")
        # obs_df len=3010 head
        #    station type  route direction  abs_pm   latitude   longitude       station description  Vehicle.Class  lanes  category time_period      volume truck_class
        # 0  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          AM  138.761905          sm
        # 1  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          EA   35.904762          sm
        # 2  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          EV  245.230769          sm
        # 3  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          MD  198.272727          sm
        # 4  4902003   ML     80         E  20.498  37.991216 -122.309341  PINOLE  APPIAN WAY|Leg O              2      4      2014          PM  147.545455          sm
        # 
        # aggregate to truck_class
        obs_df = obs_df.groupby(["station","type","route","direction","lanes","category","abs_pm","longitude","latitude","truck_class","time_period"]).aggregate(
            {"volume":"sum"})
        obs_df.reset_index(inplace=True)

        # aggregate to daily to get daily sum
        obs_daily_df = obs_df.groupby(["station","type","route","direction","lanes","category","abs_pm","longitude","latitude","truck_class"]).aggregate({"time_period":"count","volume":"sum"})
        obs_daily_df.reset_index(inplace=True)
        assert(len(obs_daily_df.loc[obs_daily_df.time_period>5])==0)
        # drop those with fewer than 5 time periods
        obs_daily_df = obs_daily_df.loc[ obs_daily_df.time_period == 5 ]
        assert( len(obs_daily_df.loc[obs_daily_df.time_period!=5])==0 )

        # add these
        obs_daily_df["time_period"] = "Daily"
        obs_df = pandas.concat([obs_df, obs_daily_df], axis="index", sort=True) # sort means sort columns first so they are aligned
        obs_df.sort_values(by=["station","type","route","direction","lanes","category","truck_class","time_period"], inplace=True)
        obs_df.reset_index(drop=True, inplace=True)
        logging.debug(f"obs_df len={len(obs_df)} head\n{obs_df.head(12)}")
        # obs_df len=516 head
        #     abs_pm  category direction  lanes   latitude   longitude  route  station time_period truck_class type        volume
        # 0   20.498      2014         E      4  37.991216 -122.309341     80  4902003          AM          hv   ML   1505.904762
        # 1   20.498      2014         E      4  37.991216 -122.309341     80  4902003       Daily          hv   ML   5738.010989
        # 2   20.498      2014         E      4  37.991216 -122.309341     80  4902003          EA          hv   ML    670.952381
        # 3   20.498      2014         E      4  37.991216 -122.309341     80  4902003          EV          hv   ML    866.153846
        # 4   20.498      2014         E      4  37.991216 -122.309341     80  4902003          MD          hv   ML   1983.454545
        # 5   20.498      2014         E      4  37.991216 -122.309341     80  4902003          PM          hv   ML    711.545455
        # 6   20.498      2014         E      4  37.991216 -122.309341     80  4902003          AM          sm   ML   2623.238095
        # 7   20.498      2014         E      4  37.991216 -122.309341     80  4902003       Daily          sm   ML  12028.665002
        # 8   20.498      2014         E      4  37.991216 -122.309341     80  4902003          EA          sm   ML    737.238095
        # 9   20.498      2014         E      4  37.991216 -122.309341     80  4902003          EV          sm   ML   1912.461538
        # 10  20.498      2014         E      4  37.991216 -122.309341     80  4902003          MD          sm   ML   3917.727273
        # 11  20.498      2014         E      4  37.991216 -122.309341     80  4902003          PM          sm   ML   2838.000000
        obs_df['category'] = obs_df.category.map(str) + ' Observed'

        # want to bring category into columns
        obs_wide = pandas.pivot_table(obs_df, 
            values="volume", 
            index=["station","type","route","direction","abs_pm","latitude","longitude","lanes","truck_class","time_period"], 
            columns="category")
        obs_wide["Average Observed"] = obs_wide.mean(axis=1)  # this will not include NaNs or missing vals so it handles them correctly
        obs_wide.reset_index(inplace=True)

        logging.debug(f"obs_df len={len(obs_df)} head\n{obs_df.head(12)}")
        # obs_wide len=180 head
        # category  station type  route direction  abs_pm   latitude   longitude  lanes truck_class time_period  2014 Observed  2015 Observed  2016 Observed  Average Observed
        # 0         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          AM    1505.904762    1521.461538    1482.666667       1503.344322
        # 1         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv       Daily    5738.010989    5901.516484    5577.844444       5739.123972
        # 2         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          EA     670.952381     739.076923     753.777778        721.269027
        # 3         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          EV     866.153846     898.285714     852.400000        872.279853
        # 4         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          MD    1983.454545    2014.153846    1903.777778       1967.128723
        # 5         4902003   ML     80         E  20.498  37.991216 -122.309341      4          hv          PM     711.545455     728.538462     585.222222        675.102046
        # 6         4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          AM    2623.238095    2063.000000    2397.222222       2361.153439
        # 7         4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm       Daily   12028.665002    9184.538462   10282.666667      10498.623377
        # 8         4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          EA     737.238095     653.846154     820.222222        737.102157
        # 9         4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          EV    1912.461538    1281.000000    1792.000000       1661.820513
        # 10        4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          MD    3917.727273    3247.538462    3142.888889       3436.051541
        # 11        4902003   ML     80         E  20.498  37.991216 -122.309341      4          sm          PM    2838.000000    1939.153846    2130.333333       2302.495726

    # model has a, b, A_B
    obs_df['a'] = -1
    obs_df['b'] = -1

    # create the final stacked table -- first the model information
    mapping_df.rename(columns={"A":"a", "B":"b"}, inplace=True)
    logging.debug(f"mapping_df len={len(mapping_df):,} head\n{mapping_df.head()}")
    #    station  district  route direction type   abs_pm   latitude   longitude      a     b    distlink  stationsonlink
    # 0   400075         4    580         E   ML   61.493  37.825760 -122.275143   2771  2716  146.028526               1
    # 1   400100         4     17         S   ML   24.943  37.295052 -121.938872   4827  4782   56.401774               1
    # 2   400106         4    101         S   ML  417.153  37.586325 -122.332169  20533  6317  244.746467               2
    # 3   400153         4    580         E   ML   31.612  37.700938 -121.828882   2760  8317   90.527145               2
    # 4   400162         4     92         W   ML   12.216  37.552538 -122.307782   6346  6303    2.382106               1
    # print("mapping_df cols:\n{}\nmodel_df cols:\n{}".format(mapping_df.dtypes, model_df.dtypes))
    model_final_df = pandas.merge(left=mapping_df, right=model_df, how='inner')
    model_final_df['category'] = f'{args.model_year} Modeled volume'
    logging.debug(f"model_final_df len={len(model_final_df):,} head\n{model_final_df.head(12)}")

    dupe_check = model_final_df.duplicated( subset=['station','a','b','time_period'])
    assert(dupe_check.sum() == 0)

    # model_final_df head - pems
    #     station  district  route direction type  abs_pm   latitude   longitude     a     b    distlink  stationsonlink  ft  at  county  lanes  sep_HOV  link_count time_period    volume      category
    # 0    400075         4    580         E   ML  61.493  37.825760 -122.275143  2771  2716  146.028526               1   2   2       4    5.0        0           1          EA   3062.34  2015 Modeled
    # 1    400075         4    580         E   ML  61.493  37.825760 -122.275143  2771  2716  146.028526               1   2   2       4    5.0        0           1          AM  19245.46  2015 Modeled
    # 2    400075         4    580         E   ML  61.493  37.825760 -122.275143  2771  2716  146.028526               1   2   2       4    5.0        0           1          MD  24839.51  2015 Modeled
    # 3    400075         4    580         E   ML  61.493  37.825760 -122.275143  2771  2716  146.028526               1   2   2       4    5.0        0           1          PM  28079.71  2015 Modeled
    # 4    400075         4    580         E   ML  61.493  37.825760 -122.275143  2771  2716  146.028526               1   2   2       4    5.0        0           1          EV  20334.17  2015 Modeled
    # 5    400075         4    580         E   ML  61.493  37.825760 -122.275143  2771  2716  146.028526               1   2   2       4    5.0        0           1       Daily  95561.19  2015 Modeled
    # 6    400100         4     17         S   ML  24.943  37.295052 -121.938872  4827  4782   56.401774               1   2   3       3    3.0        0           1          EA   1656.02  2015 Modeled
    # 7    400100         4     17         S   ML  24.943  37.295052 -121.938872  4827  4782   56.401774               1   2   3       3    3.0        0           1          AM   7793.61  2015 Modeled
    # 8    400100         4     17         S   ML  24.943  37.295052 -121.938872  4827  4782   56.401774               1   2   3       3    3.0        0           1          MD  12698.46  2015 Modeled
    # 9    400100         4     17         S   ML  24.943  37.295052 -121.938872  4827  4782   56.401774               1   2   3       3    3.0        0           1          PM  14048.97  2015 Modeled
    # 10   400100         4     17         S   ML  24.943  37.295052 -121.938872  4827  4782   56.401774               1   2   3       3    3.0        0           1          EV   8959.64  2015 Modeled
    # 11   400100         4     17         S   ML  24.943  37.295052 -121.938872  4827  4782   56.401774               1   2   3       3    3.0        0           1       Daily  45156.70  2015 Modeled

    # model_final_df head - caltrans
    #      latitude   longitude countyCode  post_mile  route direction postmileSuffix  link_rowid     a     b  dist_from_link        A_B  ft  at  county  lanes  sep_HOV  link_count time_period    volume      category
    # 0   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          EA   2287.43  2015 Modeled
    # 1   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          AM   8742.13  2015 Modeled
    # 2   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          MD  14216.70  2015 Modeled
    # 3   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          PM  17774.32  2015 Modeled
    # 4   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1          EV  14894.33  2015 Modeled
    # 5   37.854479 -122.218076        ALA      5.887     24         E              R         143  1952  1950    4.228366e+06  1952_1950   1   3       5    3.0    False           1       Daily  57914.91  2015 Modeled
    # 6   37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          EA   3557.25  2015 Modeled
    # 7   37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          AM  21925.53  2015 Modeled
    # 8   37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          MD  16331.60  2015 Modeled
    # 9   37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          PM  15007.88  2015 Modeled
    # 10  37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1          EV   8425.77  2015 Modeled
    # 11  37.854479 -122.218076        ALA      5.887     24         W              L         126  1917  1915    4.228366e+06  1917_1915   1   3       5    3.0    False           1       Daily  65248.03  2015 Modeled

    # model_final_df head - truck
    #     station  district  route direction type  abs_pm   latitude   longitude     a     b  distlink        A_B  stationsonlink  ft  at  county  lanes  sep_HOV  link_count time_period   volume truck_class      category
    # 0   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          EA   290.24          sm  2015 Modeled
    # 1   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          AM   724.00          sm  2015 Modeled
    # 2   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          MD  4367.69          sm  2015 Modeled
    # 3   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          PM  1610.82          sm  2015 Modeled
    # 4   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          EV  1103.75          sm  2015 Modeled
    # 5   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          EA    21.15          hv  2015 Modeled
    # 6   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          AM   184.86          hv  2015 Modeled
    # 7   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          MD    64.48          hv  2015 Modeled
    # 8   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          PM   102.43          hv  2015 Modeled
    # 9   4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2          EV    15.31          hv  2015 Modeled
    # 10  4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2       Daily  8096.50          sm  2015 Modeled
    # 11  4902003         4     80         E   ML  20.498  37.991216 -122.309341  2278  2288      0.01  2278_2288               1   2   4       5      4        1           2       Daily   388.23          hv  2015 Modeled
    logging.debug(f"Model columns: {sorted(model_final_df.columns.values.tolist())}")
    logging.debug(f"Obsrv columns: {sorted(obs_df.columns.values.tolist())}")
    logging.debug(f"Obs_wide columns: {sorted(obs_wide.columns.values.tolist())}")
    # followed by the observed
    table_long = pandas.concat([model_final_df, obs_df], axis="index", sort=True) # sort means sort columns first so they are aligned
    logging.debug(f'table_long length={len(table_long)} head=\n{table_long.head(12)}\ntail=\n{table_long.tail(12)}')

    # want a "wide" version, with a column for obsy1, obsy2, obsy3, modeled
    if args.caltrans_year:
        index_cols = ["countyCode","route","post_mile","direction","time_period"]
    elif args.pems_year:
        index_cols = ["station","route","direction","time_period"]
    elif args.truck_year:
        index_cols = ["station","route","direction","abs_pm","latitude","longitude","time_period","truck_class"]

    model_wide = model_final_df[index_cols + ["abs_pm","latitude","longitude","a","b","ft","at","county","sep_HOV","link_count", "stationsonlink", "distlink", "lanes","volume"]].copy()
    model_wide.rename(columns={"volume":f"{args.model_year} Modeled volume", 
                               "lanes":f"{args.model_year} Modeled lanes"}, inplace=True)
    logging.debug(f"model_wide head\n{model_wide.head(12)}")

    obs_wide.reset_index(drop=True, inplace=True)
    logging.debug(f"obs_wide length={len(obs_wide)} head\n{obs_wide.head(12)}")

    # the wide table is an inner join
    table_wide = pandas.merge(left=model_wide, right=obs_wide, how='left', on=index_cols)
    logging.debug(f"table_wide length={len(table_wide)} head\n{table_wide.head(12)}")

    if args.pems_year or args.truck_year:
        # add lanes match attribute
        table_wide['lanes match'] = 0
        # lanes match
        table_wide.loc[ table_wide[f'{args.model_year} Modeled lanes'] == table_wide['Average Observed lanes'], 'lanes match'] = 1
        # don't mark as bad if 'Average Observed lanes' is na; other time periods with lanes match are still worth keeping
        table_wide.loc[ pandas.isna(table_wide['Average Observed lanes']), 'lanes match'] = 1
    else:
        # no data for caltrans so assume match
        table_wide['lanes match'] = 1

    ### filter down to max of one station per link by adding columns "skip", "skip_reason"
    # iterate through links and whittle down to a single observed for each link
    # store results here. columns = A_B, station, skip, skip_reason
    AB_station = pandas.DataFrame()

    groupby_cols = ["a","b"]
    if args.truck_year: groupby_cols = groupby_cols + ["truck_class"]
    for A_B,group_orig in table_wide.groupby(groupby_cols):
        logging.debug(f"Processing {A_B}")
        logging.debug(f"group:\n{group_orig}")
        group = group_orig.copy() # to make it clear

        group["skip"       ] = 0
        group["skip_reason"] = ""

        # skip due to lanes mismatch
        group.loc[ (group.skip==0)&(group["lanes match"]== 0), "skip_reason"] = "lanes mismatch"
        group.loc[ (group.skip==0)&(group["lanes match"]== 0), "skip"       ] = 1

        # useable: count unique (a,b,station,skip)-tuples where skip==0
        group['unique_a_b_station'] = 1-group.duplicated(subset=['a','b','station','skip'])
        logging.debug(f"unique a,b,station:\n{group}")
        useable = group.loc[group.skip==0, 'unique_a_b_station'].sum()
        logging.debug(f"=> Group has length {len(group)} and skips {group.skip.sum()} with {useable} remaining useable (a,b,station) tuples")
        if useable <= 1: 
            AB_station = pandas.concat([AB_station, group[groupby_cols+["station","skip","skip_reason"]]])
            continue

        # if there are some with modelyear observed and some without, kick out the ones without
        obs_target_notnull = group.loc[ (group.skip==0)&pandas.notnull(group[modelyear_observed]) ]
        obs_target_isnull  = group.loc[ (group.skip==0)&pandas.isnull(group[modelyear_observed])  ]
        if len(obs_target_notnull) > 0 and len(obs_target_isnull) > 0:
            logging.debug(f"  Skipping {len(obs_target_isnull)} rows due to null {modelyear_observed}")
            group.loc[ (group.skip==0)&pandas.isnull(group[modelyear_observed]), "skip_reason" ] = f"{modelyear_observed} null"
            group.loc[ (group.skip==0)&pandas.isnull(group[modelyear_observed]), "skip"        ] = 1

            # useable: count unique (a,b,station,skip)-tuples where skip==0
            group['unique_a_b_station'] = 1-group.duplicated(subset=['a','b','station','skip'])
            useable = group.loc[group.skip==0, 'unique_a_b_station'].sum()
            logging.debug(f"  Group has length {len(group)} and skips {group.skip.sum()} with {useable} remaining useable (a,b,station) tuples")
            if useable <= 1: 
                AB_station = pandas.concat([AB_station, group[groupby_cols + ["station","skip","skip_reason"]]])
                continue

        # if there are some with more observed, kick out the ones with fewer
        group["obs_count"] = 0
        for obs_col in obs_cols: group.loc[ pandas.notnull(group[obs_col]), "obs_count"] += 1
        a_b_station_skip_obs = group.groupby(['a','b','station','skip']).agg({'obs_count':'sum'}).reset_index(drop=False)
        logging.debug(f"  group with obs_count:\n{group}")
        logging.debug(f"  a_b_station_skip_obs:\n{a_b_station_skip_obs}")
        max_obs_count   = a_b_station_skip_obs.loc[a_b_station_skip_obs.skip==0, 'obs_count'].max()
        fewer_obs_count = a_b_station_skip_obs.loc[(a_b_station_skip_obs.skip==0) & (a_b_station_skip_obs.obs_count < max_obs_count)]
        logging.debug(f"  max_obs_count={max_obs_count}; fewer_obs_count:\n{fewer_obs_count}")
        if len(fewer_obs_count) > 0:
            logging.debug(f"  Skipping {len(fewer_obs_count)} rows due to having fewer observations than {max_obs_count}")
            # merge a_b_station_skip_obs back to group and replace obs_count with summed version
            group.drop(columns=['obs_count'], inplace=True)
            group = pandas.merge(left = group, right = a_b_station_skip_obs, how="left", on=['a','b','station','skip'])
            group.loc[ (group.skip==0)&(group.obs_count < max_obs_count), "skip_reason"] = "fewer observations than {}".format(max_obs_count)
            group.loc[ (group.skip==0)&(group.obs_count < max_obs_count), "skip"       ] = 1

            # useable: count unique (a,b,station,skip)-tuples where skip==0
            group['unique_a_b_station'] = 1-group.duplicated(subset=['a','b','station','skip'])
            useable = group.loc[group.skip==0, 'unique_a_b_station'].sum()
            logging.debug(f"  Group has length {len(group)} and skips {group.skip.sum()} with {useable} remaining useable (a,b,station) tuples")
            if useable <= 1: 
                AB_station = pandas.concat([AB_station, group[groupby_cols+["station","skip","skip_reason"]]])
                continue

        # if there are more than two remaining, use distlink to break the tie
        # todo: it would be preferable to choose the median daily value or closest to the middle of the link but those are more work
        min_distlink    = group.loc[ group.skip==0, "distlink"].min()
        bigger_distlink = group.loc[ (group.skip==0)&(group.distlink > min_distlink) ]
        if len(bigger_distlink) > 0:
            logging.debug(f"  Skipping {len(bigger_distlink)} rows due to having bigger distlink than {min_distlink}")
            group.loc[ (group.skip==0)&(group.distlink > min_distlink), "skip_reason"] = "bigger distlink than {}".format(min_distlink)
            group.loc[ (group.skip==0)&(group.distlink > min_distlink), "skip"       ] = 1

            # useable: count unique (a,b,station,skip)-tuples where skip==0
            group['unique_a_b_station'] = 1-group.duplicated(subset=['a','b','station','skip'])
            useable = group.loc[group.skip==0, 'unique_a_b_station'].sum()
            logging.debug(f"  Group has length {len(group)} and skips {group.skip.sum()} with {useable} remaining useable (a,b,station) tuples")
            if useable <= 1: 
                AB_station = pandas.concat([AB_station, group[groupby_cols+["station","skip","skip_reason"]]])
                continue

        # if min distlink didn't do it, use station number to break the tie (yes, this happens)
        min_station    = group.loc[ group.skip==0, "station"].min()
        bigger_station = group.loc[ (group.skip==0)&(group.station > min_station) ]
        if len(bigger_station) > 0:
            logging.debug(f"  Skipping {len(bigger_distlink)} rows arbitrarily (station num) {min_distlink}")
            group.loc[ (group.skip==0)&(group.station > min_station), "skip_reason"] = "random (non-min station)"
            group.loc[ (group.skip==0)&(group.station > min_station), "skip"       ] = 1

            # useable: count unique (a,b,station,skip)-tuples where skip==0
            group['unique_a_b_station'] = 1-group.duplicated(subset=['a','b','station','skip'])
            useable = group.loc[group.skip==0, 'unique_a_b_station'].sum()
            logging.debug(f"  Group has length {len(group)} and skips {group.skip.sum()} with {useable} remaining useable (a,b,station) tuples")
            if useable <= 1: 
                AB_station = pandas.concat([AB_station, group[groupby_cols+["station","skip","skip_reason"]]])
                continue

        # this shouldn't happen -- but it's useful when constructing above logic
        logging.debug(group)
        logging.debug(group[["a","b","station","distlink","skip","skip_reason"]])
        value = input("Type any key to continue...\n")
        break

    # purge observed data with known bad crosswalk
    if args.pems_year:
        AB_station.loc[ AB_station.station.isin(PEMS_BAD_STATION_CROSSWALK), "skip_reason" ] = "known bad crosswalk"
        AB_station.loc[ AB_station.station.isin(PEMS_BAD_STATION_CROSSWALK), "skip"        ] = 1

    # make sure a,b,station are unique in AB_station
    AB_station.drop_duplicates(keep='first', inplace=True)
    AB_station_dupes = AB_station.loc[ AB_station.duplicated(subset=['a','b','station'], keep=False) ]
    logging.debug(f"AB_station duplicates len={len(AB_station_dupes):,}:\n{AB_station_dupes}")
    # when there are multiple for a,b, station - keep the first
    AB_station.drop_duplicates(subset=['a','b','station'], keep='first', inplace=True)

    # brink skip, skip_reason back to table_wide
    logging.debug(f"AB_station len={len(AB_station)} head=\n{AB_station.head(12)}")
    logging.debug(f'table_wide before merge with AB_station - length={len(table_wide):,} head=\n{table_wide.head(30)}')
    table_wide = pandas.merge(
        left=table_wide, 
        right=AB_station, 
        how="left", 
        on=['a','b','station'],
        validate='many_to_one')
    logging.debug(f'table_wide after merge with AB_station - length={len(table_wide):,} head=\n{table_wide.head(30)}')
    logging.debug(f'table_wide for station 400868=\n{table_wide.loc[table_wide.station==400868]}')

    # bring the attributes "lanes match", "ft", "county", "skip", "skip_reason" back to non-wide table
    lanes_match_df = table_wide[index_cols + ["lanes match","ft","county","skip","skip_reason"]].drop_duplicates()
    logging.debug(f"lanes_match_df head:\n{lanes_match_df.head()}")
    table_long = pandas.merge(left=table_long, right=lanes_match_df, how='left', on=index_cols, suffixes=("","_temp"))
    table_long.loc[ pandas.isnull(table_long.ft    )&pandas.notnull(table_long.ft_temp    ), "ft"    ] = table_long.ft_temp
    table_long.loc[ pandas.isnull(table_long.county)&pandas.notnull(table_long.county_temp), "county"] = table_long.county_temp
    table_long.rename(columns={"lanes match_temp":"lanes match"}, inplace=True)
    table_long.drop(columns=["ft_temp","county_temp"], inplace=True)
    logging.debug(f"table_long len={len(table_long)} head:\n{table_long.head()}")

    # write non-wide
    table_long.to_csv(f"{out_file}.csv", index=False)
    logging.info(f"Wrote {len(table_long)} lines to {out_file}.csv")

    # write wide
    table_wide.to_csv(f"{out_file}_wide.csv", index=False)
    logging.info(f"Wrote {len(table_wide)} lines to {out_file}_wide.csv")
