USAGE = """

    This script is run after the Travel Model and the two post-processing scripts
    (BetweenZonesVMT.py and CreateSpeedBinsWithinZones.job).
    Typically it is run automatically by the batch file RunPrepareEmfac.bat.

    It reads the files generated by BetweenZonesVMT.py and CreateSpeedBinsWithinZones.job;
    calculates the hourly VMT fractions by speed bin and subarea.

    If args.VMT_data_type == VMTbyVehFuelType, it applies the
    EMFAC Default VMT shares by Veh_Tech (per GAI/Sub-Area) and 
    applies those to the modeled VMT (per GAI/Sub-Area). 

    The custom activity file serves as an input to EMFAC. 

    See https://github.com/BayAreaMetro/modeling-website/wiki/EMFAC-settings-by-analysis-type
    for information on EMFAC input parameters.

    Also see EMFAC Run Log.xlsx (https://mtcdrive.box.com/s/mz3poeaja4nxt417k4xr8t58j6w7qy9p)
    for past EMFAC settings used.

    Note: For Area, choosing any one of the following is *equivalent*.  The only difference
    is that the resulting template will have as the first column 'County', 'MPO', or 'Sub-Area'
    but the actual Sub-areas (column C) and data will be the same.
       1) County - and then selecting the 9 Bay Area Counties 
       2) MPO - and then selecting MTC
       3) Sub-Area - and then selecting the 11 Sub-Areas in the 9 Bay Area Counites

    Inputs: 
      1) [OUTPUT/]emfac/emfac_prep/CreateSpeedBinsBetweenZones_sums.csv
      2) [OUTPUT/]emfac/emfac_prep/CreateSpeedBinsWithinZones_sums.csv
    Outputs:
      1) [OUTPUT/]emfac/{args.analysis_type}/  (one of ['SB375','EIR','AQConformity'])
           E{args.emfac_version}/
             E{args.emfac_version}_{MODEL_RUN_ID}_{args.season}.xlsx  => Custom Activity File
             E{args.emfac_version}_{MODEL_RUN_ID}_{args.season}.log   => Log file
"""
import argparse, logging, os, pathlib, re, sys
import openpyxl
import openpyxl.utils.dataframe
import pandas as pd

# -------------------------------------------------------------------
# Input/output file names and locations
# -------------------------------------------------------------------
CWD          = pathlib.Path.cwd()
MODEL_RUN_ID = CWD.name  # run this from M_DIR or model run dir on modeling machine
CWD_OUTPUT   = CWD / "OUTPUT"

# Figure out if M_DIR (with OUTPUT) or modeling machine (without OUTPUT)
if CWD_OUTPUT.exists():
    EMFAC_PREP = CWD_OUTPUT / "emfac/emfac_prep"
else:
    CWD_OUTPUT = CWD
    EMFAC_PREP = CWD / "emfac/emfac_prep"

# These are the max speed for the row, so speed < [this value]
# TODO: It would be less error prone if these were just in the input files rather than recoded here
SPEED_BIN_TO_LABEL = [
    [ 1,  "5mph"],
    [ 2, "10mph"],
    [ 3, "15mph"],
    [ 4, "20mph"],
    [ 5, "25mph"],
    [ 6, "30mph"],
    [ 7, "35mph"],
    [ 8, "40mph"],
    [ 9, "45mph"],
    [10, "50mph"],
    [11, "55mph"],
    [12, "60mph"],
    [13, "65mph"],
    [14, "70mph"],
    [15, "75mph"],
    [16, "80mph"],
    [17, "85mph"],
    [18, "90mph"],
]
SPEED_BIN_TO_LABEL_DF = pd.DataFrame.from_records(SPEED_BIN_TO_LABEL, columns=['speedBin','speedBin_label'])

# Mapping from county to Air Basin number
# https://ww2.arb.ca.gov/applications/emissions-air-basin
# SF = San Francisco Bay Area
# SV = Sacramento Valley
# NC = North Coast
COUNTY_GAI_AIRBASIN = [
    # countyName        GAI (geographic index)  AirBasin
    ["Alameda",         39,                     "SF"],
    ["Contra Costa",    40,                     "SF"],
    ["Marin",           41,                     "SF"],
    ["Napa",            42,                     "SF"],
    ["San Francisco",   43,                     "SF"],
    ["San Mateo",       44,                     "SF"],
    ["Santa Clara",     45,                     "SF"],
    ["Solano",          46,                     "SF"],
    ["Solano",          33,                     "SV"],
    ["Sonoma",          47,                     "SF"],
    ["Sonoma",          22,                     "NC"]
]
COUNTY_GAI_AIRBASIN_DF = pd.DataFrame.from_records(COUNTY_GAI_AIRBASIN, columns=['countyName', 'GAI', 'AirBasin'])

# The Travel Model doesn't have the concept of Veh_Tech, so
# - For the following 9 technologies, use the travel model's hourly fraction
# - For the rest, keep the default hourly fraction from the EMFAC template
USE_TRAVEL_MODEL_VMT_FOR_VEH_TECH = [
    'LDA - Dsl',
    'LDA - Gas',
    'LDT1 - Dsl',
    'LDT1 - Gas',
    'LDT2 - Dsl',
    'LDT2 - Gas',
    'MCY - Gas',
    'MDV - Dsl',
    'MDV - Gas'
]

# args.VMT_data_type -> sheetname
ACTIVITY_TEMPLATE_VMT_SHEETNAME = {
    'totalDailyVMT':    'Daily_Total_VMT',
    'VMTbyVehFuelType': 'Daily_VMT_By_Veh_Tech'
}

def overwrite_worksheet_data(sheet, sheet_cols, sheet_index_cols, new_data_df):
    """
    Overwrites data in a worksheet with new data.
    Verifies that the index columns haven't changed and only overwrites the rest

    Args:
        sheet (openpyxl.worksheet.worksheet.Worksheet): Worksheet to update
        sheet_cols (list of strings): list of columns; these are in row 1 starting at column 1
        sheet_cols (list of strings): list of index columns; these are in row 1 starting at column 1
        new_data_df (pd.DataFrame): new data frame to replace contents
    """
    # there's probably a better way to do this but this works and preserves formatting
    for row_index, row in new_data_df.iterrows():
        # data starts on row 2 in the sheet (since row 1 is header)
        #             and column 1 in the sheet (since openpyxl doesn't use 0-based column indexing)
        for col_index in range(len(sheet_cols)):
            # verify index columns
            if sheet_cols[col_index] in sheet_index_cols:
                assert(sheet.cell(row=row_index+2, column=col_index+1).value == row[sheet_cols[col_index]])
            else:
                #  update these
                sheet.cell(row=row_index+2, column=col_index+1).value = row[sheet_cols[col_index]]
    return

if __name__ == '__main__':

    parser = argparse.ArgumentParser(description=USAGE, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("--analysis_type",                 required=True, choices=['SB375','EIR','AQConformity'],
                        help="IF SB375 passed, assume 'Generate SB375 Template' is true")
    parser.add_argument("--emfac_version",                 required=True, choices=['2014','2017','2021'])
    parser.add_argument("--run_mode",                      required=True, choices=['emissions','emissions-rates'])
    parser.add_argument("--sub_area",                      required=True, choices=['MPO-MTC','airbasin-SF'], help="Note: MPO_MTC==9 counties==11 subareas")
    parser.add_argument("--season",                        required=True, choices=['annual','summer','winter'])
    parser.add_argument("--VMT_data_type",                 required=True, choices=['totalDailyVMT','VMTbyVehFuelType'])
    parser.add_argument("--custom_hourly_speed_fractions", action='store_true', help="For 'Custom Hourly Speed Fractions' checkbox")
    parser.add_argument("--file_suffix", required=False, help="Optional suffix for output folders/files; to QA/compare against manually created versions")

    args = parser.parse_args()

    # calendar year will come from the MODEL_RUN_ID
    calendar_year = int(MODEL_RUN_ID[:4])
    assert((calendar_year >= 2000) & (calendar_year <= 2050))

    # ================= Input custom activity template file is based on the emfac args =================
    # Note that the template is the same for all seasons, so our templates will just reflect annual
    # And we'll change the custom activity file to reflect args.season later
    input_custom_activity_template = f"E{args.emfac_version}_{args.run_mode}_{args.sub_area}_{calendar_year}_annual_{args.VMT_data_type}"
    # store true are optional add-ins
    if args.custom_hourly_speed_fractions:
        input_custom_activity_template = input_custom_activity_template + "_CustSpeed"
    if args.analysis_type == "SB375":
        input_custom_activity_template = input_custom_activity_template + "_SB375"
    input_custom_activity_template += ".xlsx"

    # and it's in GitHub
    input_custom_activity_template_fullpath = pathlib.Path(__file__).parent / "Custom_Activity_Templates" / input_custom_activity_template

    # ================= Output custom activity file is based on Harold's convention =================
    output_custom_activity_file = f"E{args.emfac_version}_{MODEL_RUN_ID}_{args.season}{args.file_suffix}.xlsx"
    # and it's local
    output_custom_activity_dir = CWD_OUTPUT / "emfac" / args.analysis_type / f"E{args.emfac_version}{args.file_suffix}"
    # create it since log file will go there
    output_custom_activity_dir.mkdir(parents=True, exist_ok=True)
    output_custom_activity_file_fullpath = output_custom_activity_dir / output_custom_activity_file
    log_file_full_path = output_custom_activity_dir / output_custom_activity_file.replace(".xlsx",".log")

    # check if input_custom_activity_template_fullpath exists
    # if not, raise NotImplementedError 
    if not input_custom_activity_template_fullpath.exists():
        error_str = f"Custom Activity Template {input_custom_activity_template_fullpath} not found.\n"
        error_str += "These EMFAC paramaters aren't supported yet."
        raise NotImplementedError(error_str)
    # ================= Create logger =================
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)
    # console handler
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p'))
    logger.addHandler(ch)
    # file handler
    fh = logging.FileHandler(log_file_full_path, mode='w')
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p'))
    logger.addHandler(fh)

    logging.info("args: {}".format(args))
    logging.info(f"MODEL_RUN_ID                            = {MODEL_RUN_ID}")
    logging.info(f"input_custom_activity_template_fullpath = {input_custom_activity_template_fullpath}")
    logging.info(f"output_custom_activity_file_fullpath    = {output_custom_activity_file_fullpath}")
    logging.info(f"log_file_full_path                      = {log_file_full_path}")
    logging.debug("SPEED_BIN_TO_LABEL_DF:\n{}".format(SPEED_BIN_TO_LABEL_DF))
    logging.debug("COUNTY_GAI_AIRBASIN_DF:\n{}".format(COUNTY_GAI_AIRBASIN_DF))

    # -------------------------------------------------------------------
    # Read the output csv files from Cube (CreateSpeedBinsBetweenZones and CreateSpeedBinsWithinZones) and reshape them
    # -------------------------------------------------------------------
    logging.info("=====================================================")
    logging.info("Reading modelled VMT csv files from Cube scripts")
    logging.info("=====================================================")

    # read in between zones VMT and intrazonal VMT
    BetweenZones_df = pd.read_csv(EMFAC_PREP / "CreateSpeedBinsBetweenZones_sums.csv")
    WithinZones_df  = pd.read_csv(EMFAC_PREP / "CreateSpeedBinsWithinZones_sums.csv")

    # what is the format of these VMT files?
    # rows    = countyName (9 counties + 1 external) x speed bin (13) + header (1) = 10 x 13 + 1 = 131
    # (in the future, rows = AirBasinName (11 airbains + 1 external) x speed bin (18) + header (1) = 12 x 18 + 1 = 217)
    # columns = 24 hours + 3 index columns (countyName, arbCounty, speedBin) = 27

    numLine_Between = len(BetweenZones_df)
    logging.info(f"  Finished reading CreateSpeedBinsBetweenZones_sums.csv; Read {numLine_Between} rows")
    # strip spaces from column names
    BetweenZones_df.columns = BetweenZones_df.columns.str.replace(" ","")
    # set index columns
    BetweenZones_df.set_index(["countyName","arbCounty","speedBin"], inplace=True)
    logging.debug("head:\n{}".format(BetweenZones_df.head()))

    numLine_Within = len(WithinZones_df.index)
    logging.debug(f"Finished reading CreateSpeedBinsWithinZones_sums.csv; Read {numLine_Within} rows")
    # strip spaces from column names
    WithinZones_df.columns = WithinZones_df.columns.str.replace(" ","")
    # convert arbCounty and speedBin to int
    WithinZones_df["arbCounty"] = WithinZones_df["arbCounty"].astype(int)
    WithinZones_df["speedBin"] = WithinZones_df["speedBin"].astype(int)
    # set index columns
    WithinZones_df.set_index(["countyName","arbCounty","speedBin"], inplace=True)
    logging.debug("head:\n{}".format(WithinZones_df.head()))

    logging.info("Start reshaping the modelled VMT data")
    # add the between zones VMT and intrazonal VMT together
    VMT_df = BetweenZones_df.add(WithinZones_df, fill_value=0)
    HOUR_LIST = VMT_df.columns.to_list()
    logging.debug("VMT_df.head:\n{}".format(VMT_df.head()))
    logging.info("HOUR_LIST={}".format(HOUR_LIST))

    # drop external VMT. arbCounty is index level 1
    VMT_df_len = len(VMT_df)
    VMT_df = VMT_df.loc[VMT_df.index.get_level_values(1) != 9999]
    logging.info("Dropped external VMT; rows changed from {} to {}".format(VMT_df_len, len(VMT_df)))

    # reset index
    VMT_df.reset_index(drop=False, inplace=True)
    # drop arbCounty
    VMT_df.drop(columns=['arbCounty'], inplace=True)
    logging.debug("VMT_df.head:\n{}".format(VMT_df.head()))
    countyName_list = VMT_df['countyName'].unique().tolist()
    logging.info("countyName_list: {}".format(countyName_list))

    # VMT_df has countyName x speedBin but the speedBins might be short
    # so expand to full speedBins by creating countyName x speedBim list
    speed_bin_label_by_county_df = pd.merge(
        VMT_df[['countyName']].drop_duplicates(),
        SPEED_BIN_TO_LABEL_DF,
        how="cross"
    )
    logging.debug("speed_bin_label_by_county_df:\n{}".format(speed_bin_label_by_county_df))

    # fill out VMT_df this way
    VMT_df = pd.merge(
        left  = speed_bin_label_by_county_df,
        right = VMT_df,
        how   = 'outer',
        on    = ['speedBin','countyName']
    )
    # fill in missing values
    fill_vals = {hour_label:0.0 for hour_label in HOUR_LIST}
    VMT_df.fillna(value=fill_vals, inplace=True)
    logging.debug("VMT_df:\n{}".format(VMT_df))

    # join with GAI, AirBasin
    # this will create duplicates for Sonoma and Solano, which both straddle two Air Basins
    VMT_df = pd.merge(
        left     = COUNTY_GAI_AIRBASIN_DF,
        right    = VMT_df,
        how      = 'right',
        on       = 'countyName',
    )
    logging.debug("VMT_df:\n{}".format(VMT_df))
    logging.debug("VMT_df.tail(30):\n{}".format(VMT_df.tail(30)))
    # columns are: countyName, speedBin, speedBin_label, hour[01-24]
    # change to: countyName, Hour; moving speedBin_label to columns
    VMT_df.drop(columns=['speedBin'], inplace=True)
    # convert to long: columns: countyName, GAI, AirBasin, speedBin_Label, hour, VMT
    VMT_df = pd.melt(
        VMT_df,
        id_vars=['countyName','GAI','AirBasin','speedBin_label'], 
        var_name='hour',
        value_name='VMT')
    VMT_df['hour'] = VMT_df.hour.str[4:].astype(int)
    logging.debug("long VMT_df:\n{}".format(VMT_df))
    # pivot to wide with speedBin_label as columns
    VMT_df = pd.pivot(
        VMT_df,
        columns='speedBin_label',
        index=['countyName','GAI','AirBasin','hour'])
    # columns have two levels: VMT, speedBin_label -- only need one
    VMT_df.columns = VMT_df.columns.get_level_values(1)
    VMT_df.columns.name = None
    # use sort order in SPEED_BIN_TO_LABEL_DF
    VMT_df = VMT_df[SPEED_BIN_TO_LABEL_DF.speedBin_label.tolist()]
    logging.debug("reshaped VMT_df:\n{}".format(VMT_df))

    # total vmt by hour
    VMT_df['HourlyTotalVMT'] = VMT_df.sum(axis='columns')
    # calculate fraction of vmt by speedBin
    for speedBin_label in SPEED_BIN_TO_LABEL_DF.speedBin_label.tolist():
        VMT_df[f'HourlyFraction_{speedBin_label}'] = VMT_df[speedBin_label] / VMT_df['HourlyTotalVMT']

    # finally, reset_index. columns are now countyName, GAI, AirBasin, hour, [speedBin_label list], [HourlyFraction_speedBin_label] 
    VMT_df.reset_index(drop=False, inplace=True)
    logging.debug("VMT_df:\n{}".format(VMT_df))
    logging.debug("VMT_df columns:\n{}".format(VMT_df.columns))
    # VMT_df.to_csv(EMFAC_PREP / "VMT_df.csv", header=True, index=True)

    logging.info("================================================================")
    logging.info("Reading from and writing to the EMFAC Custom Activity Template")
    logging.info("================================================================")

    workbook = openpyxl.load_workbook(filename=input_custom_activity_template_fullpath)
    logging.info("Workbook sheetnames: {}".format(workbook.sheetnames))

    # custom activity template is annual
    # if args.season differs, update settings
    if args.season != 'annual':
        logging.info("-------------settings --------------------")
        sheet = workbook["Settings"]

        logging.info("sheet[A3].value=[{}]".format(sheet["A3"].value))
        assert(sheet["A3"].value == "Season/Month")
        sheet["B4"] = args.season.capitalize()
        logging.info(f"Saved Settings > Season/Month to {args.season.capitalize()}")

    # -------------------------------------------------------------------
    # Reading from and writing to the EMFAC Custom Activity Template
    # Part 1: custom_hourly_speed_fractions
    # -------------------------------------------------------------------
    if args.custom_hourly_speed_fractions:
        logging.info("------------- custom_hourly_speed_fractions --------------------")
        logging.info(f"Loading the workbook {input_custom_activity_template_fullpath}")

        # select the hourly fraction worksheet
        sheet = workbook["Hourly_Fraction_Veh_Tech_Speed"]
        DefaultHourlyFraction = sheet.values

        # Set the first row as the headers for the DataFrame
        sheet_cols = next(DefaultHourlyFraction)
        sheet_cols = list(sheet_cols) # convert to list
        DefaultHourlyFraction = list(DefaultHourlyFraction)

        DefaultHourlyFraction_df = pd.DataFrame(DefaultHourlyFraction, columns=sheet_cols)
        logging.info("Finished reading <Hourly_Fraction_Veh_Tech_Speed>")
        logging.debug("DefaultHourlyFraction_df with columns={}:\n{}".format(sheet_cols, DefaultHourlyFraction_df))
        # log Veh_Tech categories
        Veh_Tech_List = DefaultHourlyFraction_df.Veh_Tech.unique().tolist()
        logging.debug("DefaultHourlyFraction_df.Veh_Tech:\n{}".format(str("\n").join(Veh_Tech_List)))
        
        # Some further notes about the "Hourly_Fraction_Veh_Tech_Speed" worksheet:

        # The "Hourly_Fraction_Veh_Tech_Speed" is essentially a table of subarea (11) x hour (24) x speed bins (18).
        # The rows are subarea (11) x hour (24) x Veh_Tech (51)
        # The columns are the 18 speed bins, plus index columns (Sub-Area, GAI, Sub-Area, Cal_Year, Veh_Tech, Hour)

        # merge DataFrames
        TM_HourlyFraction_df = pd.merge(
            left     = DefaultHourlyFraction_df, 
            right    = VMT_df[['GAI','hour'] + 
                              [f"HourlyFraction_{speedbin}" for speedbin in SPEED_BIN_TO_LABEL_DF.speedBin_label.tolist()]], 
            left_on  = ['GAI','Hour'], 
            right_on = ['GAI','hour'], 
            how      = 'left',
            validate = 'many_to_one'
        )
        logging.debug("TM_HourlyFraction_df:\n{}".format(TM_HourlyFraction_df))
        logging.debug("TM_HourlyFraction_df.columns:\n{}".format(TM_HourlyFraction_df.columns))

        # use the TM version for those rows where Veh_Tech is in USE_TRAVEL_MODEL_VMT_FOR_VEH_TECH
        for speedbin in SPEED_BIN_TO_LABEL_DF.speedBin_label.tolist():
            TM_HourlyFraction_df.loc[ TM_HourlyFraction_df.Veh_Tech.isin(USE_TRAVEL_MODEL_VMT_FOR_VEH_TECH), speedbin] = \
                TM_HourlyFraction_df[f'HourlyFraction_{speedbin}']

        # use original columns - replace all cells
        TM_HourlyFraction_df = TM_HourlyFraction_df[sheet_cols]
        logging.info(f"DefaultHourlyFraction_df.shape:{DefaultHourlyFraction_df.shape}")
        logging.info(f"TM_HourlyFraction_df.shape:{TM_HourlyFraction_df.shape}")
        assert(DefaultHourlyFraction_df.shape == TM_HourlyFraction_df.shape)

        # make list of sheet_index_columns -- these are all of them that don't end if mph
        sheet_index_cols = []
        for sheet_col in sheet_cols: 
            if sheet_col.endswith('mph')==False: sheet_index_cols.append(sheet_col)
        overwrite_worksheet_data(sheet, sheet_cols, sheet_index_cols, TM_HourlyFraction_df)
        logging.info("Wrote Travel Model results to <Hourly_Fraction_Veh_Tech_Speed>")

    # -------------------------------------------------------------------
    # Reading from and writing to the EMFAC Custom Activity Template
    # Part 2: VMT
    # -------------------------------------------------------------------
    logging.info(f"------------- {args.VMT_data_type} --------------------")
    sheet = workbook[ACTIVITY_TEMPLATE_VMT_SHEETNAME[args.VMT_data_type]]

    logging.info(f"Reading in the worksheet <{ACTIVITY_TEMPLATE_VMT_SHEETNAME[args.VMT_data_type]}>")
    DefaultVMT = sheet.values

    # Column A has the first Area categorization: MPO, County or Sub-Area
    sheet_cols = next(DefaultVMT)
    sheet_cols = list(sheet_cols) # convert to list
    DefaultVMT = list(DefaultVMT)
    DefaultVMT_df = pd.DataFrame(DefaultVMT, columns=sheet_cols)
    logging.debug("DefaultHourlyFraction_df with columns={}:\n{}".format(sheet_cols, DefaultVMT))

    # first calculate EMFAC default VMT by GAI
    DefaultVMTbyGAI_df = DefaultVMT_df[['Sub-Area','GAI','New Total VMT']].groupby(['Sub-Area','GAI'], as_index=False).sum()
    # rename the 'New Total VMT' column to avoid confusion
    DefaultVMTbyGAI_df.rename(columns={"New Total VMT": "DefaultVMT_GAI"}, inplace=True)
    logging.debug("DefaultVMTbyGAI_df:\n{}".format(DefaultVMTbyGAI_df))

    # calculate TM VMT by subarea
    ModelledVMTbyGAI_df = VMT_df[['countyName','GAI','AirBasin','HourlyTotalVMT']].groupby(
        ['countyName','GAI','AirBasin'], as_index=False).sum()
    logging.debug("ModelledVMTbyGAI_df:\n{}".format(ModelledVMTbyGAI_df))
    # Note: the Solano/Sonoma VMT by GAI isn't accurate - it's duplicated
    # So need to apportion it based on the DefaultVMT split

    # merge modelled VMT and default VMT by GAI; modeled is called 'HourlyTotalVMT'
    VMTbyGAI_df = pd.merge(
        left    = DefaultVMTbyGAI_df, 
        right   = ModelledVMTbyGAI_df, 
        on      = ['GAI'], 
        how     ='left'
    )
    logging.debug("VMTbyGAI_df with EMFAC Default and Modelled:\n{}".format(VMTbyGAI_df))
    # aggregate DefaultVMT to county and merge that
    VMTbyCounty_df = VMTbyGAI_df[['countyName','DefaultVMT_GAI']].groupby(['countyName'], as_index=False).sum()
    VMTbyCounty_df.rename(columns={'DefaultVMT_GAI':'DefaultVMT_county'}, inplace=True)
    VMTbyGAI_df = pd.merge(
        left    = VMTbyGAI_df,
        right   = VMTbyCounty_df,
        on      = ['countyName'],
        how     = 'left'
    )
    logging.debug("VMTbyGAI_df:\n{}".format(VMTbyGAI_df))
    # columns are now: Sub-Area, GAI, DefaultVMT_GAI, countyName, AirBasin, HourlyTotalVMT (modeled), DefaultVMT_county
    # apportion based on share DefaultVMT for GAI / DefaultVMT for County
    VMTbyGAI_df['HourlyTotalVMT'] = VMTbyGAI_df.HourlyTotalVMT * (VMTbyGAI_df.DefaultVMT_GAI/VMTbyGAI_df.DefaultVMT_county)
    logging.debug("VMTbyGAI_df after apportioning county by GAI:\n{}".format(VMTbyGAI_df))

    # This section is for args.VMT_data_type=='VMTbyVehFuelType'
    # But for args.VMT_data_type=='totalDailyVMT', it doesn't actually do anything (percentVMT will be 1.0)
    # so leave it for simplicity

    # Merge current VMT totals back into the original default VMT dataframe
    DefaultVMT_detail_df = pd.merge(
        left    = DefaultVMT_df,
        right   = VMTbyGAI_df.drop(columns=['DefaultVMT_county']), 
        on      = ['Sub-Area','GAI'],
        how     = 'left'
    )

    # Use Default VMT shares by Veh_Tech (per GAI) and apply to the modeled VMT (per GAI) - which is called HouryTotalVMT
    DefaultVMT_detail_df['percentVMT']   = DefaultVMT_detail_df['New Total VMT'] / DefaultVMT_detail_df['DefaultVMT_GAI']
    DefaultVMT_detail_df['Modeled VMT']  = DefaultVMT_detail_df.HourlyTotalVMT * DefaultVMT_detail_df.percentVMT
    logging.debug("DefaultVMT_detail_df after merge to original dataframe (which may include vehicle/fuel types):\n{}".format(
        DefaultVMT_detail_df))

    # keep the relevant columns for writing to the excel file, but replace 'New Total VMT' with 'Modeled VMT'
    sheet_cols[sheet_cols.index('New Total VMT')] = 'Modeled VMT'
    TM_VMT_df =  DefaultVMT_detail_df[sheet_cols]
    logging.info(f"DefaultVMT_df.shape = {DefaultVMT_df.shape}")
    logging.info(f"TM_VMT_df.shape = {TM_VMT_df.shape}")
    assert(DefaultVMT_df.shape == TM_VMT_df.shape)

    # sheet index cols here are the first 4 (for vmt total) or 5 (for vmt by fuel type)
    if args.VMT_data_type == 'totalDailyVMT':
        sheet_index_cols = sheet_cols[:4]
    else:
        sheet_index_cols = sheet_cols[:5]

    overwrite_worksheet_data(sheet, sheet_cols, sheet_index_cols, TM_VMT_df)
    logging.info(f"Wrote Travel Model results to <{ACTIVITY_TEMPLATE_VMT_SHEETNAME[args.VMT_data_type]}>")

    # the "New Total VMT" in the tab <Daily_VMT_By_Veh_Tech> should be equal to 
    # the "between zone vmt (excluding external zones)" from BetweenZonesVMT.py 
    # plus the "within zone vmt (excluding external zones)" from CreateSpeedBinsWithinZones.job
    # note that *external zones are excluded* when checking totals

    # -------------------------------------------------------------------
    # Metadata
    # -------------------------------------------------------------------
    logging.info(f"------------- metadata --------------------")
    workbook.create_sheet(title="metadata")
    sheet = workbook['metadata']
    sheet["A1"] = "Workbook written by:"
    sheet["B1"] = str(__file__)
    sheet["A2"] = "Custom Activity Template:"
    sheet["B2"] = str(input_custom_activity_template_fullpath)
    sheet["A3"] = "Log file:"
    sheet["B3"] = str(log_file_full_path)

    sheet.column_dimensions['A'].width = 25
    sheet.column_dimensions['B'].width = 160

    # save it once here
    workbook.save(output_custom_activity_file_fullpath)
    logging.info(f"Saved {output_custom_activity_file_fullpath}")
    workbook.close()
