USAGE = """

 Creates a Tableau Data Extract for validating roadway volumes compared to PeMS or CalTrans count data.

 Input:
========
1) avgload5period.csv: model data
   Required columns: a, b, lanes, volEA_tot, volAM_tot, volMD_tot, volPM_tot, volEV_tot

If PeMS years specified:

p2) model_to_pems.csv: maps model roadway links to PeMS stations
    Required columns: a, b, station, HOV

p3) (Box: Share Data\pems-typical-weekday\pems_period.csv): PeMS observed data, generated by
    https://github.com/MetropolitanTransportationCommission/pems-typical-weekday
    Required columns: station, route, direction, time_period, 
                      lanes, median_flow, avg_flow, latitude, longitude, year

If CalTrans years specified:

c2) model_to_caltrans.csv: maps model roadway links to CalTrans count locations
    Required columns: a, b, county, route, postmile, direction, leg, description

c3) (Box: Share Data\caltrans-typical-weekday\typical-weekday-counts.csv): CalTrans observed data,
    generated by https://github.com/BayAreaMetro/caltrans-typical-weekday-counts

Output:
========

If PeMS years specified:

p1) Roadways to PeMS.tde: a tableau data extract containing both modeled and observed data.
    Columns:  a, b, station, route, direction, latitude, longitude, lanes, time_period, 
              volume, category
    Where category is one of 'XXXX Modeled' or 'XXXX Observed', with XXXX being the
    relevant year


If CalTrans years specified:

"""
import argparse, os, sys
import numpy, pandas
import dataextract as tde

PEMS_MAP_FILE       = "model_to_pems.csv"
CALTRANS_MAP_FILE   = "model_to_caltrans.csv"
MODEL_FILE          = "avgload5period.csv"
SHARE_DATA          = os.path.join(os.environ["USERPROFILE"], "Box", "Modeling and Surveys", "Development", "Share Data")
PEMS_FILE           = os.path.join(SHARE_DATA, "pems-typical-weekday", "pems_period.csv")
CALTRANS_FILE       = os.path.join(SHARE_DATA, "caltrans-typical-weekday", "typical-weekday-counts.csv")
PEMS_TDE_FILE       = "Roadways to PeMS"
CALTRANS_TDE_FILE   = "Roadways to Caltrans"

MODEL_COLUMNS       = ['a','b','lanes','volEA_tot','volAM_tot','volMD_tot','volPM_tot','volEV_tot']
PEMS_COLUMNS        = ['station','route','direction','time_period','lanes','avg_flow','latitude','longitude','year']

fieldMap = { 
    'float64' :     tde.Type.DOUBLE,
    'float32' :     tde.Type.DOUBLE,
    'int64' :       tde.Type.DOUBLE,
    'int32' :       tde.Type.DOUBLE,
    'object':       tde.Type.UNICODE_STRING,
    'bool' :        tde.Type.BOOLEAN
}

# 
# from Rdata to TableauExtract.py -- move to library?
# 
def write_tde(table_df, tde_fullpath, arg_append):
    """
    Writes the given pandas dataframe to the Tableau Data Extract given by tde_fullpath
    """
    if arg_append and not os.path.isfile(tde_fullpath):
        print "Couldn't append -- file doesn't exist"
        arg_append = False

    # Remove it if already exists
    if not arg_append and os.path.exists(tde_fullpath):
        os.remove(tde_fullpath)
    tdefile = tde.Extract(tde_fullpath)

    # define the table definition
    table_def = tde.TableDefinition()

    # create a list of column names
    colnames = table_df.columns
    # create a list of column types
    coltypes = table_df.dtypes

    # for each column, add the appropriate info the Table Definition
    for col_idx in range(0, len(colnames)):
        cname = colnames[col_idx]
        ctype = fieldMap[str(coltypes[col_idx])]
        table_def.addColumn(cname, ctype)

    # create the extract from the Table Definition
    if arg_append:
        tde_table = tdefile.openTable('Extract')
    else:
        tde_table = tdefile.addTable('Extract', table_def)
    row = tde.Row(table_def)

    isnull_df = pandas.isnull(table_df)
    # print isnull_df.head()

    for r in range(0, table_df.shape[0]):
        for c in range(0, len(coltypes)):
            try:
                if isnull_df.iloc[r,c]==True:
                    row.setNull(c)
                elif str(coltypes[c]) == 'float64':
                    row.setDouble(c, table_df.iloc[r,c])
                elif str(coltypes[c]) == 'float32':
                    row.setDouble(c, table_df.iloc[r,c])
                elif str(coltypes[c]) == 'int64':
                    row.setDouble(c, table_df.iloc[r,c])
                elif str(coltypes[c]) == 'int32':
                    row.setDouble(c, table_df.iloc[r,c])
                elif str(coltypes[c]) == 'object':
                    row.setString(c, str(table_df.iloc[r,c]))
                elif str(coltypes[c]) == 'bool':
                    row.setBoolean(c, table_df.iloc[r,c])
                else:
                    row.setNull(c)
            except:
                print coltypes[c], colnames[c], table_df.iloc[r,c]
                print table_df.iloc[r,:]
                print isnull_df.iloc[r,:]
                raise

        # insert the row
        tde_table.insert(row)

    tdefile.close()
    print "Wrote %d lines to %s" % (len(table_df), tde_fullpath)

if __name__ == '__main__':

    pandas.options.display.width = 500
    pandas.options.display.max_rows = 1000

    parser = argparse.ArgumentParser(description=USAGE, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("-m","--model_year",    type=int, required=True)
    parser.add_argument("-c","--caltrans_year", type=int, nargs='*')
    parser.add_argument("-p","--pems_year",     type=int, nargs='*')
    args = parser.parse_args()
    if not args.caltrans_year and not args.pems_year:
        print USAGE
        print "No PeMS year nor CalTrans count year argument specified."
        sys.exit()
    print(args)

    ############ read the mapping first
    mapping_df = None
    tde_file   = None
    if args.pems_year:
        mapping_df = pandas.read_csv(PEMS_MAP_FILE)
        tde_file   = PEMS_TDE_FILE
    else:
        mapping_df = pandas.read_csv(CALTRANS_MAP_FILE)
        tde_file   = CALTRANS_TDE_FILE

    # strip the column names
    col_rename = {}
    for colname in mapping_df.columns.values.tolist(): col_rename[colname] = colname.strip()
    mapping_df.rename(columns=col_rename, inplace=True)
    # print mapping_df.head()

    ############ read the model data
    model_df = pandas.read_csv(MODEL_FILE)

    # strip the column names
    col_rename = {}
    for colname in model_df.columns.values.tolist(): col_rename[colname] = colname.strip()
    model_df.rename(columns=col_rename, inplace=True)

    # select only the columns we want
    model_df = model_df[MODEL_COLUMNS]

    # for caltrans, lets make a daily column
    if len(args.caltrans_year) > 0:
        model_df['Daily'] = model_df[['volEA_tot','volAM_tot','volMD_tot','volPM_tot','volEV_tot']].sum(axis=1)

    # create a multi index for stacking
    model_df.set_index(['a','b','lanes'], inplace=True)
    # stack: so now we have a series with multiindex: a,b,lanes,varname
    model_df = pandas.DataFrame({'volume': model_df.stack()})
    # reset the index
    model_df.reset_index(inplace=True)
    # and rename it
    model_df.rename(columns={'level_3':'time_period'}, inplace=True)
    # remove extra chars: 'volAM_tot' => 'AM'
    model_df.loc[model_df['time_period']!='Daily','time_period'] = model_df['time_period'].str[3:5]

    if args.pems_year:
        ############ read the pems data
        obs_df = pandas.read_csv(PEMS_FILE, na_values='NA')
    
        # strip the column names
        col_rename = {}
        for colname in obs_df.columns.values.tolist(): col_rename[colname] = colname.strip()
        obs_df.rename(columns=col_rename, inplace=True)
    
        # select only the columns we want
        obs_df = obs_df[PEMS_COLUMNS]
        # select only the years in question
        obs_df = obs_df[ obs_df['year'].isin(args.pems_year)]
    
        # get just the location information
        pems_location_df = obs_df[['station','route','direction','latitude','longitude']]
        # move NA lat,long to the end... but if the stationn moves, this is kind of silly  
        pems_location_df = pems_location_df.sort(columns=['latitude','longitude'])
        pems_location_df.drop_duplicates(subset='station', inplace=True)
        # add it to the mapping
        mapping_df = pandas.merge(left=mapping_df, right=pems_location_df, how='left')

        # create missing cols in PeMS
        obs_df['HOV'] = -1
        obs_df.rename(columns={'avg_flow':'volume', 'year':'category'}, inplace=True)
        obs_df['category'] = obs_df.category.map(str) + ' Observed'
    else:
        obs_df = pandas.read_csv(CALTRANS_FILE)
        # make columns conform to previous version and to model data
        obs_df.rename(columns={"route":"ROUTE","county":"COUNTY","description":"DESCRIP",
                      "direction":"DIR", "leg":"LEG","station":"STATION","post_mile":"PM"}, inplace=True)

        # select the relevant years
        obs_df = obs_df.loc[ obs_df.year.isin(args.caltrans_year)]

        # set the time_period
        obs_df["time_period"] = "EV"
        obs_df.loc[(obs_df.integer_hour >=  3)&(obs_df.integer_hour <  6), "time_period"] = "EA"
        obs_df.loc[(obs_df.integer_hour >=  6)&(obs_df.integer_hour < 10), "time_period"] = "AM"
        obs_df.loc[(obs_df.integer_hour >= 10)&(obs_df.integer_hour < 15), "time_period"] = "MD"
        obs_df.loc[(obs_df.integer_hour >= 15)&(obs_df.integer_hour < 19), "time_period"] = "PM"

        # create postmile rounded
        obs_df['POSTMILE_INT'] = numpy.round(obs_df['PM']).astype(int)

        # get Daily by year and add it
        id_vars = ["ROUTE","COUNTY","POSTMILE_INT","PM","LEG","DIR","STATION","DESCRIP"]
        obs_daily_df = obs_df.groupby(id_vars + ["year"]).aggregate({"integer_hour":"count","avg_count":"sum", "days_observed":"mean"})
        obs_daily_df["time_period"] = "Daily"
        obs_daily_df.reset_index(inplace=True)
        #      ROUTE COUNTY  POSTMILE_INT      PM LEG DIR  STATION                         DESCRIP  year  days_observed      avg_count  integer_hour time_period
        # 0       12    NAP           2.0   2.300   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2014      33.958333   18502.722816            24       Daily
        # 1       12    NAP           2.0   2.300   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2015      58.958333   19655.098480            24       Daily
        # 2       12    NAP           2.0   2.300   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2016      39.958333   20310.676923            24       Daily
        # 3       12    NAP           2.0   2.300   B   W    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2014      33.750000   18636.507186            24       Daily
        # 4       12    NAP           2.0   2.300   B   W    906.0  .2-MI N/O NAPA/SOLANO COUNTY L  2015      58.958333   20144.982466            24       Daily
        obs_df = pandas.concat([obs_df, obs_daily_df], axis="index")

        # group to year/time_period
        obs_wide = obs_df.groupby(id_vars + ["year","time_period"]).aggregate(
            {"integer_hour":"count", "median_count":"sum", "avg_count":"sum", "days_observed":"mean"}).reset_index()
        obs_wide = pandas.pivot_table(obs_wide, index=id_vars + ["time_period"], columns=["year"], values="avg_count")
        # print(obs_annual_df.head())
        # year  ROUTE COUNTY  POSTMILE_INT   PM LEG DIR  STATION                         DESCRIP time_period          2014          2015          2016
        # 0        12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L          AM   2916.882353   3117.661017   3187.650000
        # 1        12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L       Daily  18502.722816  19655.098480  20310.676923
        # 2        12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L          EA    310.941176    336.203390    351.200000
        # 3        12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L          EV   4072.205882   4081.437463   4329.701923
        # 4        12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L          MD   4932.352941   5198.661017   5421.075000
        rename_cols = {}
        for year in args.caltrans_year: rename_cols[year] = "{} Observed".format(year)
        obs_wide.rename(columns=rename_cols, inplace=True)
        obs_wide["Average Observed"] = obs_wide[rename_cols.values()].mean(axis=1)  # this will not include NaNs or missing vals so it handles them correctly

        obs_df = obs_wide.stack().reset_index()
        obs_df.rename(columns={"year":"category", 0:"volume"}, inplace=True)
        print(obs_df.head())
        #    ROUTE COUNTY  POSTMILE_INT   PM LEG DIR  STATION                         DESCRIP time_period          category        volume
        # 0     12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L          AM     2014 Observed   2916.882353
        # 1     12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L          AM     2015 Observed   3117.661017
        # 2     12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L          AM     2016 Observed   3187.650000
        # 3     12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L          AM  Average Observed   3074.064457
        # 4     12    NAP           2.0  2.3   B   E    906.0  .2-MI N/O NAPA/SOLANO COUNTY L       Daily     2014 Observed  18502.722816

    # model has a, b
    obs_df['a'] = -1
    obs_df['b'] = -1

    # create the final stacked table -- first the model information
    model_final_df = pandas.merge(left=mapping_df, right=model_df, how='inner')
    model_final_df['category'] = '%d Modeled' % args.model_year

    print "Model columns: ", sorted(model_final_df.columns.values.tolist())
    print "Obsrv columns: ", sorted(obs_df.columns.values.tolist())
    # followed by the observed
    table_df = model_final_df.append(obs_df)
    # write_tde(table_df, "%s.tde" % tde_file, arg_append=False)

    # CALTRANS - want a "wide" version, with a column for obsy1, obsy2, obsy3, modeled
    # PeMS - we're done
    if args.pems_year:
        sys.exit(0)

    model_wide = model_final_df[['a','b','COUNTY','ROUTE','POSTMILE_INT','DIR','LEG','lanes','time_period','volume']].copy()
    model_wide.rename(columns={"volume":"{} Modeled".format(args.model_year)}, inplace=True)

    obs_wide.reset_index(inplace=True)
    table_wide = pandas.merge(left=model_wide, right=obs_wide, how='outer', on=["COUNTY","ROUTE","POSTMILE_INT","DIR","LEG","time_period"])


    write_tde(table_wide, "%s_wide.tde" % tde_file, arg_append=False)