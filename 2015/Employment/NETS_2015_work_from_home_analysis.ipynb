{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as P\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm, matplotlib.font_manager as fm\n",
    "sns.set(style=\"darkgrid\")\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nets = pd.read_csv(os.path.join(drop,'Data/nets2015_long.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from shapely.geometry import Point\n",
    "#from fiona.crs import from_epsg\n",
    "#import geopandas as gpd\n",
    "#from pyproj import Proj\n",
    "#from geopandas.tools import sjoin\n",
    "#from shapely.geometry import Point\n",
    "#from fiona.crs import from_epsg\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import shapely\n",
    "#from platform import python_version\n",
    "\n",
    "#print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "def comma_format(x, p):\n",
    "    return '$'+format(x, \"6,.0f\").replace(\",\", \".\")\n",
    "def percent_format(x, p):\n",
    "    return format(x, \"6,.02f\").replace(\",\", \".\")\n",
    "percent_format = lambda x,pos: '{:,.0f}%'.format(x)\n",
    "comma_format = lambda x,pos: '{:,.0f}'.format(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "print(\"Current size:%s\" %fig_size)\n",
    " \n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 11\n",
    "fig_size[1] = 8.5\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "print(\"Current size:%s\" %fig_size)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct=lambda x: x/x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def easylabeler(breaks,currency=True,prefix='AGE'):\n",
    "    \"\"\"\n",
    "    turn list of breaks into list of strings for binning / cutting.\n",
    "    Parameters\n",
    "    ----------\n",
    "    breaks : list-like\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list of named strings\n",
    "    \"\"\"\n",
    "    \n",
    "    if currency:\n",
    "        ptrn_main=\"${fr:,.0f} - ${to:,.0f}\"\n",
    "        ptrn_first='Less than ${dt:,.0f}'\n",
    "        ptrn_last='Greater than ${dt:,.0f}'\n",
    "    else:\n",
    "        ptrn_main=\"{fr:,.0f} - {to:,.0f}\"\n",
    "        ptrn_first='Less than {dt:,.0f}'\n",
    "        ptrn_last='Greater than {dt:,.0f}'\n",
    "    \n",
    "    difflabels=[]\n",
    "    for f in range(len(breaks)-1):\n",
    "        \n",
    "        difflabels.append(ptrn_main.format(fr=breaks[f],to=breaks[f+1]-1))\n",
    "    ## fix first, last boundary entries\n",
    "    difflabels[0]=ptrn_first.format(dt=breaks[1])\n",
    "    difflabels[-1]=ptrn_last.format(dt=breaks[-2])\n",
    "    return difflabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load / define a few mapping files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get SOC, NAICS mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_DIR = '/Users/aolsen/Box'\n",
    "if (os.environ[\"USERNAME\"])==\"lzorn\":\n",
    "    BOX_DIR = 'C:\\\\Users\\\\lzorn\\\\Box'\n",
    "\n",
    "INPUT_PATH  = os.path.join(BOX_DIR, 'Horizon and Plan Bay Area 2050/Blueprint/Final Blueprint Modeling/Protesting Polar Bears/NETS analysis/inputs')\n",
    "OUTPUT_PATH = os.path.join(BOX_DIR, 'Horizon and Plan Bay Area 2050/Blueprint/Final Blueprint Modeling/Protesting Polar Bears/NETS analysis')\n",
    "NETS_PATH   = os.path.join(BOX_DIR, 'DataViz Projects/Data Analysis and Visualization/National Establishment Time Series')\n",
    "THIS_SCRIPT = 'https://github.com/BayAreaMetro/petrale/blob/master/applications/travel_model_lu_inputs/2015/Employment/NETS_2015_work_from_home_analysis.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc = pd.read_excel(os.path.join(INPUT_PATH,'soc_structure_2010.xls'),skiprows=11,\n",
    "                   names=[u'Major Group', u'Minor Group', u'Broad Group', u'Detailed Occupation', u'Description'])\n",
    "\n",
    "soc['maj']=soc['Major Group'].fillna('').str.split('-').apply(lambda x: x[0])\n",
    "\n",
    "def classifier(df):\n",
    "    \"\"\"\n",
    "    we need to classify each row with the appropriate hierarchy level.\n",
    "    for each row, we want to get the rightmost value available--\n",
    "    that represents the finest grained detail class\n",
    "    \"\"\"\n",
    "    \n",
    "    x = df.iloc[:4].tolist()\n",
    "    try:\n",
    "        out = next(s for s in x if not s is np.NaN)\n",
    "    except:\n",
    "        out = None\n",
    "    return  out\n",
    "\n",
    "def classlevel(s):\n",
    "    \n",
    "    try:\n",
    "        if s[3:]=='0000':\n",
    "            return 'major'\n",
    "        elif np.float64(s[3:]) % 100==0:\n",
    "            return 'minor'\n",
    "        elif np.float64(s[3:]) % 10==0:\n",
    "            return 'broad'\n",
    "        else:\n",
    "            return 'detail'\n",
    "    except:\n",
    "        return 'none'\n",
    "\n",
    "soc['class']=soc.iloc[:,:4].apply(classifier,axis=1)\n",
    "soc['hierarchy']=soc['class'].fillna('-1').map(classlevel)\n",
    "\n",
    "soc[['class','hierarchy','Description']].iloc[1200:1202]\n",
    "\n",
    "soc[['class','hierarchy','Description']].groupby('hierarchy').size()\n",
    "soc['Description']=soc.Description.fillna('0').str.lower()\n",
    "#soc.index=SOC['SOCP_2']\n",
    "\n",
    "soc['SOCP_2']=soc['class'].apply(lambda x: str(x)[:2])\n",
    "majorclass = soc.loc[soc['Major Group'].notnull(),['SOCP_2','Description']].copy()\n",
    "majorclass.index=majorclass.SOCP_2\n",
    "majorclass=majorclass.Description.to_dict()\n",
    "\n",
    "soc=soc[soc['class'].notnull()]\n",
    "soc['class']=soc['class'].str.replace('-','')\n",
    "#soc[soc.hierarchy=='major'].to_csv(os.path.join(drop,'Documents/Data/_BLS/SOC/soc_structure_2010_major.csv'))\n",
    "\n",
    "soc_map=soc[soc.hierarchy=='detail'].set_index('Detailed Occupation').Description\n",
    "\n",
    "soc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get abag-naics mapping\n",
    "names=['NAICS','ABAGSector','mtc11','mtc6','EDDSector','acsnames','acscensus']\n",
    "naics_mappings = pd.read_excel(os.path.join(INPUT_PATH, 'NAICS_to_ABAG_SECTORS.xlsx'),sheet_name='both')\n",
    "naics_mappings.columns=names\n",
    "naics_mappings['NAICS']=naics_mappings['NAICS'].astype(str)\n",
    "\n",
    "#naics_abag = pd.read_csv(os.path.join(drop, r'Documents\\Data\\BayArea\\Projections 2013\\NAICS_to_ABAG_SECTORS.csv'),sep='\\t',dtype=object)\n",
    "\n",
    "naics_abag = naics_mappings.set_index('NAICS').mtc11.to_dict()\n",
    "naics_mtc = naics_mappings.set_index('NAICS').mtc6.to_dict()\n",
    "naics_abag_11 = naics_mappings.set_index('NAICS')['mtc11'].to_dict()\n",
    "naics_mappings.mtc11=naics_mappings.mtc11.str.strip()\n",
    "naics_mappings.mtc6=naics_mappings.mtc6.str.strip()\n",
    "naics_mappings.groupby(['acsnames','mtc11']).size().reset_index()\n",
    "naics_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taz = gpd.read_file(os.path.join(drop,'Documents/Data/GIS/zones1454.shp'),crs=from_epsg(3740)).to_crs(from_epsg(3740))\n",
    "# #taz['county']=taz.county_mtc.map(countymap)\n",
    "# taz=taz[taz.geometry.notnull()]\n",
    "\n",
    "# taz['geometry']=taz.buffer(.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## read naics-3 values from file\n",
    "# naicsmap3 =  pd.read_csv(os.path.join(drop,'Documents/Data/Classification/naics_3.csv'),sep='\\t',\n",
    "#   dtype={'Naics_3': object, 'description': object}).set_index('Naics_3').description.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## naics-2 dict\n",
    "naicsmap = {'11':'Agriculture, Forestry, Fishing and Hunting',\n",
    "    '21': 'Mining, Quarrying, and Oil and Gas Extraction',\n",
    " '22': 'Utilities',\n",
    " '23': 'Construction',\n",
    " '31': 'Manufacturing',\n",
    " '32': 'Manufacturing',\n",
    " '33': 'Manufacturing',\n",
    " '42': 'Wholesale Trade',\n",
    " '44': 'Retail Trade',\n",
    " '45': 'Retail Trade',\n",
    " '48': 'Transportation and Warehousing',\n",
    " '49': 'Transportation and Warehousing',\n",
    " '51': 'Information',\n",
    " '52': 'Finance and Insurance',\n",
    " '53': 'Real Estate and Rental and Leasing',\n",
    " '54': 'Professional, Scientific, and Technical Services',\n",
    " '55': 'Management of Companies and Enterprises',\n",
    " '56': 'Administrative and Support and Waste Management and Remediation Services',\n",
    " '61': 'Educational Services',\n",
    " '62': 'Health Care and Social Assistance',\n",
    " '71': 'Arts, Entertainment, and Recreation',\n",
    " '72': 'Accommodation and Food Services',\n",
    " '81': 'Other Services except Public Administration',\n",
    " '92': 'Public Administration',\n",
    " '99': 'Unclassified Establishments'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## define dicts relating the field name in the data to the year it represents (e.g. NAICS01: 2001)\n",
    "# ## we use later to extract the year\n",
    "\n",
    "# fipstypemap = dict([('fips{:02d}'.format(int(str(x)[2:])),np.dtype('a5')) for x in range(1990,2016)]+\\\n",
    "#               [('naics{:02d}'.format(int(str(x)[2:])),np.dtype('a6')) for x in range(1990,2016)]+\\\n",
    "#               [('sales{:02d}'.format(int(str(x)[2:])),str) for x in range(1990,2016)]+\\\n",
    "#               [('emp{:02d}'.format(int(str(x)[2:])),np.int32) for x in range(1990,2016)])\n",
    "\n",
    "# fipsmap = dict([('fips{:02d}'.format(int(str(x)[2:])),x) for x in range(1990,2016)]+\\\n",
    "#               [('sales{:02d}'.format(int(str(x)[2:])),x) for x in range(1990,2016)]+\\\n",
    "#               [('naics{:02d}'.format(int(str(x)[2:])),x) for x in range(1990,2016)]+\\\n",
    "#               [('emp{:02d}'.format(int(str(x)[2:])),x) for x in range(1990,2016)])\n",
    "# fipstypemap['dunsnumber']=str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load naics to occupation mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load naics-to-soc crosswalk for naics 92 public adm data from PUMS 1-year data\n",
    "\n",
    "naics92_soc_pct = pd.read_csv(os.path.join(INPUT_PATH,'pums_2016_naics92_soc_share.csv'),dtype={'NAICSP_2':str}).rename(columns={'NAICSP_2':'naics_2','soc_x':'occ_code'}).set_index(['naics_2','occ_code']).Total\n",
    "naics92_soc_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define 'overflow' industries at 2 digit level\n",
    "\n",
    "naics_exp={'31': '31-33',\n",
    " '32': '31-33',\n",
    " '33': '31-33',\n",
    " '44': u'44-45',\n",
    " '45': u'44-45',\n",
    " '48': u'48-49',\n",
    " '49': u'48-49'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bay area counties\n",
    "bayareafips_full = {'06001':'Alameda', '06013':'Contra Costa', '06041':'Marin', '06055':'Napa', '06075':'San Francisco', '06081':'San Mateo', '06085':'Santa Clara', '06097':'Sonoma', '06095':'Solano'}\n",
    "\n",
    "bayareamsas={'06001': u'San Francisco-Oakland-Hayward, CA',\n",
    " '06013': u'San Francisco-Oakland-Hayward, CA',\n",
    " '06041': u'San Francisco-Oakland-Hayward, CA',\n",
    " '06055': u'Napa, CA',\n",
    " #'06069': u'San Jose-Sunnyvale-Santa Clara, CA',\n",
    " '06075': u'San Francisco-Oakland-Hayward, CA',\n",
    " #'06077': u'Stockton-Lodi, CA',\n",
    " '06081': u'San Francisco-Oakland-Hayward, CA',\n",
    " '06085': u'San Jose-Sunnyvale-Santa Clara, CA',\n",
    " '06087': u'Santa Cruz-Watsonville, CA',\n",
    " '06095': u'Vallejo-Fairfield, CA',\n",
    " '06097': u'Santa Rosa, CA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load naics sector to occupation crosswalk from OES research estimates, subsetted to CA only\n",
    "naics_to_occ=pd.read_excel(os.path.join(INPUT_PATH,'oes_research_data_2019_naics_to_occ_share_ca.xlsx'),dtype={'naics':str,'occ_code':str})\n",
    "naics_to_occ=naics_to_occ.set_index(['naics','occ_code']).tot_emp\n",
    "naics_to_occ.index=naics_to_occ.index.set_names('naics_2',level=0)\n",
    "naics_to_occ.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naics_to_occ_w_naics_92 = naics_to_occ.append(naics92_soc_pct)#.reset_index(name='tot_emp')\n",
    "naics_to_occ_w_naics_92.name='tot_emp'\n",
    "\n",
    "naics_to_occ_w_naics_92 = naics_to_occ_w_naics_92.reset_index()\n",
    "\n",
    "naics_to_occ_w_naics_92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load Dingel's (2020) occupational propensities\n",
    "# workfromhomeocc_onet_bls=pd.read_csv('https://raw.githubusercontent.com/jdingel/DingelNeiman-workathome/master/occ_onet_scores/output/occupations_workathome.csv')\n",
    "# workfromhomeocc_onet_bls['OCC_CODE']=workfromhomeocc_onet_bls.onetsoccode.str.split('\\.').apply(lambda x: x[0])\n",
    "# workfromhomeocc_onet_bls=workfromhomeocc_onet_bls.groupby(['OCC_CODE']).teleworkable.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load Dingel's (2020) occupational propensities, from O*NET scoring, and mapped to OES categories\n",
    "\n",
    "# 'https://raw.githubusercontent.com/jdingel/DingelNeiman-workathome/master/onet_to_BLS_crosswalk/output/onet_teleworkable_blscodes.csv')\n",
    "#workfromhomeocc_onet=pd.read_csv(os.path.join(INPUT_PATH,'onet_teleworkable_blscodes.csv'),sep='\\t')\n",
    "workfromhomeocc_onet=pd.read_csv('https://raw.githubusercontent.com/jdingel/DingelNeiman-workathome/master/onet_to_BLS_crosswalk/output/onet_teleworkable_blscodes.csv')\n",
    "workfromhomeocc=workfromhomeocc_onet.set_index('OCC_CODE').teleworkable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the employment data\n",
    "Get from postgres whenever there are query changes. Otherwise, grab from csv made each time postgres is queried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_int = lambda x: pd.to_numeric('{:.0f}'.format(pd.to_numeric(x)),errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## large file - get just a few cols\n",
    "\n",
    "nets = pd.read_csv(os.path.join(NETS_PATH,'nets2015wide.csv'),\n",
    "                   usecols=['dunsnumber','firstyear','lastyear','fips15','naics15','emp15'],\n",
    "                   na_values=[''],\n",
    "                   engine='python')\n",
    "\n",
    "nets = nets.set_index(['dunsnumber','firstyear','lastyear'])\n",
    "\n",
    "print(\"nets length:{} rows size: {} MB\".format(len(nets), nets.memory_usage(index=True).sum()*1e-6))\n",
    "nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffbreaks_5 =[0,25,50,100,250,500,1000,np.inf]\n",
    "difflabels_5 = easylabeler(breaks=diffbreaks_5,currency=False)\n",
    "difflabels_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nets_2015=nets.filter(regex='15') # lmz commented out since this appears to do nothing\n",
    "nets_2015=nets.loc[nets.fips15.notnull()].copy()\n",
    "nets_2015['naics_2']=nets_2015.naics15.astype(int).apply(lambda x: '{:0<6}'.format(x)).str.slice(0,2)\n",
    "nets_2015['naics_2_desc']=nets_2015.naics_2.map(naicsmap)\n",
    "nets_2015['naics_abag']=nets_2015.naics_2.map(naics_abag)\n",
    "nets_2015['naics_mtc']=nets_2015.naics_2.map(naics_mtc)\n",
    "nets_2015['naics_2']=nets_2015['naics_2'].replace(naics_exp)\n",
    "nets_2015['emp_size_cat']=pd.cut(nets_2015.emp15,bins=diffbreaks_5,labels=difflabels_5)\n",
    "nets_2015['emp_bucket']=pd.cut(nets_2015.emp15,bins=[0,25,np.inf],labels=['0-25 employees','25+ employees'])\n",
    "nets_2015['STCOUNTY']=nets_2015.fips15.astype(int).apply(lambda x: '{:0>5}'.format(x))\n",
    "nets_2015['CBSA']=nets_2015.STCOUNTY.map(bayareamsas)\n",
    "nets_2015=nets_2015[nets_2015.STCOUNTY.isin(bayareafips_full)]\n",
    "\n",
    "print(\"nets_2015 length: {} rows; size: {} MB\".format(len(nets_2015), nets_2015.memory_usage(index=True).sum()*1e-6))\n",
    "nets_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## basic summary by indus\n",
    "nets_2015_by_indus_size = nets_2015.groupby(['CBSA','STCOUNTY','naics_2','naics_2_desc','naics_abag','naics_mtc','emp_bucket']).emp15.sum()\n",
    "nets_2015_by_indus_size = nets_2015_by_indus_size.reset_index()\n",
    "\n",
    "print(\"nets_2015_by_indus_size has {} rows with emp15=null, {} rows with emp15 not null\".format(\n",
    "    pd.isnull(nets_2015_by_indus_size.emp15).sum(),\n",
    "    pd.notnull(nets_2015_by_indus_size.emp15).sum()))\n",
    "\n",
    "# select only those few rows with emp15 not null\n",
    "nets_2015_by_indus_size = nets_2015_by_indus_size.loc[pd.notnull(nets_2015_by_indus_size.emp15),]\n",
    "\n",
    "print(\"nets_2015_by_indus_size total emp15: {}\".format(nets_2015_by_indus_size.emp15.sum()))\n",
    "\n",
    "nets_2015_by_indus_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge emp data with naics-to-occ crosswalk\n",
    "\n",
    "nets_2015_occ_exp=nets_2015_by_indus_size.merge(naics_to_occ_w_naics_92,on=['naics_2'])\n",
    "nets_2015_occ_exp['reg']='Bay Area'\n",
    "\n",
    "## weigh employment by telecommute propensity from Dingel (2020)\n",
    "nets_2015_occ_exp['emp15_occ']=nets_2015_occ_exp.emp15*nets_2015_occ_exp.tot_emp\n",
    "\n",
    "print(\"nets_2015_occ_exp length: {} rows  size: {} MB\".format(len(nets_2015_occ_exp), sys.getsizeof(nets_2015_occ_exp)*1e-6))\n",
    "nets_2015_occ_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## codes in NETS data but *not* in the Dingel work from home matrix\n",
    "\n",
    "nets_occ_unmatched=list(set(nets_2015_occ_exp.occ_code)-set(workfromhomeocc.index))\n",
    "#oes_occ_unmatched=list(set(naics_to_occ_w_naics_92.index.get_level_values(1).unique())-set(workfromhomeocc.index))\n",
    "#onet_occ_unmatched=list(set(workfromhomeocc.index)-set(naics_to_occ_w_naics_92.index.get_level_values(1).unique()))\n",
    "\n",
    "# nets_occ_unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc_det=soc.loc[soc['Detailed Occupation'].notnull()]\n",
    "soc_det_missing_from_onet_WFH=soc_det[soc_det['Detailed Occupation'].isin(nets_occ_unmatched)].set_index('Detailed Occupation').Description.index\n",
    "\n",
    "## take the codes that do *not* have a wfh flag and assign the most common flag \n",
    "## for occupations in the containing major group. Many of them are \"other xxx\"\n",
    "onet_missing_group_imputed=pd.Series(data=soc_det_missing_from_onet_WFH.str.slice(0,2).map(workfromhomeocc.groupby(lambda x: x[:2]).median()),\n",
    "          index=soc_det_missing_from_onet_WFH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADD work from home share\n",
    "nets_2015_occ_exp['wfh_flag']=nets_2015_occ_exp.occ_code.map(workfromhomeocc.append(onet_missing_group_imputed))\n",
    "nets_2015_occ_exp['emp15_wfh']=nets_2015_occ_exp.emp15_occ*nets_2015_occ_exp.wfh_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets_2015_occ_exp.groupby(['emp_bucket'])['emp15_occ','emp15_wfh'].sum().plot(kind='barh',figsize=[8,5])\n",
    "title('Bay Area employment, total and work from home potential\\nSources: Work from home potential from Dingel (2020)\\nIndustry - Occupation matrix from BLS OES 2019 Research Estimates (CA subset)\\nEmployment data from National Establishment Timeseries (NETS) 2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=[8,6])\n",
    "ax=sns.heatmap((nets_2015_occ_exp.groupby(['CBSA','naics_mtc','emp_bucket'])['emp15_wfh'].sum()/\\\n",
    "nets_2015_occ_exp.groupby(['CBSA','naics_mtc','emp_bucket'])['emp15_occ'].sum()).unstack([0]).loc(0)[:,'25+ employees'].reset_index(1,drop=True).T\n",
    ",\n",
    "annot=True,fmt=',.2f',linewidths=.5,cmap=cm.coolwarm,\n",
    "           annot_kws={'fontsize':12})\n",
    "title('Employment susceptible to telecommuting, using NETS data\\nclassified using Dingel (2020) after mapping industry to occupation data\\nBay Area CBSAs shown')\n",
    "plt.tight_layout()\n",
    "plt.yticks(rotation=0,size=12)\n",
    "plt.xticks(rotation=45,size=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=[8,6])\n",
    "ax=sns.heatmap((nets_2015_occ_exp.groupby(['STCOUNTY','naics_mtc','emp_bucket'])['emp15_wfh'].sum()/\\\n",
    "nets_2015_occ_exp.groupby(['STCOUNTY','naics_mtc','emp_bucket'])['emp15_occ'].sum()).unstack([0]).loc(0)[:,'25+ employees'].reset_index(1,drop=True).T\n",
    ",\n",
    "annot=True,fmt=',.2f',linewidths=.5,cmap=cm.coolwarm,\n",
    "           annot_kws={'fontsize':12})\n",
    "title('Employment susceptible to telecommuting, using NETS data\\nclassified using Dingel (2020) after mapping industry to occupation data\\nBay Area CBSAs shown')\n",
    "plt.tight_layout()\n",
    "plt.yticks(rotation=0,size=12)\n",
    "plt.xticks(rotation=45,size=12)\n",
    "#savefig(os.path.join(box, 'RHNA/Analyses/equity/divergence_opportunity_corr.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=[8,6])\n",
    "ax=sns.heatmap((nets_2015_occ_exp.groupby(['naics_mtc','emp_bucket'])['emp15_wfh'].sum()/\\\n",
    "nets_2015_occ_exp.groupby(['naics_mtc','emp_bucket'])['emp15_occ'].sum()).unstack(),\n",
    "annot=True,fmt=',.2f',linewidths=.5,cmap=cm.coolwarm,\n",
    "           annot_kws={'fontsize':12})\n",
    "title('Employment susceptible to telecommuting, using NETS data\\nclassified using Dingel (2020) after mapping industry to occupation data')\n",
    "plt.tight_layout()\n",
    "plt.yticks(rotation=0,size=12)\n",
    "plt.xticks(rotation=45,size=12)\n",
    "#savefig(os.path.join(box, 'RHNA/Analyses/equity/divergence_opportunity_corr.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out spreadsheet with WFH potential share based on industry / occupation and establishment size alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "xl4=pd.ExcelWriter(os.path.join(OUTPUT_PATH,'WFH_By_Sector_V4.xlsx'))\n",
    "\n",
    "header = pd.DataFrame(data=[\"Source: {}\".format(THIS_SCRIPT),\"Table\"], columns=[\"col1\"])\n",
    "\n",
    "## WORK FROM HOME POTENTIAL SHARE, by ESTAB SIZE\n",
    "header.loc[1] = \"Table 1: Work from home potential, by establishment size\"\n",
    "header.to_excel(xl4, 'wfhshare_by_estabsize',index=False,merge_cells=False,header=False)\n",
    "\n",
    "(nets_2015_occ_exp.groupby(['emp_bucket'])['emp15_wfh'].sum()/\\\n",
    "nets_2015_occ_exp.groupby(['emp_bucket'])['emp15_occ'].sum()).reset_index(name='value').to_excel(xl4,'wfhshare_by_estabsize',index=False,merge_cells=False,startrow=3)\n",
    "\n",
    "## WORK FROM HOME POTENTIAL SHARE, by ESTAB SIZE, SECTOR\n",
    "header.loc[1] = \"Table 2: Work from home potential, by establishment size and sector\"\n",
    "header.to_excel(xl4, 'wfhshare_by_estabsize_sector',index=False,merge_cells=False,header=False)\n",
    "\n",
    "(nets_2015_occ_exp.groupby(['naics_mtc','emp_bucket'])['emp15_wfh'].sum()/\\\n",
    "nets_2015_occ_exp.groupby(['naics_mtc','emp_bucket'])['emp15_occ'].sum()).unstack(1).to_excel(xl4,'wfhshare_by_estabsize_sector',index=True,merge_cells=False,startrow=3)\n",
    "\n",
    "## Work from home potential, by establishment size, sector and MSA\n",
    "header.loc[1] = \"Table 3: Work from home potential, by establishment size, sector and MSA\"\n",
    "header.to_excel(xl4, 'wfhshare_by_cbsa_sector',index=False,merge_cells=False,header=False)\n",
    "\n",
    "(nets_2015_occ_exp.groupby(['CBSA','naics_mtc'])['emp15_wfh'].sum()/\\\n",
    "nets_2015_occ_exp.groupby(['CBSA','naics_mtc'])['emp15_occ'].sum()).reset_index(name='share').to_excel(xl4,'wfhshare_by_cbsa_sector',index=False,merge_cells=False,startrow=3)\n",
    "\n",
    "## Work from home potential, by sector\n",
    "header.loc[1] = \"Table 4: Work from home potential, by sector\"\n",
    "header.to_excel(xl4, 'wfhshare_by_sector',index=False,merge_cells=False,header=False)\n",
    "\n",
    "(nets_2015_occ_exp.groupby(['naics_mtc'])['emp15_wfh'].sum()/\\\n",
    "nets_2015_occ_exp.groupby(['naics_mtc'])['emp15_occ'].sum()).reset_index(name='share').to_excel(xl4,'wfhshare_by_sector',index=False,merge_cells=False,startrow=3)\n",
    "\n",
    "## Table 5 Work from home potential, by establishment size, sector and county fips\n",
    "header.loc[1] = \"Table 5: Work from home potential, by establishment size, sector and county fips\"\n",
    "header.to_excel(xl4, 'wfhshare_by_county_sector',index=False,merge_cells=False,header=False)\n",
    "\n",
    "(nets_2015_occ_exp.groupby(['STCOUNTY','naics_mtc'])['emp15_wfh'].sum()/\\\n",
    "nets_2015_occ_exp.groupby(['STCOUNTY','naics_mtc'])['emp15_occ'].sum()).reset_index(name='share').to_excel(xl4,'wfhshare_by_county_sector',index=False,merge_cells=False,startrow=3)\n",
    "\n",
    "## Table 6 Employees by firm size\n",
    "header.loc[1] = \"Table 6: Employment by Establishment Size\"\n",
    "header.to_excel(xl4, 'emp_by_size',index=False,merge_cells=False,header=False)\n",
    "\n",
    "emp_by_size = nets_2015.groupby(['emp_bucket'])['emp15'].sum().reset_index()\n",
    "emp_by_size[\"emp15_share\"] = emp_by_size.emp15/emp_by_size.emp15.sum()\n",
    "emp_by_size.to_excel(xl4, 'emp_by_size', index=False,merge_cells=False,startrow=3)\n",
    "\n",
    "xl4.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
